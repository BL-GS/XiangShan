diff --git a/README.md b/README.md
index 8add43c6a..0da723b76 100644
--- a/README.md
+++ b/README.md
@@ -1,4 +1,4 @@
-# XiangShan
+XiangShan
 
 XiangShan (香山) is an open-source high-performance RISC-V processor project.
 
diff --git a/difftest b/difftest
index 5b53e89cf..1b881e824 160000
--- a/difftest
+++ b/difftest
@@ -1 +1 @@
-Subproject commit 5b53e89cf27642440ec5800d45b852e4459fc296
+Subproject commit 1b881e8249ce747b65c6b5ed31d4e63861221ab5
diff --git a/huancun b/huancun
index 719ec0904..2f7889766 160000
--- a/huancun
+++ b/huancun
@@ -1 +1 @@
-Subproject commit 719ec0904f56f6a71fc63b1fd0f6994965e52505
+Subproject commit 2f788976682d341eec7d55e7aa063f9894d9108d
diff --git a/src/main/scala/xiangshan/Bundle.scala b/src/main/scala/xiangshan/Bundle.scala
index 33bc70567..5e5f05c3b 100644
--- a/src/main/scala/xiangshan/Bundle.scala
+++ b/src/main/scala/xiangshan/Bundle.scala
@@ -113,6 +113,7 @@ class CfiUpdateInfo(implicit p: Parameters) extends XSBundle with HasBPUParamete
 class CtrlFlow(implicit p: Parameters) extends XSBundle {
   val instr = UInt(32.W)
   val pc = UInt(VAddrBits.W)
+  val gpaddr = UInt(GPAddrBits.W)
   val foldpc = UInt(MemPredPCWidth.W)
   val exceptionVec = ExceptionVec()
   val trigger = new TriggerCf
@@ -193,6 +194,7 @@ class CtrlSignals(implicit p: Parameters) extends XSBundle {
   def isSoftPrefetch: Bool = {
     fuType === FuType.alu && fuOpType === ALUOpType.or && selImm === SelImm.IMM_I && ldest === 0.U
   }
+  def isHyperInst: Bool = fuType === FuType.ldu && LSUOpType.isHlv(fuOpType) || fuType === FuType.stu && LSUOpType.isHsv(fuOpType)
 }
 
 class CfCtrl(implicit p: Parameters) extends XSBundle {
@@ -437,9 +439,15 @@ class TlbSatpBundle(implicit p: Parameters) extends SatpStruct {
 
 class TlbCsrBundle(implicit p: Parameters) extends XSBundle {
   val satp = new TlbSatpBundle()
+  val vsatp = new TlbSatpBundle()
+  val hgatp = new TlbSatpBundle()
   val priv = new Bundle {
     val mxr = Bool()
     val sum = Bool()
+    val vmxr = Bool()
+    val vsum = Bool()
+    val virt = Bool()
+    val spvp = UInt(1.W)
     val imode = UInt(2.W)
     val dmode = UInt(2.W)
   }
@@ -458,8 +466,11 @@ class SfenceBundle(implicit p: Parameters) extends XSBundle {
     val addr = UInt(VAddrBits.W)
     val asid = UInt(AsidLength.W)
     val flushPipe = Bool()
+    val hv = Bool()
+    val hg = Bool()
   }
 
+
   override def toPrintable: Printable = {
     p"valid:0x${Hexadecimal(valid)} rs1:${bits.rs1} rs2:${bits.rs2} addr:${Hexadecimal(bits.addr)}, flushPipe:${bits.flushPipe}"
   }
@@ -522,6 +533,8 @@ class CustomCSRCtrlIO(implicit p: Parameters) extends XSBundle {
   val frontend_trigger = new FrontendTdataDistributeIO()
   val mem_trigger = new MemTdataDistributeIO()
   val trigger_enable = Output(Vec(10, Bool()))
+  // Virtualization Mode
+  val virtMode = Output(Bool())
 }
 
 class DistributedCSRIO(implicit p: Parameters) extends XSBundle {
diff --git a/src/main/scala/xiangshan/Parameters.scala b/src/main/scala/xiangshan/Parameters.scala
index edf32fa4b..6dfcdcaf9 100644
--- a/src/main/scala/xiangshan/Parameters.scala
+++ b/src/main/scala/xiangshan/Parameters.scala
@@ -43,17 +43,21 @@ case class XSCoreParameters
   HasPrefetch: Boolean = false,
   HartId: Int = 0,
   XLEN: Int = 64,
+  HSXLEN: Int = 64,
   HasMExtension: Boolean = true,
   HasCExtension: Boolean = true,
+  HasHExtension: Boolean = true,
   HasDiv: Boolean = true,
   HasICache: Boolean = true,
   HasDCache: Boolean = true,
   AddrBits: Int = 64,
   VAddrBits: Int = 39,
+  GPAddrBits: Int = 41,
   HasFPU: Boolean = true,
   HasCustomCSRCacheOp: Boolean = true,
   FetchWidth: Int = 8,
   AsidLength: Int = 16,
+  VmidLength: Int = 14,
   EnableBPU: Boolean = true,
   EnableBPD: Boolean = true,
   EnableRAS: Boolean = true,
@@ -173,6 +177,7 @@ case class XSCoreParameters
   EnableAccurateLoadError: Boolean = true,
   EnableUncacheWriteOutstanding: Boolean = false,
   MMUAsidLen: Int = 16, // max is 16, 0 is not supported now
+  MMUVmidLen: Int = 14,
   ReSelectLen: Int = 6, // load replay queue replay select counter len
   itlbParameters: TLBParameters = TLBParameters(
     name = "itlb",
@@ -300,18 +305,27 @@ trait HasXSParameter {
   val env = p(DebugOptionsKey)
 
   val XLEN = coreParams.XLEN
+  val HSXLEN = coreParams.HSXLEN
   val minFLen = 32
   val fLen = 64
   def xLen = XLEN
 
   val HasMExtension = coreParams.HasMExtension
   val HasCExtension = coreParams.HasCExtension
+  val HasHExtension = coreParams.HasHExtension
   val HasDiv = coreParams.HasDiv
   val HasIcache = coreParams.HasICache
   val HasDcache = coreParams.HasDCache
   val AddrBits = coreParams.AddrBits // AddrBits is used in some cases
-  val VAddrBits = coreParams.VAddrBits // VAddrBits is Virtual Memory addr bits
+  val VAddrBits =
+    if(HasHExtension) {
+      coreParams.GPAddrBits
+    }else{
+      coreParams.VAddrBits
+    } // VAddrBits is Virtual Memory addr bits
+  val GPAddrBits = coreParams.GPAddrBits
   val AsidLength = coreParams.AsidLength
+  val VmidLength = coreParams.VmidLength
   val ReSelectLen = coreParams.ReSelectLen
   val AddrBytes = AddrBits / 8 // unused
   val DataBits = XLEN
@@ -420,6 +434,7 @@ trait HasXSParameter {
   val EnableAccurateLoadError = coreParams.EnableAccurateLoadError
   val EnableUncacheWriteOutstanding = coreParams.EnableUncacheWriteOutstanding
   val asidLen = coreParams.MMUAsidLen
+  val vmidLen = coreParams.MMUVmidLen
   val BTLBWidth = coreParams.LoadPipelineWidth + coreParams.StorePipelineWidth
   val refillBothTlb = coreParams.refillBothTlb
   val itlbParams = coreParams.itlbParameters
diff --git a/src/main/scala/xiangshan/XSCore.scala b/src/main/scala/xiangshan/XSCore.scala
index 32a5e309d..a66c98443 100644
--- a/src/main/scala/xiangshan/XSCore.scala
+++ b/src/main/scala/xiangshan/XSCore.scala
@@ -369,7 +369,8 @@ class XSCoreImp(outer: XSCoreBase) extends LazyModuleImp(outer)
   ctrlBlock.perfinfo.perfEventsEu0 := exuBlocks(0).getPerf.dropRight(outer.exuBlocks(0).scheduler.numRs)
   ctrlBlock.perfinfo.perfEventsEu1 := exuBlocks(1).getPerf.dropRight(outer.exuBlocks(1).scheduler.numRs)
   if (!coreParams.softPTW) {
-    memBlock.io.perfEventsPTW := ptw.getPerf
+//    memBlock.io.perfEventsPTW := ptw.getPerf
+    memBlock.io.perfEventsPTW := DontCare
   } else {
     memBlock.io.perfEventsPTW := DontCare
   }
@@ -397,6 +398,7 @@ class XSCoreImp(outer: XSCoreBase) extends LazyModuleImp(outer)
   csrioIn.interrupt <> ctrlBlock.io.robio.toCSR.intrBitSet
   csrioIn.wfi_event <> ctrlBlock.io.robio.toCSR.wfiEvent
   csrioIn.memExceptionVAddr <> memBlock.io.lsqio.exceptionAddr.vaddr
+  csrioIn.memExceptionGPAddr <> memBlock.io.lsqio.exceptionAddr.gpaddr
 
   csrioIn.externalInterrupt.msip := outer.clint_int_sink.in.head._1(0)
   csrioIn.externalInterrupt.mtip := outer.clint_int_sink.in.head._1(1)
diff --git a/src/main/scala/xiangshan/backend/MemBlock.scala b/src/main/scala/xiangshan/backend/MemBlock.scala
index 211ba7b49..41919c28f 100644
--- a/src/main/scala/xiangshan/backend/MemBlock.scala
+++ b/src/main/scala/xiangshan/backend/MemBlock.scala
@@ -185,7 +185,7 @@ class MemBlockImp(outer: MemBlock) extends LazyModuleImp(outer)
   atomicsUnit.io.out.ready := ldOut0.ready
   loadUnits.head.io.ldout.ready := ldOut0.ready
   when(atomicsUnit.io.out.valid){
-    ldOut0.bits.uop.cf.exceptionVec := 0.U(16.W).asBools // exception will be writebacked via store wb port
+    ldOut0.bits.uop.cf.exceptionVec := 0.U(24.W).asBools // exception will be writebacked via store wb port
   }
 
   val ldExeWbReqs = ldOut0 +: loadUnits.tail.map(_.io.ldout)
@@ -274,7 +274,7 @@ class MemBlockImp(outer: MemBlock) extends LazyModuleImp(outer)
   }
 
   val ptw_resp_next = RegEnable(io.ptw.resp.bits, io.ptw.resp.valid)
-  val ptw_resp_v = RegNext(io.ptw.resp.valid && !(sfence.valid && tlbcsr.satp.changed), init = false.B)
+  val ptw_resp_v = RegNext(io.ptw.resp.valid && !(sfence.valid && tlbcsr.satp.changed && tlbcsr.vsatp.changed && tlbcsr.hgatp.changed), init = false.B)
   io.ptw.resp.ready := true.B
 
   dtlb.flatMap(a => a.ptw.req)
@@ -285,7 +285,7 @@ class MemBlockImp(outer: MemBlock) extends LazyModuleImp(outer)
       else if (i < exuParameters.LduCnt) Cat(ptw_resp_next.vector.take(exuParameters.LduCnt)).orR
       else Cat(ptw_resp_next.vector.drop(exuParameters.LduCnt)).orR
     io.ptw.req(i).valid := tlb.valid && !(ptw_resp_v && vector_hit &&
-      ptw_resp_next.data.entry.hit(tlb.bits.vpn, tlbcsr.satp.asid, allType = true, ignoreAsid = true))
+      ptw_resp_next.data.entry.hit(tlb.bits.vpn, tlbcsr.satp.asid, allType = true, ignoreAsid = true, tlb.bits.virt || tlb.bits.hyperinst))
   }
   dtlb.foreach(_.ptw.resp.bits := ptw_resp_next.data)
   if (refillBothTlb) {
@@ -660,8 +660,10 @@ class MemBlockImp(outer: MemBlock) extends LazyModuleImp(outer)
   }.elsewhen (atomicsUnit.io.exceptionAddr.valid) {
     atomicsException := true.B
   }
-  val atomicsExceptionAddress = RegEnable(atomicsUnit.io.exceptionAddr.bits, atomicsUnit.io.exceptionAddr.valid)
+  val atomicsExceptionAddress = RegEnable(atomicsUnit.io.exceptionAddr.bits.vaddr, atomicsUnit.io.exceptionAddr.valid)
+  val atomicsExceptionGPAddress = RegEnable(atomicsUnit.io.exceptionAddr.bits.gpaddr, atomicsUnit.io.exceptionAddr.valid)
   io.lsqio.exceptionAddr.vaddr := RegNext(Mux(atomicsException, atomicsExceptionAddress, lsq.io.exceptionAddr.vaddr))
+  io.lsqio.exceptionAddr.gpaddr := RegNext(Mux(atomicsException, atomicsExceptionGPAddress, lsq.io.exceptionAddr.gpaddr))
   XSError(atomicsException && atomicsUnit.io.in.valid, "new instruction before exception triggers\n")
 
   io.memInfo.sqFull := RegNext(lsq.io.sqFull)
diff --git a/src/main/scala/xiangshan/backend/decode/DecodeUnit.scala b/src/main/scala/xiangshan/backend/decode/DecodeUnit.scala
index f00f00b76..cd470ec0b 100644
--- a/src/main/scala/xiangshan/backend/decode/DecodeUnit.scala
+++ b/src/main/scala/xiangshan/backend/decode/DecodeUnit.scala
@@ -23,7 +23,7 @@ import freechips.rocketchip.rocket.Instructions
 import freechips.rocketchip.util.uintToBitPat
 import utils._
 import utility._
-import xiangshan.ExceptionNO.illegalInstr
+import xiangshan.ExceptionNO.{illegalInstr, virtualInstr}
 import xiangshan._
 import freechips.rocketchip.rocket.Instructions._
 
@@ -435,6 +435,31 @@ object CBODecode extends DecodeConstants {
   )
 }
 
+/*
+ * Hypervisor decode
+ */
+object HypervisorDecode extends DecodeConstants {
+  val table: Array[(BitPat, List[BitPat])] = Array(
+    HFENCE_GVMA -> List(SrcType.reg, SrcType.reg, SrcType.X, FuType.fence, FenceOpType.hfence_g, N, N, N, Y, Y, Y, SelImm.X),
+    HFENCE_VVMA -> List(SrcType.reg, SrcType.reg, SrcType.X, FuType.fence, FenceOpType.hfence_v, N, N, N, Y, Y, Y, SelImm.X),
+    HINVAL_GVMA -> List(SrcType.reg, SrcType.reg, SrcType.X, FuType.fence, FenceOpType.hfence_g, N, N, N, N, N, N, SelImm.X),
+    HINVAL_VVMA -> List(SrcType.reg, SrcType.reg, SrcType.X, FuType.fence, FenceOpType.hfence_v, N, N, N, N, N, N, SelImm.X),
+    HLV_B       -> List(SrcType.reg, SrcType.X, SrcType.X, FuType.ldu, LSUOpType.hlvb,  Y, N, N, N, N, N, SelImm.X),
+    HLV_BU      -> List(SrcType.reg, SrcType.X, SrcType.X, FuType.ldu, LSUOpType.hlvbu,  Y, N, N, N, N, N, SelImm.X),
+    HLV_D       -> List(SrcType.reg, SrcType.X, SrcType.X, FuType.ldu, LSUOpType.hlvd,  Y, N, N, N, N, N, SelImm.X),
+    HLV_H       -> List(SrcType.reg, SrcType.X, SrcType.X, FuType.ldu, LSUOpType.hlvh,  Y, N, N, N, N, N, SelImm.X),
+    HLV_HU      -> List(SrcType.reg, SrcType.X, SrcType.X, FuType.ldu, LSUOpType.hlvhu,  Y, N, N, N, N, N, SelImm.X),
+    HLV_W       -> List(SrcType.reg, SrcType.X, SrcType.X, FuType.ldu, LSUOpType.hlvw,  Y, N, N, N, N, N, SelImm.X),
+    HLV_WU      -> List(SrcType.reg, SrcType.X, SrcType.X, FuType.ldu, LSUOpType.hlvwu,  Y, N, N, N, N, N, SelImm.X),
+    HLVX_HU     -> List(SrcType.reg, SrcType.X, SrcType.X, FuType.ldu, LSUOpType.hlvxhu,  Y, N, N, N, N, N, SelImm.X),
+    HLVX_WU     -> List(SrcType.reg, SrcType.X, SrcType.X, FuType.ldu, LSUOpType.hlvxwu,  Y, N, N, N, N, N, SelImm.X),
+    HSV_B       -> List(SrcType.reg, SrcType.reg, SrcType.X, FuType.stu, LSUOpType.hsvb,  N, N, N, N, N, N, SelImm.X),
+    HSV_D       -> List(SrcType.reg, SrcType.reg, SrcType.X, FuType.stu, LSUOpType.hsvd,  N, N, N, N, N, N, SelImm.X),
+    HSV_H       -> List(SrcType.reg, SrcType.reg, SrcType.X, FuType.stu, LSUOpType.hsvh,  N, N, N, N, N, N, SelImm.X),
+    HSV_W       -> List(SrcType.reg, SrcType.reg, SrcType.X, FuType.stu, LSUOpType.hsvw,  N, N, N, N, N, N, SelImm.X)
+  )
+}
+
 /**
  * XiangShan Trap Decode constants
  */
@@ -585,7 +610,8 @@ class DecodeUnit(implicit p: Parameters) extends XSModule with DecodeUnitConstan
     XSTrapDecode.table ++
     BDecode.table ++
     CBODecode.table ++
-    SvinvalDecode.table
+    SvinvalDecode.table ++
+    HypervisorDecode.table
   // assertion for LUI: only LUI should be assigned `selImm === SelImm.IMM_U && fuType === FuType.alu`
   val luiMatch = (t: Seq[BitPat]) => t(3).value == FuType.alu.litValue && t.reverse.head.value == SelImm.IMM_U.litValue
   val luiTable = decode_table.filter(t => luiMatch(t._2)).map(_._1).distinct
@@ -605,6 +631,7 @@ class DecodeUnit(implicit p: Parameters) extends XSModule with DecodeUnitConstan
   val isMove = BitPat("b000000000000_?????_000_?????_0010011") === ctrl_flow.instr
   cs.isMove := isMove && ctrl_flow.instr(RD_MSB, RD_LSB) =/= 0.U
 
+
   // read src1~3 location
   cs.lsrc(0) := ctrl_flow.instr(RS1_MSB, RS1_LSB)
   cs.lsrc(1) := ctrl_flow.instr(RS2_MSB, RS2_LSB)
@@ -621,11 +648,26 @@ class DecodeUnit(implicit p: Parameters) extends XSModule with DecodeUnitConstan
     val sinval = BitPat("b0001011_?????_?????_000_00000_1110011") === ctrl_flow.instr
     val w_inval = BitPat("b0001100_00000_00000_000_00000_1110011") === ctrl_flow.instr
     val inval_ir = BitPat("b0001100_00001_00000_000_00000_1110011") === ctrl_flow.instr
-    val svinval_ii = sinval || w_inval || inval_ir
+    val hinval_gvma = HINVAL_GVMA === ctrl_flow.instr
+    val hinval_vvma = HINVAL_VVMA === ctrl_flow.instr
+    val svinval_ii = sinval || w_inval || inval_ir || hinval_gvma || hinval_vvma
     cf_ctrl.cf.exceptionVec(illegalInstr) := base_ii || svinval_ii
     cs.flushPipe := false.B
   }
 
+  when(io.csrCtrl.virtMode){
+    // vs/vu attempting to exec hyperinst will raise virtual instruction
+    cf_ctrl.cf.exceptionVec(virtualInstr) := ctrl_flow.instr === HLV_B || ctrl_flow.instr === HLV_BU ||
+      ctrl_flow.instr === HLV_H   || ctrl_flow.instr === HLV_HU ||
+      ctrl_flow.instr === HLVX_HU || ctrl_flow.instr === HLV_W  ||
+      ctrl_flow.instr === HLVX_WU || ctrl_flow.instr === HLV_WU ||
+      ctrl_flow.instr === HLV_D   || ctrl_flow.instr === HSV_B  ||
+      ctrl_flow.instr === HSV_H   || ctrl_flow.instr === HSV_W  ||
+      ctrl_flow.instr === HSV_D   || ctrl_flow.instr === HFENCE_VVMA ||
+      ctrl_flow.instr === HFENCE_GVMA || ctrl_flow.instr === HINVAL_GVMA ||
+      ctrl_flow.instr === HINVAL_VVMA
+  }
+
   // fix frflags
   //                           fflags    zero csrrs rd    csr
   val isFrflags = BitPat("b000000000001_00000_010_?????_1110011") === ctrl_flow.instr
diff --git a/src/main/scala/xiangshan/backend/exu/ExeUnit.scala b/src/main/scala/xiangshan/backend/exu/ExeUnit.scala
index 6a2121e8a..df9443d32 100644
--- a/src/main/scala/xiangshan/backend/exu/ExeUnit.scala
+++ b/src/main/scala/xiangshan/backend/exu/ExeUnit.scala
@@ -37,6 +37,9 @@ class FenceIO(implicit p: Parameters) extends XSBundle {
 class ExeUnit(config: ExuConfig)(implicit p: Parameters) extends Exu(config) {
 
   val disableSfence = WireInit(false.B)
+  val disableHfenceg = WireInit(false.B)
+  val disableHfencev = WireInit(false.B)
+  val virtMode = WireInit(false.B)
   val csr_frm = WireInit(frm.getOrElse(0.U(3.W)))
 
   val hasRedirect = config.fuConfigs.zip(functionUnits).filter(_._1.hasRedirect).map(_._2)
@@ -57,6 +60,9 @@ class ExeUnit(config: ExuConfig)(implicit p: Parameters) extends Exu(config) {
     csrio.get.trapTarget := RegNext(csr.csrio.trapTarget)
     csr.csrio.exception := DelayN(csrio.get.exception, 2)
     disableSfence := csr.csrio.disableSfence
+    disableHfenceg := csr.csrio.disableHfenceg
+    disableHfencev := csr.csrio.disableHfencev
+    virtMode := csr.csrio.customCtrl.virtMode
     csr_frm := csr.csrio.fpu.frm
     // setup skip for hpm CSR read
     io.out.bits.debug.isPerfCnt := RegNext(csr.csrio.isPerfCnt) // TODO: this is dirty
@@ -71,6 +77,9 @@ class ExeUnit(config: ExuConfig)(implicit p: Parameters) extends Exu(config) {
     fenceio.get.sbuffer <> fence.toSbuffer
     fence.io.out.ready := true.B
     fence.disableSfence := disableSfence
+    fence.disableHfenceg := disableHfenceg
+    fence.disableHfencev := disableHfencev
+    fence.virtMode := virtMode
   }
 
   val fpModules = functionUnits.zip(config.fuConfigs.zipWithIndex).filter(_._1.isInstanceOf[FPUSubModule])
diff --git a/src/main/scala/xiangshan/backend/fu/CSR.scala b/src/main/scala/xiangshan/backend/fu/CSR.scala
index 298faa577..5122ece5d 100644
--- a/src/main/scala/xiangshan/backend/fu/CSR.scala
+++ b/src/main/scala/xiangshan/backend/fu/CSR.scala
@@ -118,6 +118,7 @@ class CSRFileIO(implicit p: Parameters) extends XSBundle {
   val wfi_event = Output(Bool())
   // from LSQ
   val memExceptionVAddr = Input(UInt(VAddrBits.W))
+  val memExceptionGPAddr = Input(UInt(GPAddrBits.W))
   // from outside cpu,externalInterrupt
   val externalInterrupt = new ExternalInterruptIO
   // TLB
@@ -127,6 +128,10 @@ class CSRFileIO(implicit p: Parameters) extends XSBundle {
   val debugMode = Output(Bool())
   // to Fence to disable sfence
   val disableSfence = Output(Bool())
+  // to Fence to disable hfence.gvma
+  val disableHfenceg = Output(Bool())
+  // to Fence to disable hfence.vvma
+  val disableHfencev = Output(Bool())
   // Custom microarchiture ctrl signal
   val customCtrl = Output(new CustomCSRCtrlIO)
   // distributed csr write
@@ -151,9 +156,13 @@ class CSR(implicit p: Parameters) extends FunctionUnit with HasCSRConst with PMP
 
   // CSR define
 
+  //H extention
+  val virtMode = RegInit(false.B)
+  csrio.customCtrl.virtMode := virtMode
+
   class Priv extends Bundle {
     val m = Output(Bool())
-    val h = Output(Bool())
+    val h = Output(Bool()) // unused
     val s = Output(Bool())
     val u = Output(Bool())
   }
@@ -182,7 +191,9 @@ class CSR(implicit p: Parameters) extends FunctionUnit with HasCSRConst with PMP
   class MstatusStruct extends Bundle {
     val sd = Output(UInt(1.W))
 
-    val pad1 = if (XLEN == 64) Output(UInt(25.W)) else null
+    val pad1 = if (XLEN == 64 && HasHExtension) Output(UInt(23.W)) else if (XLEN == 64) Output(UInt(25.W)) else null
+    val mpv  = if (XLEN == 64 && HasHExtension) Output(UInt(1.W)) else null
+    val gva  = if (XLEN == 64 && HasHExtension) Output(UInt(1.W)) else null
     val mbe  = if (XLEN == 64) Output(UInt(1.W)) else null
     val sbe  = if (XLEN == 64) Output(UInt(1.W)) else null
     val sxl  = if (XLEN == 64) Output(UInt(2.W))  else null
@@ -198,7 +209,7 @@ class CSR(implicit p: Parameters) extends FunctionUnit with HasCSRConst with PMP
     val xs = Output(UInt(2.W))
     val fs = Output(UInt(2.W))
     val mpp = Output(UInt(2.W))
-    val hpp = Output(UInt(2.W))
+    val vs = Output(UInt(2.W))
     val spp = Output(UInt(1.W))
     val pie = new Priv
     val ie = new Priv
@@ -210,6 +221,25 @@ class CSR(implicit p: Parameters) extends FunctionUnit with HasCSRConst with PMP
     }
   }
 
+  class HstatusStruct extends Bundle {
+    val pad4 = if (HSXLEN == 64) Output(UInt(30.W)) else null
+    val vsxl = if (HSXLEN == 64) Output(UInt(2.W)) else null
+    val pad3 = Output(UInt(9.W))
+    val vtsr = Output(UInt(1.W))
+    val vtw = Output(UInt(1.W))
+    val vtvm = Output(UInt(1.W))
+    val pad2 = Output(UInt(2.W))
+    val vgein = Output(UInt(6.W))
+    val pad1 = Output(UInt(2.W))
+    val hu = Output(UInt(1.W))
+    val spvp = Output(UInt(1.W))
+    val spv = Output(UInt(1.W))
+    val gva = Output(UInt(1.W))
+    val vsbe = Output(UInt(1.W))
+    val pad0 = Output(UInt(5.W))
+    assert(this.getWidth == XLEN)
+  }
+
   class Interrupt extends Bundle {
 //  val d = Output(Bool())    // Debug
     val e = new Priv
@@ -351,23 +381,32 @@ class CSR(implicit p: Parameters) extends FunctionUnit with HasCSRConst with PMP
   val mcounteren = RegInit(UInt(XLEN.W), 0.U)
   val mcause = RegInit(UInt(XLEN.W), 0.U)
   val mtval = RegInit(UInt(XLEN.W), 0.U)
-  val mepc = Reg(UInt(XLEN.W))
+  val mtval2 = RegInit(UInt(XLEN.W), 0.U)
+  val mtinst = RegInit(UInt(XLEN.W), 0.U)
+  val mepc = RegInit(UInt(XLEN.W), 0.U)
   // Page 36 in riscv-priv: The low bit of mepc (mepc[0]) is always zero.
   val mepcMask = ~(0x1.U(XLEN.W))
 
   val mie = RegInit(0.U(XLEN.W))
   val mipWire = WireInit(0.U.asTypeOf(new Interrupt))
   val mipReg  = RegInit(0.U(XLEN.W))
-  val mipFixMask = ZeroExt(GenMask(9) | GenMask(5) | GenMask(1), XLEN)
+  val mipMask = ZeroExt(GenMask(9) | GenMask(5) | GenMask(1), XLEN)
   val mip = (mipWire.asUInt | mipReg).asTypeOf(new Interrupt)
 
+  val mip_mie_WMask_H = if(HasHExtension){((1 << 2) | (1 << 6) | (1 << 10) | (1 << 12)).U(XLEN.W)}else{0.U(XLEN.W)}
+  val vssip_Mask = (1 << 2).U(XLEN.W)
+
+  val mipWMask = vssip_Mask | ((1 << 9) | (1 << 5) | (1 << 1)).U(XLEN.W)
+  val mieWMask = mip_mie_WMask_H | ((1 << 11) | (1 << 9) | (1 << 7) | (1 << 5) | (1 << 3) | (1 << 1)).U(XLEN.W)
+
   def getMisaMxl(mxl: BigInt): BigInt = mxl << (XLEN - 2)
   def getMisaExt(ext: Char): Long = 1 << (ext.toInt - 'a'.toInt)
   var extList = List('a', 's', 'i', 'u')
   if (HasMExtension) { extList = extList :+ 'm' }
   if (HasCExtension) { extList = extList :+ 'c' }
+  if (HasHExtension) { extList = extList :+ 'h' }
   if (HasFPU) { extList = extList ++ List('f', 'd') }
-  val misaInitVal = getMisaMxl(2) | extList.foldLeft(0L)((sum, i) => sum | getMisaExt(i)) //"h8000000000141105".U
+  val misaInitVal = getMisaMxl(2) | extList.foldLeft(0L)((sum, i) => sum | getMisaExt(i)) //"h8000000000141185".U
   val misa = RegInit(UInt(XLEN.W), misaInitVal.U)
 
   // MXL = 2          | 0 | EXT = b 00 0000 0100 0001 0001 0000 0101
@@ -411,7 +450,11 @@ class CSR(implicit p: Parameters) extends FunctionUnit with HasCSRConst with PMP
   }
 
   val mstatusWMask = (~ZeroExt((
-    GenMask(XLEN - 2, 36) | // WPRI
+    (if(HasHExtension) {
+      GenMask(XLEN - 2, 40) |
+      GenMask(37, 36)          // MBE SBE
+    } else
+      GenMask(XLEN - 2, 36)) | // WPRI
     GenMask(35, 32)       | // SXL and UXL cannot be changed
     GenMask(31, 23)       | // WPRI
     GenMask(16, 15)       | // XS is read-only
@@ -420,7 +463,11 @@ class CSR(implicit p: Parameters) extends FunctionUnit with HasCSRConst with PMP
     GenMask(2)              // WPRI
   ), 64)).asUInt
   val mstatusMask = (~ZeroExt((
-    GenMask(XLEN - 2, 36) | // WPRI
+    (if (HasHExtension) {
+      GenMask(XLEN - 2, 40) |
+        GenMask(37, 36) // MBE SBE
+    } else
+      GenMask(XLEN - 2, 36)) | // WPRI
     GenMask(31, 23)       | // WPRI
     GenMask(10, 9)        | // WPRI
     GenMask(6)            | // WPRI
@@ -428,16 +475,26 @@ class CSR(implicit p: Parameters) extends FunctionUnit with HasCSRConst with PMP
   ), 64)).asUInt
 
   val medeleg = RegInit(UInt(XLEN.W), 0.U)
-  val mideleg = RegInit(UInt(XLEN.W), 0.U)
+  val midelegInit = if(HasHExtension){((1 << 12) | (1 << 10) | (1 << 6) | (1 << 2)).U}else{0.U}
+  val medelegWMask = if(HasHExtension) {
+      ((1 << 23) | (1 << 22) | (1 << 21) | (1 << 20) | (1 << 15) | (1 << 13) | (1 << 12) | (1 << 10) | (1 << 9) | (1 << 8) | (1 << 3) | (1 << 0)).U(XLEN.W)
+  }else {
+    "hb3ff".U(XLEN.W)
+  }
+
+
+  val mideleg = RegInit(UInt(XLEN.W), midelegInit)
   val mscratch = RegInit(UInt(XLEN.W), 0.U)
 
+  val midelegWMask = "h222".U(XLEN.W)
+
   // PMP Mapping
   val pmp = Wire(Vec(NumPMP, new PMPEntry())) // just used for method parameter
   val pma = Wire(Vec(NumPMA, new PMPEntry())) // just used for method parameter
   val pmpMapping = pmp_gen_mapping(pmp_init, NumPMP, PmpcfgBase, PmpaddrBase, pmp)
   val pmaMapping = pmp_gen_mapping(pma_init, NumPMA, PmacfgBase, PmaaddrBase, pma)
 
-  // Superviser-Level CSRs
+  // Supervisor-Level CSRs
 
   // val sstatus = RegInit(UInt(XLEN.W), "h00000000".U)
   val sstatusWmask = "hc6122".U(XLEN.W)
@@ -466,7 +523,7 @@ class CSR(implicit p: Parameters) extends FunctionUnit with HasCSRConst with PMP
   // Page 60 in riscv-priv: The low bit of sepc (sepc[0]) is always zero.
   val sepcMask = ~(0x1.U(XLEN.W))
   val scause = RegInit(UInt(XLEN.W), 0.U)
-  val stval = Reg(UInt(XLEN.W))
+  val stval = RegInit(UInt(XLEN.W), 0.U)
   val sscratch = RegInit(UInt(XLEN.W), 0.U)
   val scounteren = RegInit(UInt(XLEN.W), 0.U)
 
@@ -572,9 +629,47 @@ class CSR(implicit p: Parameters) extends FunctionUnit with HasCSRConst with PMP
   csrio.customCtrl.svinval_enable := srnctl(1)
   csrio.customCtrl.wfi_enable := srnctl(2)
 
+  // Hypervisor CSRs
+  val hstatusWMask = "h7003c0".U(XLEN.W)
+  // hstatus: vtsr, vtw, vtvm, hu, spvp, spv, gva,
+  val hstatus = RegInit("h200000000".U(XLEN.W))
+  val hstatusStruct = hstatus.asTypeOf(new HstatusStruct)
+  val hedeleg = RegInit(UInt(XLEN.W), 0.U)
+  val hideleg = RegInit(UInt(XLEN.W), 0.U)
+  val hidelegRMask = mideleg
+  val hidelegWMask = ((1 << 10) | (1 << 6) | (1 << 2)).U(XLEN.W)
+  val hgeie   = RegInit(UInt(XLEN.W), 0.U)
+  val htval = RegInit(UInt(XLEN.W), 0.U)
+  // hvip hip hie is part of mip or mie
+  val hvipMask = ((1 << 10) | (1 << 6) | (1 << 2)).U(XLEN.W)
+  val hipRMask = (((1 << 12).U | hvipMask) & mideleg)
+  val hipWMask = ((1 << 2).U & mideleg)// vssip
+  val hieMask = hipRMask
+  val htinst = RegInit(UInt(XLEN.W), 0.U)
+  val hgeip = RegInit(UInt(XLEN.W), 0.U)
+  val henvcfg = RegInit(UInt(XLEN.W), 0.U)
+  val hgatp = RegInit(UInt(XLEN.W), 0.U)
+  val hgatpMask = Cat("h8".U(Hgatp_Mode_len.W), satp_part_wmask(Hgatp_Vmid_len, VmidLength), satp_part_wmask(Hgatp_Addr_len, PAddrBits-12))
+  val htimedelta = RegInit(UInt(XLEN.W), 0.U)
+  val hcounteren = RegInit(UInt(XLEN.W), 0.U)
+
+  val vsstatus = RegInit("ha00002000".U(XLEN.W))
+  val vsstatusStruct = vsstatus.asTypeOf(new MstatusStruct)
+  //vsie vsip
+  val vsMask = ((1 << 10) | (1 << 6) | (1 << 2)).U(XLEN.W)
+  val vsip_ie_Mask = ZeroExt((hideleg & mideleg & vsMask), XLEN)
+  val vsip_WMask = ZeroExt((hideleg & mideleg & vssip_Mask), XLEN)
+  val vstvec = RegInit(UInt(XLEN.W), 0.U)
+  val vsscratch = RegInit(UInt(XLEN.W), 0.U)
+  val vsepc = RegInit(UInt(XLEN.W), 0.U)
+  val vscause = RegInit(UInt(XLEN.W), 0.U)
+  val vstval = RegInit(UInt(XLEN.W), 0.U)
+  val vsatp = RegInit(UInt(XLEN.W), 0.U)
+
   val tlbBundle = Wire(new TlbCsrBundle)
   tlbBundle.satp.apply(satp)
-
+  tlbBundle.vsatp.apply(vsatp)
+  tlbBundle.hgatp.apply(hgatp)
   csrio.tlb := tlbBundle
 
   // User-Level CSRs
@@ -694,12 +789,12 @@ class CSR(implicit p: Parameters) extends FunctionUnit with HasCSRConst with PMP
     MaskedRegMap(Sepc, sepc, sepcMask, MaskedRegMap.NoSideEffect, sepcMask),
     MaskedRegMap(Scause, scause),
     MaskedRegMap(Stval, stval),
-    MaskedRegMap(Sip, mip.asUInt, sipWMask, MaskedRegMap.Unwritable, sipMask),
+    MaskedRegMap(Sip, mipReg.asUInt, sipWMask, MaskedRegMap.NoSideEffect, sipMask),
 
     //--- Supervisor Protection and Translation ---
     MaskedRegMap(Satp, satp, satpMask, MaskedRegMap.NoSideEffect, satpMask),
 
-    //--- Supervisor Custom Read/Write Registers
+    //--- Supervisor Custom Read/Write Registers ---
     MaskedRegMap(Sbpctl, sbpctl),
     MaskedRegMap(Spfctl, spfctl),
     MaskedRegMap(Sfetchctl, sfetchctl),
@@ -708,7 +803,7 @@ class CSR(implicit p: Parameters) extends FunctionUnit with HasCSRConst with PMP
     MaskedRegMap(Smblockctl, smblockctl),
     MaskedRegMap(Srnctl, srnctl),
 
-    //--- Machine Information Registers ---
+  //--- Machine Information Registers ---
     MaskedRegMap(Mvendorid, mvendorid, 0.U(XLEN.W), MaskedRegMap.Unwritable),
     MaskedRegMap(Marchid, marchid, 0.U(XLEN.W), MaskedRegMap.Unwritable),
     MaskedRegMap(Mimpid, mimpid, 0.U(XLEN.W), MaskedRegMap.Unwritable),
@@ -718,9 +813,9 @@ class CSR(implicit p: Parameters) extends FunctionUnit with HasCSRConst with PMP
     //--- Machine Trap Setup ---
     MaskedRegMap(Mstatus, mstatus, mstatusWMask, mstatusUpdateSideEffect, mstatusMask),
     MaskedRegMap(Misa, misa, 0.U, MaskedRegMap.Unwritable), // now whole misa is unchangeable
-    MaskedRegMap(Medeleg, medeleg, "hb3ff".U(XLEN.W)),
-    MaskedRegMap(Mideleg, mideleg, "h222".U(XLEN.W)),
-    MaskedRegMap(Mie, mie),
+    MaskedRegMap(Medeleg, medeleg, medelegWMask),
+    MaskedRegMap(Mideleg, mideleg, midelegWMask, MaskedRegMap.NoSideEffect),
+    MaskedRegMap(Mie, mie, mieWMask, MaskedRegMap.NoSideEffect),
     MaskedRegMap(Mtvec, mtvec, mtvecMask, MaskedRegMap.NoSideEffect, mtvecMask),
     MaskedRegMap(Mcounteren, mcounteren),
 
@@ -729,7 +824,7 @@ class CSR(implicit p: Parameters) extends FunctionUnit with HasCSRConst with PMP
     MaskedRegMap(Mepc, mepc, mepcMask, MaskedRegMap.NoSideEffect, mepcMask),
     MaskedRegMap(Mcause, mcause),
     MaskedRegMap(Mtval, mtval),
-    MaskedRegMap(Mip, mip.asUInt, 0.U(XLEN.W), MaskedRegMap.Unwritable),
+    MaskedRegMap(Mip, mipReg.asUInt, mipWMask, MaskedRegMap.NoSideEffect, mipMask),
 
     //--- Trigger ---
     MaskedRegMap(Tselect, tselectPhy, WritableMask, WriteTselect),
@@ -748,6 +843,48 @@ class CSR(implicit p: Parameters) extends FunctionUnit with HasCSRConst with PMP
     MaskedRegMap(Minstret, minstret),
   )
 
+  // hypervisor csr map
+  val hcsrMapping = Map(
+    //--- Hypervisor Trap Setup ---
+    MaskedRegMap(Hstatus, hstatus, hstatusWMask),
+    MaskedRegMap(Hedeleg, hedeleg),
+    MaskedRegMap(Hideleg, hideleg, hidelegWMask, MaskedRegMap.NoSideEffect, hidelegRMask),
+    MaskedRegMap(Hie, mie, hieMask, MaskedRegMap.NoSideEffect, hieMask),
+    MaskedRegMap(Hcounteren, hcounteren),
+    MaskedRegMap(Hgeie, hgeie),
+
+    //--- Hypervisor Trap Handling ---
+    MaskedRegMap(Htval, htval),
+    MaskedRegMap(Hip, mipReg.asUInt, hipWMask, MaskedRegMap.NoSideEffect, hipRMask),
+    MaskedRegMap(Hvip, mipReg.asUInt, hvipMask, MaskedRegMap.NoSideEffect, hvipMask),
+    MaskedRegMap(Htinst, htinst),
+    MaskedRegMap(Hgeip, hgeip),
+
+    //--- Hypervisor Configuration ---
+    MaskedRegMap(Henvcfg, henvcfg),
+
+    //--- Hypervisor Protection and Translation ---
+    MaskedRegMap(Hgatp, hgatp),
+
+    //--- Hypervisor Counter/Timer Virtualization Registers ---
+    MaskedRegMap(Htimedelta, htimedelta),
+
+    //--- Virtual Supervisor Registers ---
+    MaskedRegMap(Vsstatus, vsstatus, rmask = sstatusRmask, wmask = sstatusWmask),
+    MaskedRegMap(Vsie, mie, rmask = vsip_ie_Mask, wmask = vsip_ie_Mask),
+    MaskedRegMap(Vstvec, vstvec),
+    MaskedRegMap(Vsscratch, vsscratch),
+    MaskedRegMap(Vsepc, vsepc),
+    MaskedRegMap(Vscause, vscause),
+    MaskedRegMap(Vstval, vstval),
+    MaskedRegMap(Vsip, mipReg.asUInt, vsip_WMask, MaskedRegMap.NoSideEffect, vsip_ie_Mask),
+    MaskedRegMap(Vsatp, vsatp, satpMask, MaskedRegMap.NoSideEffect, satpMask),
+
+    //--- Machine Registers ---
+    MaskedRegMap(Mtval2, mtval2),
+    MaskedRegMap(Mtinst, mtinst),
+  )
+
   val perfCntMapping = (0 until 29).map(i => {Map(
     MaskedRegMap(addr = Mhpmevent3 +i,
                  reg  = perfEvents(i),
@@ -778,12 +915,32 @@ class CSR(implicit p: Parameters) extends FunctionUnit with HasCSRConst with PMP
                 pmpMapping ++
                 pmaMapping ++
                 (if (HasFPU) fcsrMapping else Nil) ++
-                (if (HasCustomCSRCacheOp) cacheopMapping else Nil)
-
-  val addr = src2(11, 0)
+                (if (HasCustomCSRCacheOp) cacheopMapping else Nil) ++
+                (if (HasHExtension) hcsrMapping else Nil)
+
+
+  val vs_s_csr_map = Map(
+    Sstatus.U  -> Vsstatus.U,
+    Sie.U      -> Vsie.U,
+    Stvec.U    -> Vstvec.U,
+    Sscratch.U -> Vsscratch.U,
+    Sepc.U     -> Vsepc.U,
+    Scause.U   -> Vscause.U,
+    Stval.U    -> Vstval.U,
+    Sip.U      -> Vsip.U,
+    Satp.U     -> Vsatp.U
+  )
+  val addr = Wire(UInt(12.W))
+  val vscsr_addr = LookupTreeDefault(src2(11, 0), src2(11, 0), vs_s_csr_map)
+  when(virtMode){
+    addr := vscsr_addr
+  }.otherwise{
+    addr := src2(11, 0)
+  }
   val csri = ZeroExt(src2(16, 12), XLEN)
   val rdata = Wire(UInt(XLEN.W))
-  val wdata = LookupTree(func, List(
+  val rdata_tmp = Wire(UInt(XLEN.W))
+  val wdata_tmp = LookupTree(func, List(
     CSROpType.wrt  -> src1,
     CSROpType.set  -> (rdata | src1),
     CSROpType.clr  -> (rdata & (~src1).asUInt),
@@ -791,7 +948,8 @@ class CSR(implicit p: Parameters) extends FunctionUnit with HasCSRConst with PMP
     CSROpType.seti -> (rdata | csri),
     CSROpType.clri -> (rdata & (~csri).asUInt)
   ))
-
+  val is_vsip_ie = addr === Vsip.U || addr === Vsie.U
+  val wdata = Mux(is_vsip_ie, ZeroExt(wdata_tmp << 1, XLEN), wdata_tmp)
   val addrInPerfCnt = (addr >= Mcycle.U) && (addr <= Mhpmcounter31.U) ||
     (addr >= Mcountinhibit.U) && (addr <= Mhpmevent31.U) ||
     addr === Mip.U
@@ -801,19 +959,24 @@ class CSR(implicit p: Parameters) extends FunctionUnit with HasCSRConst with PMP
   val satpLegalMode = (wdata.asTypeOf(new SatpStruct).mode===0.U) || (wdata.asTypeOf(new SatpStruct).mode===8.U)
 
   // csr access check, special case
-  val tvmNotPermit = (priviledgeMode === ModeS && mstatusStruct.tvm.asBool)
+  val tvmNotPermit = (priviledgeMode === ModeS && !virtMode && mstatusStruct.tvm.asBool)
   val accessPermitted = !(addr === Satp.U && tvmNotPermit)
-  csrio.disableSfence := tvmNotPermit
-
+  val vtvmNotPermit = (priviledgeMode === ModeS && virtMode && hstatusStruct.vtvm.asBool)
+  val vaccessPermitted = !(addr === Vsatp.U && vtvmNotPermit)
+  csrio.disableSfence := (tvmNotPermit || !virtMode && priviledgeMode < ModeS) || (vtvmNotPermit || virtMode && priviledgeMode < ModeS)
+  csrio.disableHfenceg := !((!virtMode && priviledgeMode === ModeS && !mstatusStruct.tvm.asBool) || (priviledgeMode === ModeM)) // only valid in HS and mstatus.tvm == 0 or in M
+  csrio.disableHfencev :=  !(priviledgeMode === ModeM || (!virtMode && priviledgeMode === ModeS))
   // general CSR wen check
-  val wen = valid && CSROpType.needAccess(func) && (addr=/=Satp.U || satpLegalMode)
+  val wen = valid && CSROpType.needAccess(func) && ((addr=/=Satp.U && addr =/= Vsatp.U) || satpLegalMode)
   val dcsrPermitted = dcsrPermissionCheck(addr, false.B, debugMode)
   val triggerPermitted = triggerPermissionCheck(addr, true.B, debugMode) // todo dmode
-  val modePermitted = csrAccessPermissionCheck(addr, false.B, priviledgeMode) && dcsrPermitted && triggerPermitted
+  val HasH = (HasHExtension == true).asBool()
+  val csrAccess = csrAccessPermissionCheck(addr, false.B, priviledgeMode, virtMode, HasH)
+  val modePermitted = csrAccess === 0.U && dcsrPermitted && triggerPermitted
   val perfcntPermitted = perfcntPermissionCheck(addr, priviledgeMode, mcounteren, scounteren)
-  val permitted = Mux(addrInPerfCnt, perfcntPermitted, modePermitted) && accessPermitted
-
-  MaskedRegMap.generate(mapping, addr, rdata, wen && permitted, wdata)
+  val permitted = Mux(addrInPerfCnt, perfcntPermitted, modePermitted) && Mux(virtMode, vaccessPermitted, accessPermitted)
+  MaskedRegMap.generate(mapping, addr, rdata_tmp, wen && permitted, wdata)
+  rdata := Mux(is_vsip_ie, ZeroExt(rdata_tmp >> 1, XLEN), rdata_tmp)
   io.out.bits.data := rdata
   io.out.bits.uop := io.in.bits.uop
   io.out.bits.uop.cf := cfOut
@@ -824,21 +987,6 @@ class CSR(implicit p: Parameters) extends FunctionUnit with HasCSRConst with PMP
   csrio.customCtrl.distribute_csr.w.bits.data := wdata
   csrio.customCtrl.distribute_csr.w.bits.addr := addr
 
-  // Fix Mip/Sip write
-  val fixMapping = Map(
-    MaskedRegMap(Mip, mipReg.asUInt, mipFixMask),
-    MaskedRegMap(Sip, mipReg.asUInt, sipWMask, MaskedRegMap.NoSideEffect, sipMask)
-  )
-  val rdataFix = Wire(UInt(XLEN.W))
-  val wdataFix = LookupTree(func, List(
-    CSROpType.wrt  -> src1,
-    CSROpType.set  -> (rdataFix | src1),
-    CSROpType.clr  -> (rdataFix & (~src1).asUInt),
-    CSROpType.wrti -> csri,
-    CSROpType.seti -> (rdataFix | csri),
-    CSROpType.clri -> (rdataFix & (~csri).asUInt)
-  ))
-  MaskedRegMap.generate(fixMapping, addr, rdataFix, wen && permitted, wdataFix)
 
   when (RegNext(csrio.fpu.fflags.valid)) {
     fcsr := fflags_wfn(update = true)(RegNext(csrio.fpu.fflags.bits))
@@ -879,32 +1027,56 @@ class CSR(implicit p: Parameters) extends FunctionUnit with HasCSRConst with PMP
   // Illegal priviledged operation list
   val illegalMret = valid && isMret && priviledgeMode < ModeM
   val illegalSret = valid && isSret && priviledgeMode < ModeS
-  val illegalSModeSret = valid && isSret && priviledgeMode === ModeS && mstatusStruct.tsr.asBool
+  val illegalSModeSret = valid && isSret && priviledgeMode === ModeS && virtMode === false.B && mstatusStruct.tsr.asBool
+  // when hstatus.vtsr == 1, if sret is executed in VS-mode, it will cause virtual instruction
+  val illegalVSModeSret = valid && isSret && priviledgeMode === ModeS && virtMode && hstatusStruct.vtsr.asBool
+
   // When TW=1, then if WFI is executed in any less-privileged mode,
   // and it does not complete within an implementation-specific, bounded time limit,
   // the WFI instruction causes an illegal instruction exception.
   // The time limit may always be 0, in which case WFI always causes
   // an illegal instruction exception in less-privileged modes when TW=1.
-  val illegalWFI = valid && isWFI && priviledgeMode < ModeM && mstatusStruct.tw === 1.U
-
+  val illegalWFI = valid && isWFI && (priviledgeMode < ModeM && mstatusStruct.tw === 1.U ||  priviledgeMode === ModeU && !virtMode)
+  val illegalVWFI = valid && isWFI && ((virtMode && priviledgeMode === ModeS && hstatusStruct.vtw === 1.U && mstatusStruct.tw === 0.U)||
+      (virtMode && priviledgeMode === ModeU && mstatusStruct.tw === 0.U))
   // Illegal priviledged instruction check
   val isIllegalAddr = valid && CSROpType.needAccess(func) && MaskedRegMap.isIllegalAddr(mapping, addr)
-  val isIllegalAccess = wen && !permitted
+  val isIllegalAccess = !virtMode && wen && !(Mux(addrInPerfCnt, perfcntPermitted, csrAccess === 0.U && dcsrPermitted && triggerPermitted) && accessPermitted)
   val isIllegalPrivOp = illegalMret || illegalSret || illegalSModeSret || illegalWFI
 
+  val isIllegalVAccess = virtMode && wen && (csrAccess === 2.U || !vaccessPermitted)
+  val isIllegalVPrivOp = illegalVSModeSret || illegalVWFI
   // expose several csr bits for tlb
   tlbBundle.priv.mxr   := mstatusStruct.mxr.asBool
   tlbBundle.priv.sum   := mstatusStruct.sum.asBool
+  tlbBundle.priv.vmxr := vsstatusStruct.mxr.asBool
+  tlbBundle.priv.vsum := vsstatusStruct.sum.asBool
+  tlbBundle.priv.spvp := hstatusStruct.spvp
+  tlbBundle.priv.virt  := Mux(mstatusStruct.mprv.asBool, mstatusStruct.mpv & (mstatusStruct.mpp =/= ModeM), virtMode)
   tlbBundle.priv.imode := priviledgeMode
   tlbBundle.priv.dmode := Mux(debugMode && dcsr.asTypeOf(new DcsrStruct).mprven, ModeM, Mux(mstatusStruct.mprv.asBool, mstatusStruct.mpp, priviledgeMode))
 
   // Branch control
-  val retTarget = Wire(UInt(VAddrBits.W))
+  val retTarget = WireInit(0.U)
   val resetSatp = addr === Satp.U && wen // write to satp will cause the pipeline be flushed
   flushPipe := resetSatp || (valid && func === CSROpType.jmp && !isEcall && !isEbreak)
 
-  retTarget := DontCare
-  // val illegalEret = TODO
+  private val illegalRetTarget = WireInit(false.B)
+  when(valid) {
+    when(isDret) {
+      retTarget := dpc(VAddrBits - 1, 0)
+    }.elsewhen(isMret && !illegalMret) {
+      retTarget := mepc(VAddrBits - 1, 0)
+    }.elsewhen(isSret && !illegalSret && !illegalSModeSret && !illegalVSModeSret) {
+      retTarget := Mux(virtMode, vsepc(VAddrBits - 1, 0), sepc(VAddrBits - 1, 0))
+    }.elsewhen(isUret) {
+      retTarget := uepc(VAddrBits - 1, 0)
+    }.otherwise {
+      illegalRetTarget := true.B
+    }
+  }.otherwise {
+    illegalRetTarget := true.B // when illegalRetTarget setted, retTarget should never be used
+  }
 
   when (valid && isDret) {
     val mstatusOld = WireInit(mstatus.asTypeOf(new MstatusStruct))
@@ -914,7 +1086,6 @@ class CSR(implicit p: Parameters) extends FunctionUnit with HasCSRConst with PMP
     when (dcsr.asTypeOf(new DcsrStruct).prv =/= ModeM) {mstatusNew.mprv := 0.U} //If the new privilege mode is less privileged than M-mode, MPRV in mstatus is cleared.
     mstatus := mstatusNew.asUInt
     priviledgeMode := dcsrNew.prv
-    retTarget := dpc(VAddrBits-1, 0)
     debugModeNew := false.B
     debugIntrEnable := true.B
     debugMode := debugModeNew
@@ -926,25 +1097,42 @@ class CSR(implicit p: Parameters) extends FunctionUnit with HasCSRConst with PMP
     val mstatusNew = WireInit(mstatus.asTypeOf(new MstatusStruct))
     mstatusNew.ie.m := mstatusOld.pie.m
     priviledgeMode := mstatusOld.mpp
+    if(HasHExtension) {
+      virtMode := mstatusOld.mpv
+      mstatusNew.mpv := 0.U
+    }
     mstatusNew.pie.m := true.B
     mstatusNew.mpp := ModeU
     when (mstatusOld.mpp =/= ModeM) { mstatusNew.mprv := 0.U }
     mstatus := mstatusNew.asUInt
-    // lr := false.B
-    retTarget := mepc(VAddrBits-1, 0)
   }
 
-  when (valid && isSret && !illegalSret && !illegalSModeSret) {
+  when (valid && isSret && !illegalSret && !illegalSModeSret && !illegalVSModeSret) {
     val mstatusOld = WireInit(mstatus.asTypeOf(new MstatusStruct))
     val mstatusNew = WireInit(mstatus.asTypeOf(new MstatusStruct))
-    mstatusNew.ie.s := mstatusOld.pie.s
-    priviledgeMode := Cat(0.U(1.W), mstatusOld.spp)
-    mstatusNew.pie.s := true.B
-    mstatusNew.spp := ModeU
-    mstatus := mstatusNew.asUInt
-    when (mstatusOld.spp =/= ModeM) { mstatusNew.mprv := 0.U }
-    // lr := false.B
-    retTarget := sepc(VAddrBits-1, 0)
+    val hstatusOld = WireInit(hstatus.asTypeOf(new HstatusStruct))
+    val hstatusNew = WireInit(hstatus.asTypeOf(new HstatusStruct))
+    val vsstatusOld = WireInit(vsstatus.asTypeOf(new MstatusStruct))
+    val vsstatusNew = WireInit(vsstatus.asTypeOf(new MstatusStruct))
+    when (virtMode === 0.U) {
+      virtMode := hstatusOld.spv
+      hstatusNew.spv := 0.U
+      mstatusNew.ie.s := mstatusOld.pie.s
+      priviledgeMode := Cat(0.U(1.W), mstatusOld.spp)
+      mstatusNew.pie.s := true.B
+      mstatusNew.spp := ModeU
+      when(mstatusOld.spp =/= ModeM) {
+        mstatusNew.mprv := 0.U
+      }
+      mstatus := mstatusNew.asUInt
+      hstatus := hstatusNew.asUInt
+    }.otherwise{
+      priviledgeMode := vsstatusOld.spp
+      vsstatusNew.spp := ModeU
+      vsstatusNew.ie.s := vsstatusOld.pie.s
+      vsstatusNew.pie.s := 1.U
+      vsstatus := vsstatusNew.asUInt
+    }
   }
 
   when (valid && isUret) {
@@ -955,7 +1143,6 @@ class CSR(implicit p: Parameters) extends FunctionUnit with HasCSRConst with PMP
     priviledgeMode := ModeU
     mstatusNew.pie.u := true.B
     mstatus := mstatusNew.asUInt
-    retTarget := uepc(VAddrBits-1, 0)
   }
 
   io.in.ready := true.B
@@ -966,12 +1153,14 @@ class CSR(implicit p: Parameters) extends FunctionUnit with HasCSRConst with PMP
   val csrExceptionVec = WireInit(cfIn.exceptionVec)
   csrExceptionVec(breakPoint) := io.in.valid && isEbreak && (ebreakCauseException || debugMode)
   csrExceptionVec(ecallM) := priviledgeMode === ModeM && io.in.valid && isEcall
-  csrExceptionVec(ecallS) := priviledgeMode === ModeS && io.in.valid && isEcall
+  csrExceptionVec(ecallVS) := priviledgeMode === ModeS && virtMode && io.in.valid && isEcall
+  csrExceptionVec(ecallS) := priviledgeMode === ModeS && !virtMode && io.in.valid && isEcall
   csrExceptionVec(ecallU) := priviledgeMode === ModeU && io.in.valid && isEcall
   // Trigger an illegal instr exception when:
   // * unimplemented csr is being read/written
   // * csr access is illegal
   csrExceptionVec(illegalInstr) := isIllegalAddr || isIllegalAccess || isIllegalPrivOp
+  csrExceptionVec(virtualInstr) := isIllegalVAccess || isIllegalVPrivOp
   cfOut.exceptionVec := csrExceptionVec
 
   XSDebug(io.in.valid && isEbreak, s"Debug Mode: an Ebreak is executed, ebreak cause exception ? ${ebreakCauseException}\n")
@@ -979,16 +1168,19 @@ class CSR(implicit p: Parameters) extends FunctionUnit with HasCSRConst with PMP
   /**
     * Exception and Intr
     */
-  val ideleg =  (mideleg & mip.asUInt)
-  def priviledgedEnableDetect(x: Bool): Bool = Mux(x, ((priviledgeMode === ModeS) && mstatusStruct.ie.s) || (priviledgeMode < ModeS),
+  val idelegS =  (mideleg & mip.asUInt)
+  val idelegVS = (hideleg & mideleg & mip.asUInt)
+  def priviledgedEnableDetect(idelegS: Bool, idelegVS: Bool): Bool = Mux(idelegS,
+    Mux(idelegVS, (virtMode && priviledgeMode === ModeS && vsstatusStruct.ie.s) || (virtMode && priviledgeMode < ModeS),
+      ((priviledgeMode === ModeS) && mstatusStruct.ie.s) || (priviledgeMode < ModeS) || virtMode),
     ((priviledgeMode === ModeM) && mstatusStruct.ie.m) || (priviledgeMode < ModeM))
 
   val debugIntr = csrio.externalInterrupt.debug & debugIntrEnable
   XSDebug(debugIntr, "Debug Mode: debug interrupt is asserted and valid!")
   // send interrupt information to ROB
-  val intrVecEnable = Wire(Vec(12, Bool()))
+  val intrVecEnable = Wire(Vec(13, Bool()))
   val disableInterrupt = debugMode || (dcsrData.step && !dcsrData.stepie)
-  intrVecEnable.zip(ideleg.asBools).map{case(x,y) => x := priviledgedEnableDetect(y) && !disableInterrupt}
+  intrVecEnable.zip(idelegS.asBools).zip(idelegVS.asBools).map{case((x,y),z) => x := priviledgedEnableDetect(y, z) && !disableInterrupt}
   val intrVec = Cat(debugIntr && !debugMode, (mie(11,0) & mip.asUInt & intrVecEnable.asUInt))
   val intrBitSet = intrVec.orR
   csrio.interrupt := intrBitSet
@@ -1007,6 +1199,7 @@ class CSR(implicit p: Parameters) extends FunctionUnit with HasCSRConst with PMP
   val raiseIntr = csrio.exception.valid && csrio.exception.bits.isInterrupt
   val ivmEnable = tlbBundle.priv.imode < ModeM && satp.asTypeOf(new SatpStruct).mode === 8.U
   val iexceptionPC = Mux(ivmEnable, SignExt(csrio.exception.bits.uop.cf.pc, XLEN), csrio.exception.bits.uop.cf.pc)
+  val iexceptionGPAddr = Mux(ivmEnable, SignExt(csrio.exception.bits.uop.cf.gpaddr, XLEN), csrio.exception.bits.uop.cf.gpaddr)
   val dvmEnable = tlbBundle.priv.dmode < ModeM && satp.asTypeOf(new SatpStruct).mode === 8.U
   val dexceptionPC = Mux(dvmEnable, SignExt(csrio.exception.bits.uop.cf.pc, XLEN), csrio.exception.bits.uop.cf.pc)
   XSDebug(raiseIntr, "interrupt: pc=0x%x, %d\n", dexceptionPC, intrNO)
@@ -1025,6 +1218,9 @@ class CSR(implicit p: Parameters) extends FunctionUnit with HasCSRConst with PMP
   val hasbreakPoint = csrio.exception.bits.uop.cf.exceptionVec(breakPoint) && raiseException
   val hasSingleStep = csrio.exception.bits.uop.ctrl.singleStep && raiseException
   val hasTriggerHit = (csrio.exception.bits.uop.cf.trigger.hit) && raiseException
+  val hasInstGuestPageFault = csrio.exception.bits.uop.cf.exceptionVec(instrGuestPageFault) && raiseException
+  val hasLoadGuestPageFault = csrio.exception.bits.uop.cf.exceptionVec(loadGuestPageFault) && raiseException
+  val hasStoreGuestPageFault = csrio.exception.bits.uop.cf.exceptionVec(storeGuestPageFault) && raiseException
 
   XSDebug(hasSingleStep, "Debug Mode: single step exception\n")
   XSDebug(hasTriggerHit, p"Debug Mode: trigger hit, is frontend? ${Binary(csrio.exception.bits.uop.cf.trigger.frontendHit.asUInt)} " +
@@ -1055,6 +1251,7 @@ class CSR(implicit p: Parameters) extends FunctionUnit with HasCSRConst with PMP
   // mtval write logic
   // Due to timing reasons of memExceptionVAddr, we delay the write of mtval and stval
   val memExceptionAddr = SignExt(csrio.memExceptionVAddr, XLEN)
+  val memExceptionGPAddr = SignExt(csrio.memExceptionGPAddr, XLEN)
   val updateTval = VecInit(Seq(
     hasInstrPageFault,
     hasLoadPageFault,
@@ -1063,11 +1260,19 @@ class CSR(implicit p: Parameters) extends FunctionUnit with HasCSRConst with PMP
     hasLoadAccessFault,
     hasStoreAccessFault,
     hasLoadAddrMisaligned,
-    hasStoreAddrMisaligned
+    hasStoreAddrMisaligned,
+    hasInstGuestPageFault,
+    hasLoadGuestPageFault,
+    hasStoreGuestPageFault
+  )).asUInt.orR
+  val updateTval_h = VecInit(Seq(
+    hasInstGuestPageFault,
+    hasLoadGuestPageFault,
+    hasStoreGuestPageFault
   )).asUInt.orR
   when (RegNext(RegNext(updateTval))) {
       val tval = Mux(
-        RegNext(RegNext(hasInstrPageFault || hasInstrAccessFault)),
+        RegNext(RegNext(hasInstrPageFault || hasInstrAccessFault || hasInstGuestPageFault)),
         RegNext(RegNext(Mux(
           csrio.exception.bits.uop.cf.crossPageIPFFix,
           SignExt(csrio.exception.bits.uop.cf.pc + 2.U, XLEN),
@@ -1075,20 +1280,46 @@ class CSR(implicit p: Parameters) extends FunctionUnit with HasCSRConst with PMP
         ))),
         memExceptionAddr
     )
+    // because we update tval two beats later, we can choose xtval according to the privilegeMode which has been updated
     when (RegNext(priviledgeMode === ModeM)) {
       mtval := tval
     }.otherwise {
-      stval := tval
+      when (virtMode){
+        vstval := tval
+      }.otherwise{
+        stval := tval
+      }
+    }
+  }
+
+  when(RegNext(RegNext(updateTval_h))) {
+    val tval_tmp = Mux(
+      RegNext(RegNext(hasInstGuestPageFault)),
+      RegNext(RegNext(Mux(
+        csrio.exception.bits.uop.cf.crossPageIPFFix,
+        SignExt(csrio.exception.bits.uop.cf.gpaddr + 2.U, XLEN),
+        iexceptionGPAddr
+      ))),
+      memExceptionGPAddr
+    )
+    val tval = tval_tmp >> 2
+    when(RegNext(priviledgeMode === ModeM)) {
+      mtval2 := tval
+    }.otherwise {
+      htval := tval
     }
   }
 
   val debugTrapTarget = Mux(!isEbreak && debugMode, 0x38020808.U, 0x38020800.U) // 0x808 is when an exception occurs in debug mode prog buf exec
   val deleg = Mux(raiseIntr, mideleg , medeleg)
+  val hdeleg = Mux(raiseIntr, hideleg, hedeleg)
   // val delegS = ((deleg & (1 << (causeNO & 0xf))) != 0) && (priviledgeMode < ModeM);
-  val delegS = deleg(causeNO(3,0)) && (priviledgeMode < ModeM)
+  val delegS = deleg(causeNO(7,0)) && (priviledgeMode < ModeM)
+  val delegVS = virtMode && delegS && hdeleg(causeNO(7, 0)) && (priviledgeMode < ModeM)
   val clearTval = !updateTval || raiseIntr
+  val clearTval_h = !updateTval_h || raiseIntr
   val isXRet = io.in.valid && func === CSROpType.jmp && !isEcall && !isEbreak
-
+  val isHyperInst = csrio.exception.bits.uop.ctrl.isHyperInst
   // ctrl block will use theses later for flush
   val isXRetFlag = RegInit(false.B)
   when (DelayN(io.redirectIn.valid, 5)) {
@@ -1097,26 +1328,33 @@ class CSR(implicit p: Parameters) extends FunctionUnit with HasCSRConst with PMP
     isXRetFlag := true.B
   }
   csrio.isXRet := isXRetFlag
-  val retTargetReg = RegEnable(retTarget, isXRet)
+  private val retTargetReg = RegEnable(retTarget, isXRet && !illegalRetTarget)
+  private val illegalXret = RegEnable(illegalMret || illegalSret || illegalSModeSret || illegalVSModeSret, isXRet)
+  val xtvec = Mux(delegS, Mux(delegVS, vstvec, stvec), mtvec)
+  val xtvecBase = xtvec(VAddrBits - 1, 2)
+  // When MODE=Vectored, all synchronous exceptions into M/S mode
+  // cause the pc to be set to the address in the BASE field, whereas
+  // interrupts cause the pc to be set to the address in the BASE field
+  // plus four times the interrupt cause number.
+  private val pcFromXtvec = Cat(xtvecBase + Mux(xtvec(0) && raiseIntr, causeNO(3, 0), 0.U), 0.U(2.W))
 
-  val tvec = Mux(delegS, stvec, mtvec)
-  val tvecBase = tvec(VAddrBits - 1, 2)
   // XRet sends redirect instead of Flush and isXRetFlag is true.B before redirect.valid.
   // ROB sends exception at T0 while CSR receives at T2.
   // We add a RegNext here and trapTarget is valid at T3.
-  csrio.trapTarget := RegEnable(Mux(isXRetFlag,
-    retTargetReg,
-    Mux(raiseDebugExceptionIntr || ebreakEnterParkLoop, debugTrapTarget,
-      // When MODE=Vectored, all synchronous exceptions into M/S mode
-      // cause the pc to be set to the address in the BASE field, whereas
-      // interrupts cause the pc to be set to the address in the BASE field
-      // plus four times the interrupt cause number.
-      Cat(tvecBase + Mux(tvec(0) && raiseIntr, causeNO(3, 0), 0.U), 0.U(2.W))
-  )), isXRetFlag || csrio.exception.valid)
+  csrio.trapTarget := RegEnable(
+    MuxCase(pcFromXtvec, Seq(
+      (isXRetFlag && !illegalXret) -> retTargetReg,
+      (raiseDebugExceptionIntr || ebreakEnterParkLoop) -> debugTrapTarget
+    )),
+    isXRetFlag || csrio.exception.valid)
 
   when (raiseExceptionIntr) {
     val mstatusOld = WireInit(mstatus.asTypeOf(new MstatusStruct))
     val mstatusNew = WireInit(mstatus.asTypeOf(new MstatusStruct))
+    val hstatusOld = WireInit(hstatus.asTypeOf(new HstatusStruct))
+    val hstatusNew = WireInit(hstatus.asTypeOf(new HstatusStruct))
+    val vsstatusOld = WireInit(vsstatus.asTypeOf(new MstatusStruct))
+    val vsstatusNew = WireInit(vsstatus.asTypeOf(new MstatusStruct))
     val dcsrNew = WireInit(dcsr.asTypeOf(new DcsrStruct))
     val debugModeNew = WireInit(debugMode)
 
@@ -1142,7 +1380,26 @@ class CSR(implicit p: Parameters) extends FunctionUnit with HasCSRConst with PMP
       debugIntrEnable := false.B
     }.elsewhen (debugMode) {
       //do nothing
+    }.elsewhen (delegVS) {
+      vscause := (raiseIntr << (XLEN-1)).asUInt | Mux(raiseIntr, intrNO >> 1.U, exceptionNO)
+      vsepc := Mux(hasInstrPageFault || hasInstrAccessFault, iexceptionPC, dexceptionPC)
+      vsstatusNew.spp := priviledgeMode
+      vsstatusNew.pie.s := vsstatusOld.ie.s
+      vsstatusNew.ie.s := false.B
+      when (clearTval) {vstval := 0.U}
+      virtMode := true.B
+      priviledgeMode := ModeS
     }.elsewhen (delegS) {
+      val virt = Mux(mstatusOld.mprv.asBool(), mstatusOld.mpv, virtMode)
+      // to do hld st
+      hstatusNew.gva := (hasInstGuestPageFault || hasLoadGuestPageFault || hasStoreGuestPageFault ||
+                      ((virt.asBool() || isHyperInst) && ((raiseException && 0.U <= exceptionNO && exceptionNO <= 7.U && exceptionNO =/= 2.U)
+                      || hasInstrPageFault || hasLoadPageFault || hasStorePageFault)))
+      hstatusNew.spv := virtMode
+      when(virtMode){
+        hstatusNew.spvp := priviledgeMode
+      }
+      virtMode := false.B
       scause := causeNO
       sepc := Mux(hasInstrPageFault || hasInstrAccessFault, iexceptionPC, dexceptionPC)
       mstatusNew.spp := priviledgeMode
@@ -1150,7 +1407,15 @@ class CSR(implicit p: Parameters) extends FunctionUnit with HasCSRConst with PMP
       mstatusNew.ie.s := false.B
       priviledgeMode := ModeS
       when (clearTval) { stval := 0.U }
+      when (clearTval_h) {htval := 0.U}
     }.otherwise {
+      val virt = Mux(mstatusOld.mprv.asBool(), mstatusOld.mpv, virtMode)
+      // to do hld st
+      mstatusNew.gva := (hasInstGuestPageFault || hasLoadGuestPageFault || hasStoreGuestPageFault ||
+      ((virt.asBool() || isHyperInst) && ((raiseException && 0.U <= exceptionNO && exceptionNO <= 7.U && exceptionNO =/= 2.U)
+        || hasInstrPageFault || hasLoadPageFault || hasStorePageFault)))
+      mstatusNew.mpv := virtMode
+      virtMode := false.B
       mcause := causeNO
       mepc := Mux(hasInstrPageFault || hasInstrAccessFault, iexceptionPC, dexceptionPC)
       mstatusNew.mpp := priviledgeMode
@@ -1158,8 +1423,11 @@ class CSR(implicit p: Parameters) extends FunctionUnit with HasCSRConst with PMP
       mstatusNew.ie.m := false.B
       priviledgeMode := ModeM
       when (clearTval) { mtval := 0.U }
+      when (clearTval_h) {mtval2 := 0.U}
     }
     mstatus := mstatusNew.asUInt
+    vsstatus := vsstatusNew.asUInt
+    hstatus := hstatusNew.asUInt
     debugMode := debugModeNew
   }
 
@@ -1247,6 +1515,29 @@ class CSR(implicit p: Parameters) extends FunctionUnit with HasCSRConst with PMP
     difftest.io.mideleg := mideleg
     difftest.io.medeleg := medeleg
   }
+  if (env.AlwaysBasicDiff || env.EnableDifftest) {
+    val difftest = Module(new DifftestHCSRState)
+    difftest.io.clock := clock
+    difftest.io.coreid := csrio.hartId
+    difftest.io.virtMode := virtMode
+    difftest.io.mtval2 := mtval2
+    difftest.io.mtinst := mtinst
+    difftest.io.hstatus := hstatus
+    difftest.io.hideleg := hideleg
+    difftest.io.hedeleg := hedeleg
+    difftest.io.hcounteren := hcounteren
+    difftest.io.htval := htval
+    difftest.io.htinst := htinst
+    difftest.io.hgatp := hgatp
+    difftest.io.vsstatus := vsstatus
+    difftest.io.vstvec := vstvec
+    difftest.io.vsepc := vsepc
+    difftest.io.vscause := vscause
+    difftest.io.vstval := vstval
+    difftest.io.vsatp := vsatp
+    difftest.io.vsscratch := vsscratch
+  }
+
 
   if(env.AlwaysBasicDiff || env.EnableDifftest) {
     val difftest = Module(new DifftestDebugMode)
diff --git a/src/main/scala/xiangshan/backend/fu/Fence.scala b/src/main/scala/xiangshan/backend/fu/Fence.scala
index c13eb80d4..d60bcb381 100644
--- a/src/main/scala/xiangshan/backend/fu/Fence.scala
+++ b/src/main/scala/xiangshan/backend/fu/Fence.scala
@@ -21,8 +21,7 @@ import chisel3._
 import chisel3.util._
 import xiangshan._
 import utils._
-import utility._
-import xiangshan.ExceptionNO.illegalInstr
+import xiangshan.ExceptionNO.{illegalInstr, virtualInstr}
 
 class FenceToSbuffer extends Bundle {
   val flushSb = Output(Bool())
@@ -35,7 +34,9 @@ class Fence(implicit p: Parameters) extends FunctionUnit {
   val fencei = IO(Output(Bool()))
   val toSbuffer = IO(new FenceToSbuffer)
   val disableSfence = IO(Input(Bool()))
-
+  val disableHfenceg = IO(Input(Bool()))
+  val disableHfencev = IO(Input(Bool()))
+  val virtMode = IO(Input(Bool()))
   val (valid, src1) = (
     io.in.valid,
     io.in.bits.src(0)
@@ -61,10 +62,12 @@ class Fence(implicit p: Parameters) extends FunctionUnit {
   // NOTE: icache & tlb & sbuffer must receive flush signal at any time
   sbuffer      := state === s_wait && !(func === FenceOpType.sfence && disableSfence)
   fencei       := state === s_icache
-  sfence.valid := state === s_tlb && !disableSfence
+  sfence.valid := state === s_tlb && ((!disableSfence && func === FenceOpType.sfence) || (!disableHfencev && func === FenceOpType.hfence_v) || (!disableHfenceg && func === FenceOpType.hfence_g))
   sfence.bits.rs1  := uop.ctrl.imm(4, 0) === 0.U
   sfence.bits.rs2  := uop.ctrl.imm(9, 5) === 0.U
   sfence.bits.flushPipe := uop.ctrl.flushPipe
+  sfence.bits.hv := !disableHfencev && func === FenceOpType.hfence_v
+  sfence.bits.hg := !disableHfenceg && func === FenceOpType.hfence_g
   XSError(sfence.valid && uop.ctrl.lsrc(0) =/= uop.ctrl.imm(4, 0), "lsrc0 is passed by imm\n")
   XSError(sfence.valid && uop.ctrl.lsrc(1) =/= uop.ctrl.imm(9, 5), "lsrc1 is passed by imm\n")
   sfence.bits.addr := RegEnable(io.in.bits.src(0), io.in.fire())
@@ -72,7 +75,9 @@ class Fence(implicit p: Parameters) extends FunctionUnit {
 
   when (state === s_idle && io.in.valid) { state := s_wait }
   when (state === s_wait && func === FenceOpType.fencei && sbEmpty) { state := s_icache }
-  when (state === s_wait && func === FenceOpType.sfence && (sbEmpty || disableSfence)) { state := s_tlb }
+  when (state === s_wait && ((func === FenceOpType.sfence && (sbEmpty || disableSfence))
+    || (func === FenceOpType.hfence_g && (sbEmpty || disableHfenceg))
+    || (func === FenceOpType.hfence_v && (sbEmpty || disableHfencev)))) { state := s_tlb }
   when (state === s_wait && func === FenceOpType.fence  && sbEmpty) { state := s_fence }
   when (state === s_wait && func === FenceOpType.nofence  && sbEmpty) { state := s_nofence }
   when (state =/= s_idle && state =/= s_wait) { state := s_idle }
@@ -81,7 +86,11 @@ class Fence(implicit p: Parameters) extends FunctionUnit {
   io.out.valid := state =/= s_idle && state =/= s_wait
   io.out.bits.data := DontCare
   io.out.bits.uop := uop
-  io.out.bits.uop.cf.exceptionVec(illegalInstr) := func === FenceOpType.sfence && disableSfence
+  val illegalsfence = func === FenceOpType.sfence && disableSfence
+  val illegalhfenceg = func === FenceOpType.hfence_g && disableHfenceg
+  val illegalhfencev = func === FenceOpType.hfence_v && disableHfencev
+  io.out.bits.uop.cf.exceptionVec(illegalInstr) := (illegalsfence || illegalhfenceg || illegalhfencev) && !virtMode
+  io.out.bits.uop.cf.exceptionVec(virtualInstr) := (illegalsfence || illegalhfenceg || illegalhfencev) && virtMode
 
   XSDebug(io.in.valid, p"In(${io.in.valid} ${io.in.ready}) state:${state} Inpc:0x${Hexadecimal(io.in.bits.uop.cf.pc)} InrobIdx:${io.in.bits.uop.robIdx}\n")
   XSDebug(state =/= s_idle, p"state:${state} sbuffer(flush:${sbuffer} empty:${sbEmpty}) fencei:${fencei} sfence:${sfence}\n")
diff --git a/src/main/scala/xiangshan/backend/fu/util/CSRConst.scala b/src/main/scala/xiangshan/backend/fu/util/CSRConst.scala
index a75d30854..00220bb0f 100644
--- a/src/main/scala/xiangshan/backend/fu/util/CSRConst.scala
+++ b/src/main/scala/xiangshan/backend/fu/util/CSRConst.scala
@@ -78,6 +78,41 @@ trait HasCSRConst {
 
   val Sdsid         = 0x9C0
 
+  // Hypervisor Trap Setup
+  val Hstatus       = 0x600
+  val Hedeleg       = 0x602
+  val Hideleg       = 0x603
+  val Hie           = 0x604
+  val Hcounteren    = 0x606
+  val Hgeie         = 0x607
+
+  // Hypervisor Trap Handling
+  val Htval         = 0x643
+  val Hip           = 0x644
+  val Hvip          = 0x645
+  val Htinst        = 0x64A
+  val Hgeip         = 0xE12
+
+  // Hypervisor Configuration
+  val Henvcfg       = 0x60A
+
+  // Hypervisor Protection and Translation
+  val Hgatp         = 0x680
+
+  //Hypervisor Counter/Timer Virtualization Registers
+  val Htimedelta    = 0x605
+
+  // Virtual Supervisor Registers
+  val Vsstatus      = 0x200
+  val Vsie          = 0x204
+  val Vstvec        = 0x205
+  val Vsscratch     = 0x240
+  val Vsepc         = 0x241
+  val Vscause       = 0x242
+  val Vstval        = 0x243
+  val Vsip          = 0x244
+  val Vsatp         = 0x280
+
   // Machine Information Registers
   val Mvendorid     = 0xF11
   val Marchid       = 0xF12
@@ -100,6 +135,8 @@ trait HasCSRConst {
   val Mcause        = 0x342
   val Mtval         = 0x343
   val Mip           = 0x344
+  val Mtinst        = 0x34A
+  val Mtval2        = 0x34B
 
   // Machine Memory Protection
   // TBD
@@ -203,19 +240,27 @@ trait HasCSRConst {
   def ModeS     = 0x1.U
   def ModeU     = 0x0.U
 
-  def IRQ_UEIP  = 0
-  def IRQ_SEIP  = 1
-  def IRQ_MEIP  = 3
+  def IRQ_USIP  = 0
+  def IRQ_SSIP  = 1
+  def IRQ_VSSIP = 2
+  def IRQ_MSIP  = 3
 
   def IRQ_UTIP  = 4
   def IRQ_STIP  = 5
+  def IRQ_VSTIP = 6
   def IRQ_MTIP  = 7
 
-  def IRQ_USIP  = 8
-  def IRQ_SSIP  = 9
-  def IRQ_MSIP  = 11
+  def IRQ_UEIP  = 8
+  def IRQ_SEIP  = 9
+  def IRQ_VSEIP = 10
+  def IRQ_MEIP  = 11
+
+  def IRQ_SGEIP = 12
+  def IRQ_DEBUG = 13
 
-  def IRQ_DEBUG = 12
+  val Hgatp_Mode_len = 4
+  val Hgatp_Vmid_len = 14
+  val Hgatp_Addr_len = 44
 
   val Satp_Mode_len = 4
   val Satp_Asid_len = 16
@@ -229,13 +274,29 @@ trait HasCSRConst {
     IRQ_DEBUG,
     IRQ_MEIP, IRQ_MSIP, IRQ_MTIP,
     IRQ_SEIP, IRQ_SSIP, IRQ_STIP,
-    IRQ_UEIP, IRQ_USIP, IRQ_UTIP
+    IRQ_UEIP, IRQ_USIP, IRQ_UTIP,
+    IRQ_VSEIP, IRQ_VSSIP, IRQ_VSTIP, IRQ_SGEIP
   )
 
-  def csrAccessPermissionCheck(addr: UInt, wen: Bool, mode: UInt): Bool = {
-    val readOnly = addr(11,10) === "b11".U
+  def csrAccessPermissionCheck(addr: UInt, wen: Bool, mode: UInt, virt: Bool, hasH: Bool): UInt = {
+    val readOnly = addr(11, 10) === "b11".U
     val lowestAccessPrivilegeLevel = addr(9,8)
-    mode >= lowestAccessPrivilegeLevel && !(wen && readOnly)
+    val priv = Mux(mode === ModeS, ModeH, mode)
+    val ret = Wire(Bool()) //0.U: normal, 1.U: illegal_instruction, 2.U: virtual instruction
+    when (lowestAccessPrivilegeLevel === ModeH && !hasH){
+      ret := 1.U
+    }.elsewhen (readOnly && wen) {
+      ret := 1.U
+    }.elsewhen (priv < lowestAccessPrivilegeLevel) {
+      when(virt && lowestAccessPrivilegeLevel <= ModeH){
+        ret := 2.U
+      }.otherwise{
+        ret := 1.U
+      }
+    }.otherwise{
+      ret := 0.U
+    }
+    ret
   }
 
   def perfcntPermissionCheck(addr: UInt, mode: UInt, mmask: UInt, smask: UInt): Bool = {
diff --git a/src/main/scala/xiangshan/cache/mmu/L2TLB.scala b/src/main/scala/xiangshan/cache/mmu/L2TLB.scala
index d944b19de..8260a590f 100644
--- a/src/main/scala/xiangshan/cache/mmu/L2TLB.scala
+++ b/src/main/scala/xiangshan/cache/mmu/L2TLB.scala
@@ -43,10 +43,8 @@ class L2TLB()(implicit p: Parameters) extends LazyModule with HasPtwConst {
 
   lazy val module = new L2TLBImp(this)
 }
-
-@chiselName
+@chiselName // just for temp
 class L2TLBImp(outer: L2TLB)(implicit p: Parameters) extends PtwModule(outer) with HasCSRConst with HasPerfEvents {
-
   val (mem, edge) = outer.node.out.head
 
   val io = IO(new L2TLBIO)
@@ -56,209 +54,102 @@ class L2TLBImp(outer: L2TLB)(implicit p: Parameters) extends PtwModule(outer) wi
     val ptwData = Output(Vec(4, UInt(64.W)))
   })
 
-  /* Ptw processes multiple requests
-   * Divide Ptw procedure into two stages: cache access ; mem access if cache miss
-   *           miss queue itlb       dtlb
-   *               |       |         |
-   *               ------arbiter------
-   *                            |
-   *                    l1 - l2 - l3 - sp
-   *                            |
-   *          -------------------------------------------
-   *    miss  |  queue                                  | hit
-   *    [][][][][][]                                    |
-   *          |                                         |
-   *    state machine accessing mem                     |
-   *          |                                         |
-   *          ---------------arbiter---------------------
-   *                 |                    |
-   *                itlb                 dtlb
-   */
-
   difftestIO <> DontCare
 
   val sfence_tmp = DelayN(io.sfence, 1)
-  val csr_tmp    = DelayN(io.csr.tlb, 1)
+  val csr_tmp = DelayN(io.csr.tlb, 1)
   val sfence_dup = Seq.fill(8)(RegNext(sfence_tmp))
   val csr_dup = Seq.fill(7)(RegNext(csr_tmp))
-  val satp   = csr_dup(0).satp
-  val priv   = csr_dup(0).priv
-  val flush  = sfence_dup(0).valid || satp.changed
+  val satp = csr_dup(0).satp
+  val priv = csr_dup(0).priv
+  val flush = sfence_dup(0).valid || satp.changed
 
   val pmp = Module(new PMP())
-  val pmp_check = VecInit(Seq.fill(2)(Module(new PMPChecker(lgMaxSize = 3, sameCycle = true)).io))
+  val pmp_check = VecInit(Seq.fill(2)(Module(new PMPChecker(lgMaxSize = 2, sameCycle = true)).io))
   pmp.io.distribute_csr := io.csr.distribute_csr
   pmp_check.foreach(_.check_env.apply(ModeS, pmp.io.pmp, pmp.io.pma))
 
-  val missQueue = Module(new L2TlbMissQueue)
-  val cache = Module(new PtwCache)
-  val ptw = Module(new PTW)
-  val llptw = Module(new LLPTW)
-  val blockmq = Module(new BlockHelper(3))
   val arb1 = Module(new Arbiter(new PtwReq, PtwWidth))
-  val arb2 = Module(new Arbiter(new Bundle {
-    val vpn = UInt(vpnLen.W)
-    val source = UInt(bSourceWidth.W)
-  }, if (l2tlbParams.enablePrefetch) 4 else 3))
-  val outArb = (0 until PtwWidth).map(i => Module(new Arbiter(new PtwResp, 3)).io)
-  val outArbCachePort = 0
-  val outArbFsmPort = 1
-  val outArbMqPort = 2
-
-  // arb2 input port
-  val InArbPTWPort = 0
-  val InArbMissQueuePort = 1
-  val InArbTlbPort = 2
-  val InArbPrefetchPort = 3
-  // NOTE: when cache out but miss and ptw doesnt accept,
+  val ptw = Module(new PTW)
+  val hptw = Module(new HPTW)
+
   arb1.io.in <> VecInit(io.tlb.map(_.req(0)))
-  arb1.io.out.ready := arb2.io.in(InArbTlbPort).ready
-
-  arb2.io.in(InArbPTWPort).valid := ptw.io.llptw.valid
-  arb2.io.in(InArbPTWPort).bits.vpn := ptw.io.llptw.bits.req_info.vpn
-  arb2.io.in(InArbPTWPort).bits.source := ptw.io.llptw.bits.req_info.source
-  ptw.io.llptw.ready := arb2.io.in(InArbPTWPort).ready
-  block_decoupled(missQueue.io.out, arb2.io.in(InArbMissQueuePort), !ptw.io.req.ready)
-
-  arb2.io.in(InArbTlbPort).valid := arb1.io.out.valid
-  arb2.io.in(InArbTlbPort).bits.vpn := arb1.io.out.bits.vpn
-  arb2.io.in(InArbTlbPort).bits.source := arb1.io.chosen
-  if (l2tlbParams.enablePrefetch) {
-    val prefetch = Module(new L2TlbPrefetch())
-    val recv = cache.io.resp
-    // NOTE: 1. prefetch doesn't gen prefetch 2. req from mq doesn't gen prefetch
-    // NOTE: 1. miss req gen prefetch 2. hit but prefetched gen prefetch
-    prefetch.io.in.valid := recv.fire() && !from_pre(recv.bits.req_info.source) && (!recv.bits.hit  ||
-      recv.bits.prefetch) && recv.bits.isFirst
-    prefetch.io.in.bits.vpn := recv.bits.req_info.vpn
-    prefetch.io.sfence := sfence_dup(0)
-    prefetch.io.csr := csr_dup(0)
-    arb2.io.in(InArbPrefetchPort) <> prefetch.io.out
-
-    val L2TlbPrefetchTable = ChiselDB.createTable("L2TlbPrefetch_hart" + p(XSCoreParamsKey).HartId.toString, new L2TlbPrefetchDB)
-    val L2TlbPrefetchDB = Wire(new L2TlbPrefetchDB)
-    L2TlbPrefetchDB.vpn := prefetch.io.out.bits.vpn
-    L2TlbPrefetchTable.log(L2TlbPrefetchDB, prefetch.io.out.fire, "L2TlbPrefetch", clock, reset)
-  }
-  arb2.io.out.ready := cache.io.req.ready
-
-
-  val mq_arb = Module(new Arbiter(new L2TlbInnerBundle, 2))
-  mq_arb.io.in(0).valid := cache.io.resp.valid && !cache.io.resp.bits.hit &&
-    (!cache.io.resp.bits.toFsm.l2Hit || cache.io.resp.bits.bypassed) &&
-    !from_pre(cache.io.resp.bits.req_info.source) &&
-    (cache.io.resp.bits.bypassed || cache.io.resp.bits.isFirst || !ptw.io.req.ready)
-  mq_arb.io.in(0).bits :=  cache.io.resp.bits.req_info
-  mq_arb.io.in(1) <> llptw.io.cache
-  missQueue.io.in <> mq_arb.io.out
-  missQueue.io.sfence  := sfence_dup(6)
-  missQueue.io.csr := csr_dup(5)
-
-  blockmq.io.start := missQueue.io.out.fire
-  blockmq.io.enable := ptw.io.req.fire()
-
-  llptw.io.in.valid := cache.io.resp.valid && !cache.io.resp.bits.hit && cache.io.resp.bits.toFsm.l2Hit && !cache.io.resp.bits.bypassed
-  llptw.io.in.bits.req_info := cache.io.resp.bits.req_info
-  llptw.io.in.bits.ppn := cache.io.resp.bits.toFsm.ppn
-  llptw.io.sfence := sfence_dup(1)
-  llptw.io.csr := csr_dup(1)
-
-  cache.io.req.valid := arb2.io.out.valid
-  cache.io.req.bits.req_info.vpn := arb2.io.out.bits.vpn
-  cache.io.req.bits.req_info.source := arb2.io.out.bits.source
-  cache.io.req.bits.isFirst := arb2.io.chosen =/= InArbMissQueuePort.U
-  cache.io.req.bits.bypassed.map(_ := false.B)
-  cache.io.sfence := sfence_dup(2)
-  cache.io.csr := csr_dup(2)
-  cache.io.sfence_dup.zip(sfence_dup.drop(2).take(4)).map(s => s._1 := s._2)
-  cache.io.csr_dup.zip(csr_dup.drop(2).take(3)).map(c => c._1 := c._2)
-  cache.io.resp.ready := Mux(cache.io.resp.bits.hit,
-    outReady(cache.io.resp.bits.req_info.source, outArbCachePort),
-    Mux(cache.io.resp.bits.toFsm.l2Hit && !cache.io.resp.bits.bypassed, llptw.io.in.ready,
-    Mux(cache.io.resp.bits.bypassed || cache.io.resp.bits.isFirst, mq_arb.io.in(0).ready, mq_arb.io.in(0).ready || ptw.io.req.ready)))
-
-  // NOTE: missQueue req has higher priority
-  ptw.io.req.valid := cache.io.resp.valid && !cache.io.resp.bits.hit && !cache.io.resp.bits.toFsm.l2Hit &&
-    !cache.io.resp.bits.bypassed &&
-    !cache.io.resp.bits.isFirst
-  ptw.io.req.bits.req_info := cache.io.resp.bits.req_info
-  ptw.io.req.bits.l1Hit := cache.io.resp.bits.toFsm.l1Hit
-  ptw.io.req.bits.ppn := cache.io.resp.bits.toFsm.ppn
+  arb1.io.out.ready := ptw.io.req.ready
+
+  ptw.io.req.valid := arb1.io.out.valid
+  ptw.io.req.bits.req_info.vpn := arb1.io.out.bits.vpn
+  ptw.io.req.bits.ppn := DontCare
+  ptw.io.req.bits.req_info.gvpn := arb1.io.out.bits.gvpn
+  ptw.io.req.bits.req_info.source := arb1.io.chosen
+  ptw.io.req.bits.req_info.hyperinst := arb1.io.out.bits.hyperinst
+  ptw.io.req.bits.req_info.virt := arb1.io.out.bits.virt
+  ptw.io.req.bits.req_info.hlvx := arb1.io.out.bits.hlvx
+  ptw.io.req.bits.req_info.cmd := arb1.io.out.bits.cmd
+  ptw.io.req.bits.l1Hit := false.B
+  ptw.io.llptw.ready := DontCare
   ptw.io.sfence := sfence_dup(7)
   ptw.io.csr := csr_dup(6)
-  ptw.io.resp.ready := outReady(ptw.io.resp.bits.source, outArbFsmPort)
+  ptw.io.resp.ready := MuxLookup(ptw.io.resp.bits.source, true.B, (0 until PtwWidth).map(i => i.U -> io.tlb(i).resp.ready))
+
 
   // mem req
   def blockBytes_align(addr: UInt) = {
     Cat(addr(PAddrBits - 1, log2Up(l2tlbParams.blockBytes)), 0.U(log2Up(l2tlbParams.blockBytes).W))
   }
+
   def addr_low_from_vpn(vpn: UInt) = {
-    vpn(log2Ceil(l2tlbParams.blockBytes)-log2Ceil(XLEN/8)-1, 0)
+    vpn(log2Ceil(l2tlbParams.blockBytes) - log2Ceil(XLEN / 8) - 1, 0)
   }
+
   def addr_low_from_paddr(paddr: UInt) = {
-    paddr(log2Up(l2tlbParams.blockBytes)-1, log2Up(XLEN/8))
+    paddr(log2Up(l2tlbParams.blockBytes) - 1, log2Up(XLEN / 8))
   }
-  def from_missqueue(id: UInt) = {
-    (id =/= l2tlbParams.llptwsize.U)
+
+  def from_ptw(id: UInt) = {
+    id === FsmReqID.U
+  }
+
+  def from_hptw(id: UInt) = {
+    id === HptwReqID.U
   }
+
   val waiting_resp = RegInit(VecInit(Seq.fill(MemReqWidth)(false.B)))
   val flush_latch = RegInit(VecInit(Seq.fill(MemReqWidth)(false.B)))
   for (i <- waiting_resp.indices) {
     assert(!flush_latch(i) || waiting_resp(i)) // when sfence_latch wait for mem resp, waiting_resp should be true
   }
-
-  val llptw_out = llptw.io.out
-  val llptw_mem = llptw.io.mem
-  llptw_mem.req_mask := waiting_resp.take(l2tlbParams.llptwsize)
-  ptw.io.mem.mask := waiting_resp.last
-
+  ptw.io.mem.mask := waiting_resp.take(l2tlbParams.llptwsize + 1).last
+  hptw.io.mem.mask := waiting_resp.take(l2tlbParams.llptwsize + 2).last
   val mem_arb = Module(new Arbiter(new L2TlbMemReqBundle(), 2))
   mem_arb.io.in(0) <> ptw.io.mem.req
-  mem_arb.io.in(1) <> llptw_mem.req
+  mem_arb.io.in(1) <> hptw.io.mem.req
   mem_arb.io.out.ready := mem.a.ready && !flush
 
-  // assert, should not send mem access at same addr for twice.
-  val last_resp_vpn = RegEnable(cache.io.refill.bits.req_info_dup(0).vpn, cache.io.refill.valid)
-  val last_resp_level = RegEnable(cache.io.refill.bits.level_dup(0), cache.io.refill.valid)
-  val last_resp_v = RegInit(false.B)
-  val last_has_invalid = !Cat(cache.io.refill.bits.ptes.asTypeOf(Vec(blockBits/XLEN, UInt(XLEN.W))).map(a => a(0))).andR || cache.io.refill.bits.sel_pte_dup(0).asTypeOf(new PteBundle).isAf()
-  when (cache.io.refill.valid) { last_resp_v := !last_has_invalid}
-  when (flush) { last_resp_v := false.B }
-  XSError(last_resp_v && cache.io.refill.valid &&
-    (cache.io.refill.bits.req_info_dup(0).vpn === last_resp_vpn) &&
-    (cache.io.refill.bits.level_dup(0) === last_resp_level),
-    "l2tlb should not access mem at same addr for twice")
-  // ATTENTION: this may wronngly assert when: a ptes is l2, last part is valid,
-  // but the current part is invalid, so one more mem access happened
-  // If this happened, remove the assert.
-
   val req_addr_low = Reg(Vec(MemReqWidth, UInt((log2Up(l2tlbParams.blockBytes)-log2Up(XLEN/8)).W)))
 
-  when (llptw.io.in.fire()) {
-    // when enq miss queue, set the req_addr_low to receive the mem resp data part
-    req_addr_low(llptw_mem.enq_ptr) := addr_low_from_vpn(llptw.io.in.bits.req_info.vpn)
-  }
-  when (mem_arb.io.out.fire()) {
+  when(mem_arb.io.out.fire()) {
     req_addr_low(mem_arb.io.out.bits.id) := addr_low_from_paddr(mem_arb.io.out.bits.addr)
     waiting_resp(mem_arb.io.out.bits.id) := true.B
   }
   // mem read
-  val memRead =  edge.Get(
+  val memRead = edge.Get(
     fromSource = mem_arb.io.out.bits.id,
     // toAddress  = memAddr(log2Up(CacheLineSize / 2 / 8) - 1, 0),
-    toAddress  = blockBytes_align(mem_arb.io.out.bits.addr),
-    lgSize     = log2Up(l2tlbParams.blockBytes).U
+    toAddress = blockBytes_align(mem_arb.io.out.bits.addr),
+    lgSize = log2Up(l2tlbParams.blockBytes).U
   )._2
   mem.a.bits := memRead
   mem.a.valid := mem_arb.io.out.valid && !flush
   mem.d.ready := true.B
+
   // mem -> data buffer
   val refill_data = Reg(Vec(blockBits / l1BusDataWidth, UInt(l1BusDataWidth.W)))
   val refill_helper = edge.firstlastHelper(mem.d.bits, mem.d.fire())
   val mem_resp_done = refill_helper._3
-  val mem_resp_from_mq = from_missqueue(mem.d.bits.source)
+  val mem_resp_from_ptw = from_ptw(mem.d.bits.source)
+  val mem_resp_from_hptw = from_hptw(mem.d.bits.source)
   when (mem.d.valid) {
-    assert(mem.d.bits.source <= l2tlbParams.llptwsize.U)
+    assert(mem.d.bits.source <= (l2tlbParams.llptwsize + 1).U)
     refill_data(refill_helper._4) := mem.d.bits.data
   }
   // refill_data_tmp is the wire fork of refill_data, but one cycle earlier
@@ -268,33 +159,29 @@ class L2TLBImp(outer: L2TLB)(implicit p: Parameters) extends PtwModule(outer) wi
   // save only one pte for each id
   // (miss queue may can't resp to tlb with low latency, it should have highest priority, but diffcult to design cache)
   val resp_pte = VecInit((0 until MemReqWidth).map(i =>
-    if (i == l2tlbParams.llptwsize) {RegEnable(get_part(refill_data_tmp, req_addr_low(i)), mem_resp_done && !mem_resp_from_mq) }
-    else { DataHoldBypass(get_part(refill_data, req_addr_low(i)), llptw_mem.buffer_it(i)) }
-    // llptw could not use refill_data_tmp, because enq bypass's result works at next cycle
+    if (i == FsmReqID) {RegEnable(get_part(refill_data_tmp, req_addr_low(i)), mem_resp_done  && !mem_resp_from_hptw && mem_resp_from_ptw) }
+    else if( i == HptwReqID) {RegEnable(get_part(refill_data_tmp, req_addr_low(i)), mem_resp_done && !mem_resp_from_ptw && mem_resp_from_hptw)}
+    else {DontCare}
   ))
 
-  // mem -> miss queue
-  llptw_mem.resp.valid := mem_resp_done && mem_resp_from_mq
-  llptw_mem.resp.bits.id := DataHoldBypass(mem.d.bits.source, mem.d.valid)
   // mem -> ptw
   ptw.io.mem.req.ready := mem.a.ready
-  ptw.io.mem.resp.valid := mem_resp_done && !mem_resp_from_mq
-  ptw.io.mem.resp.bits := resp_pte.last
-  // mem -> cache
-  val refill_from_mq = mem_resp_from_mq
-  val refill_level = Mux(refill_from_mq, 2.U, RegEnable(ptw.io.refill.level, init = 0.U, ptw.io.mem.req.fire()))
-  val refill_valid = mem_resp_done && !flush && !flush_latch(mem.d.bits.source)
-
-  cache.io.refill.valid := RegNext(refill_valid, false.B)
-  cache.io.refill.bits.ptes := refill_data.asUInt
-  cache.io.refill.bits.req_info_dup.map(_ := RegEnable(Mux(refill_from_mq, llptw_mem.refill, ptw.io.refill.req_info), refill_valid))
-  cache.io.refill.bits.level_dup.map(_ := RegEnable(refill_level, refill_valid))
-  cache.io.refill.bits.levelOH(refill_level, refill_valid)
-  cache.io.refill.bits.sel_pte_dup.map(_ := RegNext(sel_data(refill_data_tmp.asUInt, req_addr_low(mem.d.bits.source))))
+  ptw.io.mem.resp.valid := mem_resp_done && mem_resp_from_ptw
+  ptw.io.mem.resp.bits := resp_pte.take(l2tlbParams.llptwsize + 1).last
+  // hptw <> ptw
+  hptw.io.ptw <> ptw.io.hptw
+  hptw.io.llptw := DontCare
+  hptw.io.pageCache := DontCare
+  hptw.io.mem.req.ready := mem.a.ready
+  hptw.io.mem.resp.valid := mem_resp_done && mem_resp_from_hptw
+  hptw.io.mem.resp.bits := resp_pte.take(l2tlbParams.llptwsize + 2).last
+  hptw.io.csr := DontCare
+  hptw.io.sfence := DontCare
 
+  val refill_valid = mem_resp_done && !flush && !flush_latch(mem.d.bits.source)
   if (env.EnableDifftest) {
     val difftest_ptw_addr = RegInit(VecInit(Seq.fill(MemReqWidth)(0.U(PAddrBits.W))))
-    when (mem.a.valid) {
+    when(mem.a.valid) {
       difftest_ptw_addr(mem.a.bits.source) := mem.a.bits.address
     }
 
@@ -302,7 +189,7 @@ class L2TLBImp(outer: L2TLB)(implicit p: Parameters) extends PtwModule(outer) wi
     difftest.io.clock := clock
     difftest.io.coreid := p(XSCoreParamsKey).HartId.asUInt
     difftest.io.cacheid := 2.U
-    difftest.io.valid := cache.io.refill.valid
+    difftest.io.valid := RegNext(refill_valid, false.B)
     difftest.io.addr := difftest_ptw_addr(RegNext(mem.d.bits.source))
     difftest.io.data := refill_data.asTypeOf(difftest.io.data)
   }
@@ -314,7 +201,11 @@ class L2TLBImp(outer: L2TLB)(implicit p: Parameters) extends PtwModule(outer) wi
       difftest.io.coreid := p(XSCoreParamsKey).HartId.asUInt
       difftest.io.valid := io.tlb(i).resp.fire && !io.tlb(i).resp.bits.af
       difftest.io.index := i.U
-      difftest.io.satp := io.csr.tlb.satp.ppn
+      difftest.io.satp := Cat(io.csr.tlb.satp.mode, io.csr.tlb.satp.asid, io.csr.tlb.satp.ppn)
+      difftest.io.vsatp := Cat(io.csr.tlb.vsatp.mode, io.csr.tlb.vsatp.asid, io.csr.tlb.vsatp.ppn)
+      difftest.io.hgatp := Cat(io.csr.tlb.hgatp.mode, io.csr.tlb.hgatp.asid, io.csr.tlb.hgatp.ppn)
+      difftest.io.s2xlate := io.tlb(i).resp.bits.entry.s2xlate
+      difftest.io.hlvx := ptw.io.resp.bits.hlvx
       difftest.io.vpn := io.tlb(i).resp.bits.entry.tag
       difftest.io.ppn := io.tlb(i).resp.bits.entry.ppn
       difftest.io.perm := io.tlb(i).resp.bits.entry.perm.getOrElse(0.U.asTypeOf(new PtePermBundle)).asUInt
@@ -322,95 +213,45 @@ class L2TLBImp(outer: L2TLB)(implicit p: Parameters) extends PtwModule(outer) wi
       difftest.io.pf := io.tlb(i).resp.bits.pf
     }
   }
-
   // pmp
   pmp_check(0).req <> ptw.io.pmp.req
   ptw.io.pmp.resp <> pmp_check(0).resp
-  pmp_check(1).req <> llptw.io.pmp.req
-  llptw.io.pmp.resp <> pmp_check(1).resp
+  pmp_check(1).req <> hptw.io.pmp.req
+  hptw.io.pmp.resp <> pmp_check(1).resp
 
-  llptw_out.ready := outReady(llptw_out.bits.req_info.source, outArbMqPort)
   for (i <- 0 until PtwWidth) {
-    outArb(i).in(outArbCachePort).valid := cache.io.resp.valid && cache.io.resp.bits.hit && cache.io.resp.bits.req_info.source===i.U
-    outArb(i).in(outArbCachePort).bits.entry := cache.io.resp.bits.toTlb
-    outArb(i).in(outArbCachePort).bits.pf := !cache.io.resp.bits.toTlb.v
-    outArb(i).in(outArbCachePort).bits.af := false.B
-    outArb(i).in(outArbFsmPort).valid := ptw.io.resp.valid && ptw.io.resp.bits.source===i.U
-    outArb(i).in(outArbFsmPort).bits := ptw.io.resp.bits.resp
-    outArb(i).in(outArbMqPort).valid := llptw_out.valid && llptw_out.bits.req_info.source===i.U
-    outArb(i).in(outArbMqPort).bits := pte_to_ptwResp(resp_pte(llptw_out.bits.id), llptw_out.bits.req_info.vpn, llptw_out.bits.af, true)
-  }
-
-  // io.tlb.map(_.resp) <> outArb.map(_.out)
-  io.tlb.map(_.resp).zip(outArb.map(_.out)).map{
-    case (resp, out) => resp <> out
+    io.tlb(i).resp.valid := ptw.io.resp.valid && ptw.io.resp.bits.source === i.U
+    io.tlb(i).resp.bits := ptw.io.resp.bits.resp
   }
-
   // sfence
-  when (flush) {
+  when(flush) {
     for (i <- 0 until MemReqWidth) {
-      when (waiting_resp(i)) {
+      when(waiting_resp(i)) {
         flush_latch(i) := true.B
       }
     }
   }
   // mem -> control signal
   // waiting_resp and sfence_latch will be reset when mem_resp_done
-  when (mem_resp_done) {
+  when(mem_resp_done) {
     waiting_resp(mem.d.bits.source) := false.B
     flush_latch(mem.d.bits.source) := false.B
   }
-
-  def block_decoupled[T <: Data](source: DecoupledIO[T], sink: DecoupledIO[T], block_signal: Bool) = {
-    sink.valid   := source.valid && !block_signal
-    source.ready := sink.ready   && !block_signal
-    sink.bits    := source.bits
-  }
-
   def get_part(data: Vec[UInt], index: UInt): UInt = {
     val inner_data = data.asTypeOf(Vec(data.getWidth / XLEN, UInt(XLEN.W)))
     inner_data(index)
   }
 
-  def pte_to_ptwResp(pte: UInt, vpn: UInt, af: Bool, af_first: Boolean) : PtwResp = {
-    val pte_in = pte.asTypeOf(new PteBundle())
-    val ptw_resp = Wire(new PtwResp())
-    ptw_resp.entry.ppn := pte_in.ppn
-    ptw_resp.entry.level.map(_ := 2.U)
-    ptw_resp.entry.perm.map(_ := pte_in.getPerm())
-    ptw_resp.entry.tag := vpn
-    ptw_resp.pf := (if (af_first) !af else true.B) && pte_in.isPf(2.U)
-    ptw_resp.af := (if (!af_first) pte_in.isPf(2.U) else true.B) && (af || pte_in.isAf())
-    ptw_resp.entry.v := !ptw_resp.pf
-    ptw_resp.entry.prefetch := DontCare
-    ptw_resp.entry.asid := satp.asid
-    ptw_resp
-  }
-
-  def outReady(source: UInt, port: Int): Bool = {
-    MuxLookup(source, true.B,
-      (0 until PtwWidth).map(i => i.U -> outArb(i).in(port).ready))
-  }
-
-  // debug info
-  for (i <- 0 until PtwWidth) {
-    XSDebug(p"[io.tlb(${i.U})] ${io.tlb(i)}\n")
-  }
-  XSDebug(p"[sfence] ${io.sfence}\n")
-  XSDebug(p"[io.csr.tlb] ${io.csr.tlb}\n")
-
   for (i <- 0 until PtwWidth) {
     XSPerfAccumulate(s"req_count${i}", io.tlb(i).req(0).fire())
     XSPerfAccumulate(s"req_blocked_count_${i}", io.tlb(i).req(0).valid && !io.tlb(i).req(0).ready)
   }
-  XSPerfAccumulate(s"req_blocked_by_mq", arb1.io.out.valid && missQueue.io.out.valid)
   for (i <- 0 until (MemReqWidth + 1)) {
     XSPerfAccumulate(s"mem_req_util${i}", PopCount(waiting_resp) === i.U)
   }
   XSPerfAccumulate("mem_cycle", PopCount(waiting_resp) =/= 0.U)
   XSPerfAccumulate("mem_count", mem.a.fire())
   for (i <- 0 until PtwWidth) {
-    XSPerfAccumulate(s"llptw_ppn_af${i}", outArb(i).in(outArbMqPort).valid && outArb(i).in(outArbMqPort).bits.af && !llptw_out.bits.af)
     XSPerfAccumulate(s"access_fault${i}", io.tlb(i).resp.fire && io.tlb(i).resp.bits.af)
   }
 
@@ -423,56 +264,494 @@ class L2TLBImp(outer: L2TLB)(implicit p: Parameters) extends PtwModule(outer) wi
     TimeOutAssert(flush_latch(i), timeOutThreshold, s"ptw mem resp time out flush_latch${i}")
   }
 
-
-  val perfEvents  = Seq(llptw, cache, ptw).flatMap(_.getPerfEvents)
+  val perfEvents  = Seq(ptw).flatMap(_.getPerfEvents)
   generatePerfEvent()
-
-  val L1TlbTable = ChiselDB.createTable("L1Tlb_hart" + p(XSCoreParamsKey).HartId.toString, new L1TlbDB)
-  val ITlbReqDB, DTlbReqDB, ITlbRespDB, DTlbRespDB = Wire(new L1TlbDB)
-  ITlbReqDB.vpn := io.tlb(0).req(0).bits.vpn
-  DTlbReqDB.vpn := io.tlb(1).req(0).bits.vpn
-  ITlbRespDB.vpn := io.tlb(0).resp.bits.entry.tag
-  DTlbRespDB.vpn := io.tlb(1).resp.bits.entry.tag
-  L1TlbTable.log(ITlbReqDB, io.tlb(0).req(0).fire, "ITlbReq", clock, reset)
-  L1TlbTable.log(DTlbReqDB, io.tlb(1).req(0).fire, "DTlbReq", clock, reset)
-  L1TlbTable.log(ITlbRespDB, io.tlb(0).resp.fire, "ITlbResp", clock, reset)
-  L1TlbTable.log(DTlbRespDB, io.tlb(1).resp.fire, "DTlbResp", clock, reset)
-
-  val PageCacheTable = ChiselDB.createTable("PageCache_hart" + p(XSCoreParamsKey).HartId.toString, new PageCacheDB)
-  val PageCacheDB = Wire(new PageCacheDB)
-  PageCacheDB.vpn := cache.io.resp.bits.toTlb.tag
-  PageCacheDB.source := cache.io.resp.bits.req_info.source
-  PageCacheDB.bypassed := cache.io.resp.bits.bypassed
-  PageCacheDB.is_first := cache.io.resp.bits.isFirst
-  PageCacheDB.prefetched := cache.io.resp.bits.toTlb.prefetch
-  PageCacheDB.prefetch := cache.io.resp.bits.prefetch
-  PageCacheDB.l2Hit := cache.io.resp.bits.toFsm.l2Hit
-  PageCacheDB.l1Hit := cache.io.resp.bits.toFsm.l1Hit
-  PageCacheDB.hit := cache.io.resp.bits.hit
-  PageCacheTable.log(PageCacheDB, cache.io.resp.fire, "PageCache", clock, reset)
-
-  val PTWTable = ChiselDB.createTable("PTW_hart" + p(XSCoreParamsKey).HartId.toString, new PTWDB)
-  val PTWReqDB, PTWRespDB, LLPTWReqDB, LLPTWRespDB = Wire(new PTWDB)
-  PTWReqDB.vpn := ptw.io.req.bits.req_info.vpn
-  PTWReqDB.source := ptw.io.req.bits.req_info.source
-  PTWRespDB.vpn := ptw.io.refill.req_info.vpn
-  PTWRespDB.source := ptw.io.refill.req_info.source
-  LLPTWReqDB.vpn := llptw.io.in.bits.req_info.vpn
-  LLPTWReqDB.source := llptw.io.in.bits.req_info.source
-  LLPTWRespDB.vpn := llptw.io.mem.refill.vpn
-  LLPTWRespDB.source := llptw.io.mem.refill.source
-  PTWTable.log(PTWReqDB, ptw.io.req.fire, "PTWReq", clock, reset)
-  PTWTable.log(PTWRespDB, ptw.io.mem.resp.fire, "PTWResp", clock, reset)
-  PTWTable.log(LLPTWReqDB, llptw.io.in.fire, "LLPTWReq", clock, reset)
-  PTWTable.log(LLPTWRespDB, llptw.io.mem.resp.fire, "LLPTWResp", clock, reset)
-
-  val L2TlbMissQueueTable = ChiselDB.createTable("L2TlbMissQueue_hart" + p(XSCoreParamsKey).HartId.toString, new L2TlbMissQueueDB)
-  val L2TlbMissQueueInDB, L2TlbMissQueueOutDB = Wire(new L2TlbMissQueueDB)
-  L2TlbMissQueueInDB.vpn := missQueue.io.in.bits.vpn
-  L2TlbMissQueueOutDB.vpn := missQueue.io.out.bits.vpn
-  L2TlbMissQueueTable.log(L2TlbMissQueueInDB, missQueue.io.in.fire, "L2TlbMissQueueIn", clock, reset)
-  L2TlbMissQueueTable.log(L2TlbMissQueueOutDB, missQueue.io.out.fire, "L2TlbMissQueueOut", clock, reset)
 }
+//@chiselName
+//class L2TLBImp(outer: L2TLB)(implicit p: Parameters) extends PtwModule(outer) with HasCSRConst with HasPerfEvents {
+//
+//  val (mem, edge) = outer.node.out.head
+//
+//  val io = IO(new L2TLBIO)
+//  val difftestIO = IO(new Bundle() {
+//    val ptwResp = Output(Bool())
+//    val ptwAddr = Output(UInt(64.W))
+//    val ptwData = Output(Vec(4, UInt(64.W)))
+//  })
+//
+//  /* Ptw processes multiple requests
+//   * Divide Ptw procedure into two stages: cache access ; mem access if cache miss
+//   *           miss queue itlb       dtlb
+//   *               |       |         |
+//   *               ------arbiter------
+//   *                            |
+//   *                    l1 - l2 - l3 - sp
+//   *                            |
+//   *          -------------------------------------------
+//   *    miss  |  queue                                  | hit
+//   *    [][][][][][]                                    |
+//   *          |                                         |
+//   *    state machine accessing mem                     |
+//   *          |                                         |
+//   *          ---------------arbiter---------------------
+//   *                 |                    |
+//   *                itlb                 dtlb
+//   */
+//
+//  difftestIO <> DontCare
+//
+//  val sfence_tmp = DelayN(io.sfence, 1)
+//  val csr_tmp    = DelayN(io.csr.tlb, 1)
+//  val sfence_dup = Seq.fill(8)(RegNext(sfence_tmp))
+//  val csr_dup = Seq.fill(7)(RegNext(csr_tmp))
+//  val satp   = csr_dup(0).satp
+//  val hgatp   = csr_dup(0).hgatp
+//  val priv   = csr_dup(0).priv
+//  val flush  = sfence_dup(0).valid || satp.changed
+//
+//  val pmp = Module(new PMP())
+//  val pmp_check = VecInit(Seq.fill(3)(Module(new PMPChecker(lgMaxSize = 3, sameCycle = true)).io))
+//  pmp.io.distribute_csr := io.csr.distribute_csr
+//  pmp_check.foreach(_.check_env.apply(ModeS, pmp.io.pmp, pmp.io.pma))
+//
+//  val missQueue = Module(new L2TlbMissQueue)
+//  val cache = Module(new PtwCache)
+//  val ptw = Module(new PTW)
+//  val hptw = Module(new HPTW)
+//  val llptw = Module(new LLPTW)
+//  val blockmq = Module(new BlockHelper(3))
+//  val arb1 = Module(new Arbiter(new PtwReq, PtwWidth))
+//  val arb2 = Module(new Arbiter(new Bundle {
+//    val vpn = UInt(vpnLen.W)
+//    val gvpn = UInt(gvpnLen.W)
+//    val hyperinst = Bool()
+//    val hlvx = Bool()
+//    val virt = Bool()
+//    val source = UInt(bSourceWidth.W)
+//  }, if (l2tlbParams.enablePrefetch) 4 else 3))
+//  val outArb = (0 until PtwWidth).map(i => Module(new Arbiter(new PtwResp, 3)).io)
+//  val outArbCachePort = 0
+//  val outArbFsmPort = 1
+//  val outArbMqPort = 2
+//
+//  // arb2 input port
+//  val InArbPTWPort = 0
+//  val InArbMissQueuePort = 1
+//  val InArbTlbPort = 2
+//  val InArbPrefetchPort = 3
+//  // NOTE: when cache out but miss and ptw doesnt accept,
+//  arb1.io.in <> VecInit(io.tlb.map(_.req(0)))
+//  arb1.io.out.ready := arb2.io.in(InArbTlbPort).ready
+//
+//  arb2.io.in(InArbPTWPort).valid := ptw.io.llptw.valid
+//  arb2.io.in(InArbPTWPort).bits.vpn := ptw.io.llptw.bits.req_info.vpn
+//  arb2.io.in(InArbPTWPort).bits.gvpn := ptw.io.llptw.bits.req_info.gvpn
+//  arb2.io.in(InArbPTWPort).bits.source := ptw.io.llptw.bits.req_info.source
+//  arb2.io.in(InArbPTWPort).bits.hyperinst := ptw.io.llptw.bits.req_info.hyperinst
+//  arb2.io.in(InArbPTWPort).bits.hlvx := ptw.io.llptw.bits.req_info.hlvx
+//  arb2.io.in(InArbPTWPort).bits.virt := ptw.io.llptw.bits.req_info.virt
+//  ptw.io.llptw.ready := arb2.io.in(InArbPTWPort).ready
+//  block_decoupled(missQueue.io.out, arb2.io.in(InArbMissQueuePort), !ptw.io.req.ready)
+//
+//  arb2.io.in(InArbTlbPort).valid := arb1.io.out.valid
+//  arb2.io.in(InArbTlbPort).bits.vpn := arb1.io.out.bits.vpn
+//  arb2.io.in(InArbTlbPort).bits.gvpn := arb1.io.out.bits.gvpn
+//  arb2.io.in(InArbTlbPort).bits.source := arb1.io.chosen
+//  arb2.io.in(InArbTlbPort).bits.hyperinst := arb1.io.out.bits.hyperinst
+//  arb2.io.in(InArbTlbPort).bits.virt := arb1.io.out.bits.virt
+//  arb2.io.in(InArbTlbPort).bits.hlvx := arb1.io.out.bits.hlvx
+//  if (l2tlbParams.enablePrefetch) {
+//    val prefetch = Module(new L2TlbPrefetch())
+//    val recv = cache.io.resp
+//    // NOTE: 1. prefetch doesn't gen prefetch 2. req from mq doesn't gen prefetch
+//    // NOTE: 1. miss req gen prefetch 2. hit but prefetched gen prefetch
+//    prefetch.io.in.valid := recv.fire() && !from_pre(recv.bits.req_info.source) && (!recv.bits.hit  ||
+//      recv.bits.prefetch) && recv.bits.isFirst
+//    prefetch.io.in.bits.vpn := recv.bits.req_info.vpn
+//    prefetch.io.sfence := sfence_dup(0)
+//    prefetch.io.csr := csr_dup(0)
+//    arb2.io.in(InArbPrefetchPort).bits.vpn := prefetch.io.out.bits.vpn
+//    arb2.io.in(InArbPrefetchPort).bits.source := prefetch.io.out.bits.source
+//    arb2.io.in(InArbPrefetchPort).bits.gvpn := DontCare
+//    arb2.io.in(InArbPrefetchPort).bits.hlvx := DontCare
+//    arb2.io.in(InArbPrefetchPort).bits.hyperinst := DontCare
+//    arb2.io.in(InArbPrefetchPort).bits.virt := DontCare
+//    prefetch.io.out.ready := arb2.io.in(InArbPrefetchPort).ready
+//    arb2.io.in(InArbPrefetchPort).valid := prefetch.io.out.valid
+//
+//    val L2TlbPrefetchTable = ChiselDB.createTable("L2TlbPrefetch_hart" + p(XSCoreParamsKey).HartId.toString, new L2TlbPrefetchDB)
+//    val L2TlbPrefetchDB = Wire(new L2TlbPrefetchDB)
+//    L2TlbPrefetchDB.vpn := prefetch.io.out.bits.vpn
+//    L2TlbPrefetchTable.log(L2TlbPrefetchDB, prefetch.io.out.fire, "L2TlbPrefetch", clock, reset)
+//  }
+//  arb2.io.out.ready := cache.io.req.ready
+//
+//
+//  val mq_arb = Module(new Arbiter(new L2TlbInnerBundle, 2))
+//  mq_arb.io.in(0).valid := cache.io.resp.valid && !cache.io.resp.bits.hit &&
+//    (!cache.io.resp.bits.toFsm.l2Hit || cache.io.resp.bits.bypassed) &&
+//    !from_pre(cache.io.resp.bits.req_info.source) &&
+//    (cache.io.resp.bits.bypassed || cache.io.resp.bits.isFirst || !ptw.io.req.ready)
+//  mq_arb.io.in(0).bits :=  cache.io.resp.bits.req_info
+//  mq_arb.io.in(1) <> llptw.io.cache
+//  missQueue.io.in <> mq_arb.io.out
+//  missQueue.io.sfence  := sfence_dup(6)
+//  missQueue.io.csr := csr_dup(5)
+//
+//  blockmq.io.start := missQueue.io.out.fire
+//  blockmq.io.enable := ptw.io.req.fire()
+//
+//  llptw.io.in.valid := cache.io.resp.valid && !cache.io.resp.bits.hit && cache.io.resp.bits.toFsm.l2Hit && !cache.io.resp.bits.bypassed
+//  llptw.io.in.bits.req_info := cache.io.resp.bits.req_info
+//  llptw.io.in.bits.ppn := cache.io.resp.bits.toFsm.ppn
+//  llptw.io.sfence := sfence_dup(1)
+//  llptw.io.csr := csr_dup(1)
+//
+//  cache.io.req.valid := arb2.io.out.valid
+//  cache.io.req.bits.req_info.vpn := arb2.io.out.bits.vpn
+//  cache.io.req.bits.req_info.gvpn := arb2.io.out.bits.gvpn
+//  cache.io.req.bits.req_info.hlvx := arb2.io.out.bits.hlvx
+//  cache.io.req.bits.req_info.hyperinst := arb2.io.out.bits.hyperinst
+//  cache.io.req.bits.req_info.virt := arb2.io.out.bits.virt
+//  cache.io.req.bits.req_info.source := arb2.io.out.bits.source
+//  cache.io.req.bits.isFirst := arb2.io.chosen =/= InArbMissQueuePort.U
+//  cache.io.req.bits.bypassed.map(_ := false.B)
+//  cache.io.sfence := sfence_dup(2)
+//  cache.io.csr := csr_dup(2)
+//  cache.io.sfence_dup.zip(sfence_dup.drop(2).take(4)).map(s => s._1 := s._2)
+//  cache.io.csr_dup.zip(csr_dup.drop(2).take(3)).map(c => c._1 := c._2)
+//  cache.io.resp.ready := Mux(cache.io.resp.bits.hit,
+//    outReady(cache.io.resp.bits.req_info.source, outArbCachePort),
+//    Mux(cache.io.resp.bits.toFsm.l2Hit && !cache.io.resp.bits.bypassed, llptw.io.in.ready,
+//    Mux(cache.io.resp.bits.bypassed || cache.io.resp.bits.isFirst, mq_arb.io.in(0).ready, mq_arb.io.in(0).ready || ptw.io.req.ready)))
+//
+//  // NOTE: missQueue req has higher priority
+//  ptw.io.req.valid := cache.io.resp.valid && !cache.io.resp.bits.hit && !cache.io.resp.bits.toFsm.l2Hit &&
+//    !cache.io.resp.bits.bypassed &&
+//    !cache.io.resp.bits.isFirst
+//  ptw.io.req.bits.req_info := cache.io.resp.bits.req_info
+//  ptw.io.req.bits.l1Hit := cache.io.resp.bits.toFsm.l1Hit
+//  ptw.io.req.bits.ppn := cache.io.resp.bits.toFsm.ppn
+//  ptw.io.sfence := sfence_dup(7)
+//  ptw.io.csr := csr_dup(6)
+//  ptw.io.resp.ready := outReady(ptw.io.resp.bits.source, outArbFsmPort)
+//
+//  // mem req
+//  def blockBytes_align(addr: UInt) = {
+//    Cat(addr(PAddrBits - 1, log2Up(l2tlbParams.blockBytes)), 0.U(log2Up(l2tlbParams.blockBytes).W))
+//  }
+//  def addr_low_from_vpn(vpn: UInt) = {
+//    vpn(log2Ceil(l2tlbParams.blockBytes)-log2Ceil(XLEN/8)-1, 0)
+//  }
+//  def addr_low_from_paddr(paddr: UInt) = {
+//    paddr(log2Up(l2tlbParams.blockBytes)-1, log2Up(XLEN/8))
+//  }
+//  def from_llptw(id: UInt) = {
+//    id =/= FsmReqID.U && id =/= HptwReqID.U
+//  }
+//  def from_ptw(id: UInt) = {
+//    id === FsmReqID.U
+//  }
+//  def from_hptw(id: UInt) = {
+//    id === HptwReqID.U
+//  }
+//  val waiting_resp = RegInit(VecInit(Seq.fill(MemReqWidth)(false.B)))
+//  val flush_latch = RegInit(VecInit(Seq.fill(MemReqWidth)(false.B)))
+//  for (i <- waiting_resp.indices) {
+//    assert(!flush_latch(i) || waiting_resp(i)) // when sfence_latch wait for mem resp, waiting_resp should be true
+//  }
+//
+//  val llptw_out = llptw.io.out
+//  val llptw_mem = llptw.io.mem
+//  llptw_mem.req_mask := waiting_resp.take(l2tlbParams.llptwsize)
+//  ptw.io.mem.mask := waiting_resp.take(l2tlbParams.llptwsize + 1).last
+//  hptw.io.mem.mask := waiting_resp.take(l2tlbParams.llptwsize + 2).last
+//  val mem_arb = Module(new Arbiter(new L2TlbMemReqBundle(), 3))
+//  mem_arb.io.in(0) <> ptw.io.mem.req
+//  mem_arb.io.in(1) <> llptw_mem.req
+//  mem_arb.io.in(2) <> hptw.io.mem.req
+//  mem_arb.io.out.ready := mem.a.ready && !flush
+//
+//  // assert, should not send mem access at same addr for twice.
+//  val last_resp_vpn = RegEnable(cache.io.refill.bits.req_info_dup(0).vpn, cache.io.refill.valid)
+//  val last_resp_level = RegEnable(cache.io.refill.bits.level_dup(0), cache.io.refill.valid)
+//  val last_resp_v = RegInit(false.B)
+//  val last_has_invalid = !Cat(cache.io.refill.bits.ptes.asTypeOf(Vec(blockBits/XLEN, UInt(XLEN.W))).map(a => a(0))).andR || cache.io.refill.bits.sel_pte_dup(0).asTypeOf(new PteBundle).isAf()
+//  when (cache.io.refill.valid) { last_resp_v := !last_has_invalid}
+//  when (flush) { last_resp_v := false.B }
+//  XSError(last_resp_v && cache.io.refill.valid &&
+//    (cache.io.refill.bits.req_info_dup(0).vpn === last_resp_vpn) &&
+//    (cache.io.refill.bits.level_dup(0) === last_resp_level),
+//    "l2tlb should not access mem at same addr for twice")
+//  // ATTENTION: this may wronngly assert when: a ptes is l2, last part is valid,
+//  // but the current part is invalid, so one more mem access happened
+//  // If this happened, remove the assert.
+//
+//  val req_addr_low = Reg(Vec(MemReqWidth, UInt((log2Up(l2tlbParams.blockBytes)-log2Up(XLEN/8)).W)))
+//
+//  when (llptw.io.in.fire()) {
+//    // when enq miss queue, set the req_addr_low to receive the mem resp data part
+//    req_addr_low(llptw_mem.enq_ptr) := addr_low_from_vpn(llptw.io.in.bits.req_info.vpn)
+//  }
+//  when (mem_arb.io.out.fire()) {
+//    req_addr_low(mem_arb.io.out.bits.id) := addr_low_from_paddr(mem_arb.io.out.bits.addr)
+//    waiting_resp(mem_arb.io.out.bits.id) := true.B
+//  }
+//  // mem read
+//  val memRead =  edge.Get(
+//    fromSource = mem_arb.io.out.bits.id,
+//    // toAddress  = memAddr(log2Up(CacheLineSize / 2 / 8) - 1, 0),
+//    toAddress  = blockBytes_align(mem_arb.io.out.bits.addr),
+//    lgSize     = log2Up(l2tlbParams.blockBytes).U
+//  )._2
+//  mem.a.bits := memRead
+//  mem.a.valid := mem_arb.io.out.valid && !flush
+//  mem.d.ready := true.B
+//  // mem -> data buffer
+//  val refill_data = Reg(Vec(blockBits / l1BusDataWidth, UInt(l1BusDataWidth.W)))
+//  val refill_helper = edge.firstlastHelper(mem.d.bits, mem.d.fire())
+//  val mem_resp_done = refill_helper._3
+//  val mem_resp_from_llptw = from_llptw(mem.d.bits.source)
+//  val mem_resp_from_ptw = from_ptw(mem.d.bits.source)
+//  val mem_resp_from_hptw = from_hptw(mem.d.bits.source)
+//  when (mem.d.valid) {
+//    assert(mem.d.bits.source <= (l2tlbParams.llptwsize + 1).U)
+//    refill_data(refill_helper._4) := mem.d.bits.data
+//  }
+//  // refill_data_tmp is the wire fork of refill_data, but one cycle earlier
+//  val refill_data_tmp = WireInit(refill_data)
+//  refill_data_tmp(refill_helper._4) := mem.d.bits.data
+//
+//  // save only one pte for each id
+//  // (miss queue may can't resp to tlb with low latency, it should have highest priority, but diffcult to design cache)
+//  val resp_pte = VecInit((0 until MemReqWidth).map(i =>
+//    if (i == FsmReqID) {RegEnable(get_part(refill_data_tmp, req_addr_low(i)), mem_resp_done && !mem_resp_from_llptw && !mem_resp_from_hptw && mem_resp_from_ptw) }
+//    else if( i == HptwReqID) {RegEnable(get_part(refill_data_tmp, req_addr_low(i)), mem_resp_done && !mem_resp_from_llptw && !mem_resp_from_ptw && mem_resp_from_hptw)}
+//    else { DataHoldBypass(get_part(refill_data, req_addr_low(i)), llptw_mem.buffer_it(i)) }
+//    // llptw could not use refill_data_tmp, because enq bypass's result works at next cycle
+//  ))
+//
+//  // mem -> miss queue
+//  llptw_mem.resp.valid := mem_resp_done && mem_resp_from_llptw
+//  llptw_mem.resp.bits.id := DataHoldBypass(mem.d.bits.source, mem.d.valid)
+//  // mem -> ptw
+//  ptw.io.mem.req.ready := mem.a.ready
+//  ptw.io.mem.resp.valid := mem_resp_done && mem_resp_from_ptw
+//  ptw.io.mem.resp.bits := resp_pte.take(l2tlbParams.llptwsize + 1).last
+//  // mem -> cache
+//  val refill_from_llptw = mem_resp_from_llptw
+//  val refill_level = Mux(refill_from_llptw, 2.U, RegEnable(ptw.io.refill.level, init = 0.U, ptw.io.mem.req.fire()))
+//  val refill_valid = mem_resp_done && !flush && !flush_latch(mem.d.bits.source)
+//
+//  cache.io.refill.valid := RegNext(refill_valid, false.B)
+//  cache.io.refill.bits.ptes := refill_data.asUInt
+//  cache.io.refill.bits.req_info_dup.map(_ := RegEnable(Mux(refill_from_llptw, llptw_mem.refill, ptw.io.refill.req_info), refill_valid))
+//  cache.io.refill.bits.level_dup.map(_ := RegEnable(refill_level, refill_valid))
+//  cache.io.refill.bits.levelOH(refill_level, refill_valid)
+//  cache.io.refill.bits.sel_pte_dup.map(_ := RegNext(sel_data(refill_data_tmp.asUInt, req_addr_low(mem.d.bits.source))))
+//
+//  // hptw, hptw <> ptw, hptw <> llptw, hptw <> mem, hptw <> pageCache
+//  hptw.io.ptw <> ptw.io.hptw
+//  hptw.io.llptw <> llptw.io.hptw
+//  hptw.io.pageCache <> cache.io.hptw
+//  hptw.io.mem.req.ready := mem.a.ready
+//  hptw.io.mem.resp.valid := mem_resp_done && mem_resp_from_hptw
+//  hptw.io.mem.resp.bits := resp_pte.take(l2tlbParams.llptwsize + 2).last
+//  hptw.io.csr := DontCare
+//  hptw.io.sfence := DontCare
+//
+//
+//  if (env.EnableDifftest) {
+//    val difftest_ptw_addr = RegInit(VecInit(Seq.fill(MemReqWidth)(0.U(PAddrBits.W))))
+//    when (mem.a.valid) {
+//      difftest_ptw_addr(mem.a.bits.source) := mem.a.bits.address
+//    }
+//
+//    val difftest = Module(new DifftestRefillEvent)
+//    difftest.io.clock := clock
+//    difftest.io.coreid := p(XSCoreParamsKey).HartId.asUInt
+//    difftest.io.cacheid := 2.U
+//    difftest.io.valid := cache.io.refill.valid
+//    difftest.io.addr := difftest_ptw_addr(RegNext(mem.d.bits.source))
+//    difftest.io.data := refill_data.asTypeOf(difftest.io.data)
+//  }
+//
+//  if (env.EnableDifftest) {
+//    for (i <- 0 until PtwWidth) {
+//      val difftest = Module(new DifftestL2TLBEvent)
+//      difftest.io.clock := clock
+//      difftest.io.coreid := p(XSCoreParamsKey).HartId.asUInt
+//      difftest.io.valid := io.tlb(i).resp.fire && !io.tlb(i).resp.bits.af
+//      difftest.io.index := i.U
+//      difftest.io.satp := Cat(io.csr.tlb.satp.mode, io.csr.tlb.satp.asid, io.csr.tlb.satp.ppn)
+//      difftest.io.vsatp := Cat(io.csr.tlb.vsatp.mode, io.csr.tlb.vsatp.asid, io.csr.tlb.vsatp.ppn)
+//      difftest.io.hgatp := Cat(io.csr.tlb.hgatp.mode, io.csr.tlb.hgatp.asid, io.csr.tlb.hgatp.ppn)
+//      difftest.io.s2xlate := io.tlb(i).resp.bits.entry.s2xlate
+//      difftest.io.vpn := io.tlb(i).resp.bits.entry.tag
+//      difftest.io.ppn := io.tlb(i).resp.bits.entry.ppn
+//      difftest.io.perm := io.tlb(i).resp.bits.entry.perm.getOrElse(0.U.asTypeOf(new PtePermBundle)).asUInt
+//      difftest.io.level := io.tlb(i).resp.bits.entry.level.getOrElse(0.U.asUInt)
+//      difftest.io.pf := io.tlb(i).resp.bits.pf
+//    }
+//  }
+//
+//  // pmp
+//  pmp_check(0).req <> ptw.io.pmp.req
+//  ptw.io.pmp.resp <> pmp_check(0).resp
+//  pmp_check(1).req <> llptw.io.pmp.req
+//  llptw.io.pmp.resp <> pmp_check(1).resp
+//  pmp_check(2).req <> hptw.io.pmp.req
+//  hptw.io.pmp.resp <> pmp_check(2).resp
+//
+//  llptw_out.ready := outReady(llptw_out.bits.req_info.source, outArbMqPort)
+//  for (i <- 0 until PtwWidth) {
+//    outArb(i).in(outArbCachePort).valid := cache.io.resp.valid && cache.io.resp.bits.hit && cache.io.resp.bits.req_info.source===i.U
+//    outArb(i).in(outArbCachePort).bits.entry := cache.io.resp.bits.toTlb
+//    outArb(i).in(outArbCachePort).bits.pf := !cache.io.resp.bits.toTlb.v
+//    outArb(i).in(outArbCachePort).bits.af := false.B
+//    outArb(i).in(outArbCachePort).bits.gpf := false.B
+//    outArb(i).in(outArbFsmPort).valid := ptw.io.resp.valid && ptw.io.resp.bits.source===i.U
+//    outArb(i).in(outArbFsmPort).bits := ptw.io.resp.bits.resp
+//    outArb(i).in(outArbMqPort).valid := llptw_out.valid && llptw_out.bits.req_info.source===i.U
+//    outArb(i).in(outArbMqPort).bits := pte_to_ptwResp(resp_pte(llptw_out.bits.id), llptw_out.bits.req_info.vpn, llptw_out.bits.af, llptw_out.bits.gpf, true, llptw_out.bits.gpaddr, llptw_out.bits.s2xlate)
+//  }
+//
+//  // io.tlb.map(_.resp) <> outArb.map(_.out)
+//  io.tlb.map(_.resp).zip(outArb.map(_.out)).map{
+//    case (resp, out) => resp <> out
+//  }
+//
+//  // sfence
+//  when (flush) {
+//    for (i <- 0 until MemReqWidth) {
+//      when (waiting_resp(i)) {
+//        flush_latch(i) := true.B
+//      }
+//    }
+//  }
+//  // mem -> control signal
+//  // waiting_resp and sfence_latch will be reset when mem_resp_done
+//  when (mem_resp_done) {
+//    waiting_resp(mem.d.bits.source) := false.B
+//    flush_latch(mem.d.bits.source) := false.B
+//  }
+//
+//  def block_decoupled[T <: Data](source: DecoupledIO[T], sink: DecoupledIO[T], block_signal: Bool) = {
+//    sink.valid   := source.valid && !block_signal
+//    source.ready := sink.ready   && !block_signal
+//    sink.bits    := source.bits
+//  }
+//
+//  def get_part(data: Vec[UInt], index: UInt): UInt = {
+//    val inner_data = data.asTypeOf(Vec(data.getWidth / XLEN, UInt(XLEN.W)))
+//    inner_data(index)
+//  }
+//
+//  def pte_to_ptwResp(pte: UInt, vpn: UInt, af: Bool, gpf: Bool, af_first: Boolean, gpaddr: UInt, s2xlate: Bool) : PtwResp = {
+//    val pte_in = pte.asTypeOf(new PteBundle())
+//    val ptw_resp = Wire(new PtwResp())
+//    ptw_resp.entry.ppn := pte_in.ppn
+//    ptw_resp.entry.gvpn := gpaddr >> 12
+//    ptw_resp.entry.level.map(_ := 2.U)
+//    ptw_resp.entry.perm.map(_ := pte_in.getPerm())
+//    ptw_resp.entry.tag := vpn
+//    ptw_resp.pf := (if (af_first) !af else true.B) && pte_in.isPf(2.U)
+//    ptw_resp.af := (if (!af_first) pte_in.isPf(2.U) else true.B) && (af || pte_in.isAf())
+//    ptw_resp.gpf := (if (af_first) !af else true.B) && gpf
+//    ptw_resp.entry.v := !ptw_resp.pf
+//    ptw_resp.entry.prefetch := DontCare
+//    ptw_resp.entry.asid := satp.asid
+//    ptw_resp.entry.vmid := hgatp.asid
+//    ptw_resp.entry.s2xlate := s2xlate
+//    ptw_resp
+//  }
+//
+//  def outReady(source: UInt, port: Int): Bool = {
+//    MuxLookup(source, true.B,
+//      (0 until PtwWidth).map(i => i.U -> outArb(i).in(port).ready))
+//  }
+//
+//  // debug info
+//  for (i <- 0 until PtwWidth) {
+//    XSDebug(p"[io.tlb(${i.U})] ${io.tlb(i)}\n")
+//  }
+//  XSDebug(p"[sfence] ${io.sfence}\n")
+//  XSDebug(p"[io.csr.tlb] ${io.csr.tlb}\n")
+//
+//  for (i <- 0 until PtwWidth) {
+//    XSPerfAccumulate(s"req_count${i}", io.tlb(i).req(0).fire())
+//    XSPerfAccumulate(s"req_blocked_count_${i}", io.tlb(i).req(0).valid && !io.tlb(i).req(0).ready)
+//  }
+//  XSPerfAccumulate(s"req_blocked_by_mq", arb1.io.out.valid && missQueue.io.out.valid)
+//  for (i <- 0 until (MemReqWidth + 1)) {
+//    XSPerfAccumulate(s"mem_req_util${i}", PopCount(waiting_resp) === i.U)
+//  }
+//  XSPerfAccumulate("mem_cycle", PopCount(waiting_resp) =/= 0.U)
+//  XSPerfAccumulate("mem_count", mem.a.fire())
+//  for (i <- 0 until PtwWidth) {
+//    XSPerfAccumulate(s"llptw_ppn_af${i}", outArb(i).in(outArbMqPort).valid && outArb(i).in(outArbMqPort).bits.af && !llptw_out.bits.af)
+//    XSPerfAccumulate(s"access_fault${i}", io.tlb(i).resp.fire && io.tlb(i).resp.bits.af)
+//  }
+//
+//  // print configs
+//  println(s"${l2tlbParams.name}: a ptw, a llptw with size ${l2tlbParams.llptwsize}, miss queue size ${MissQueueSize} l1:${l2tlbParams.l1Size} fa l2: nSets ${l2tlbParams.l2nSets} nWays ${l2tlbParams.l2nWays} l3: ${l2tlbParams.l3nSets} nWays ${l2tlbParams.l3nWays} blockBytes:${l2tlbParams.blockBytes}")
+//
+//  // time out assert
+//  for (i <- 0 until MemReqWidth) {
+//    TimeOutAssert(waiting_resp(i), timeOutThreshold, s"ptw mem resp time out wait_resp${i}")
+//    TimeOutAssert(flush_latch(i), timeOutThreshold, s"ptw mem resp time out flush_latch${i}")
+//  }
+//
+//
+//  val perfEvents  = Seq(llptw, cache, ptw).flatMap(_.getPerfEvents)
+//  generatePerfEvent()
+//
+//  val L1TlbTable = ChiselDB.createTable("L1Tlb_hart" + p(XSCoreParamsKey).HartId.toString, new L1TlbDB)
+//  val ITlbReqDB, DTlbReqDB, ITlbRespDB, DTlbRespDB = Wire(new L1TlbDB)
+//  ITlbReqDB.vpn := io.tlb(0).req(0).bits.vpn
+//  DTlbReqDB.vpn := io.tlb(1).req(0).bits.vpn
+//  ITlbRespDB.vpn := io.tlb(0).resp.bits.entry.tag
+//  DTlbRespDB.vpn := io.tlb(1).resp.bits.entry.tag
+//  L1TlbTable.log(ITlbReqDB, io.tlb(0).req(0).fire, "ITlbReq", clock, reset)
+//  L1TlbTable.log(DTlbReqDB, io.tlb(1).req(0).fire, "DTlbReq", clock, reset)
+//  L1TlbTable.log(ITlbRespDB, io.tlb(0).resp.fire, "ITlbResp", clock, reset)
+//  L1TlbTable.log(DTlbRespDB, io.tlb(1).resp.fire, "DTlbResp", clock, reset)
+//
+//  val PageCacheTable = ChiselDB.createTable("PageCache_hart" + p(XSCoreParamsKey).HartId.toString, new PageCacheDB)
+//  val PageCacheDB = Wire(new PageCacheDB)
+//  PageCacheDB.vpn := cache.io.resp.bits.toTlb.tag
+//  PageCacheDB.source := cache.io.resp.bits.req_info.source
+//  PageCacheDB.bypassed := cache.io.resp.bits.bypassed
+//  PageCacheDB.is_first := cache.io.resp.bits.isFirst
+//  PageCacheDB.prefetched := cache.io.resp.bits.toTlb.prefetch
+//  PageCacheDB.prefetch := cache.io.resp.bits.prefetch
+//  PageCacheDB.l2Hit := cache.io.resp.bits.toFsm.l2Hit
+//  PageCacheDB.l1Hit := cache.io.resp.bits.toFsm.l1Hit
+//  PageCacheDB.hit := cache.io.resp.bits.hit
+//  PageCacheTable.log(PageCacheDB, cache.io.resp.fire, "PageCache", clock, reset)
+//
+//  val PTWTable = ChiselDB.createTable("PTW_hart" + p(XSCoreParamsKey).HartId.toString, new PTWDB)
+//  val PTWReqDB, PTWRespDB, LLPTWReqDB, LLPTWRespDB = Wire(new PTWDB)
+//  PTWReqDB.vpn := ptw.io.req.bits.req_info.vpn
+//  PTWReqDB.source := ptw.io.req.bits.req_info.source
+//  PTWRespDB.vpn := ptw.io.refill.req_info.vpn
+//  PTWRespDB.source := ptw.io.refill.req_info.source
+//  LLPTWReqDB.vpn := llptw.io.in.bits.req_info.vpn
+//  LLPTWReqDB.source := llptw.io.in.bits.req_info.source
+//  LLPTWRespDB.vpn := llptw.io.mem.refill.vpn
+//  LLPTWRespDB.source := llptw.io.mem.refill.source
+//  PTWTable.log(PTWReqDB, ptw.io.req.fire, "PTWReq", clock, reset)
+//  PTWTable.log(PTWRespDB, ptw.io.mem.resp.fire, "PTWResp", clock, reset)
+//  PTWTable.log(LLPTWReqDB, llptw.io.in.fire, "LLPTWReq", clock, reset)
+//  PTWTable.log(LLPTWRespDB, llptw.io.mem.resp.fire, "LLPTWResp", clock, reset)
+//
+//  val L2TlbMissQueueTable = ChiselDB.createTable("L2TlbMissQueue_hart" + p(XSCoreParamsKey).HartId.toString, new L2TlbMissQueueDB)
+//  val L2TlbMissQueueInDB, L2TlbMissQueueOutDB = Wire(new L2TlbMissQueueDB)
+//  L2TlbMissQueueInDB.vpn := missQueue.io.in.bits.vpn
+//  L2TlbMissQueueOutDB.vpn := missQueue.io.out.bits.vpn
+//  L2TlbMissQueueTable.log(L2TlbMissQueueInDB, missQueue.io.in.fire, "L2TlbMissQueueIn", clock, reset)
+//  L2TlbMissQueueTable.log(L2TlbMissQueueOutDB, missQueue.io.out.fire, "L2TlbMissQueueOut", clock, reset)
+//}
 
 /** BlockHelper, block missqueue, not to send too many req to cache
  *  Parameter:
diff --git a/src/main/scala/xiangshan/cache/mmu/L2TlbPrefetch.scala b/src/main/scala/xiangshan/cache/mmu/L2TlbPrefetch.scala
index 2a52b9c16..4c42f4578 100644
--- a/src/main/scala/xiangshan/cache/mmu/L2TlbPrefetch.scala
+++ b/src/main/scala/xiangshan/cache/mmu/L2TlbPrefetch.scala
@@ -44,7 +44,7 @@ class L2TlbPrefetch(implicit p: Parameters) extends XSModule with HasPtwConst {
     Cat(old_reqs.zip(old_v).map{ case (o,v) => dup(o,vpn) && v}).orR
   }
 
-  val flush = io.sfence.valid || io.csr.satp.changed
+  val flush = io.sfence.valid || io.csr.satp.changed || (io.csr.priv.virt && io.csr.vsatp.changed)
   val next_line = get_next_line(io.in.bits.vpn)
   val next_req = RegEnable(next_line, io.in.valid)
   val input_valid = io.in.valid && !flush && !already_have(next_line)
diff --git a/src/main/scala/xiangshan/cache/mmu/MMUBundle.scala b/src/main/scala/xiangshan/cache/mmu/MMUBundle.scala
index 0f366920e..96e18762c 100644
--- a/src/main/scala/xiangshan/cache/mmu/MMUBundle.scala
+++ b/src/main/scala/xiangshan/cache/mmu/MMUBundle.scala
@@ -38,6 +38,11 @@ class VaBundle(implicit p: Parameters) extends TlbBundle {
   val off  = UInt(offLen.W)
 }
 
+class GpaBundle(implicit p: Parameters) extends TlbBundle {
+  val gvpn  = UInt(gvpnLen.W)
+  val off  = UInt(offLen.W)
+}
+
 class PtePermBundle(implicit p: Parameters) extends TlbBundle {
   val d = Bool()
   val a = Bool()
@@ -46,7 +51,7 @@ class PtePermBundle(implicit p: Parameters) extends TlbBundle {
   val x = Bool()
   val w = Bool()
   val r = Bool()
-
+  val v = Bool()
   override def toPrintable: Printable = {
     p"d:${d} a:${a} g:${g} u:${u} x:${x} w:${w} r:${r}"// +
     //(if(hasV) (p"v:${v}") else p"")
@@ -72,6 +77,7 @@ class TlbPMBundle(implicit p: Parameters) extends TlbBundle {
 class TlbPermBundle(implicit p: Parameters) extends TlbBundle {
   val pf = Bool() // NOTE: if this is true, just raise pf
   val af = Bool() // NOTE: if this is true, just raise af
+  val gpf = Bool() // NOTE: if this is true, just raise gpf
   // pagetable perm (software defined)
   val d = Bool()
   val a = Bool()
@@ -80,13 +86,14 @@ class TlbPermBundle(implicit p: Parameters) extends TlbBundle {
   val x = Bool()
   val w = Bool()
   val r = Bool()
-
+  val v = Bool() // for H extention, but it will remove after rewrite mmu about H extention
   val pm = new TlbPMBundle
 
   def apply(item: PtwResp, pm: PMPConfig) = {
     val ptePerm = item.entry.perm.get.asTypeOf(new PtePermBundle().cloneType)
     this.pf := item.pf
     this.af := item.af
+    this.gpf := item.gpf
     this.d := ptePerm.d
     this.a := ptePerm.a
     this.g := ptePerm.g
@@ -94,7 +101,7 @@ class TlbPermBundle(implicit p: Parameters) extends TlbBundle {
     this.x := ptePerm.x
     this.w := ptePerm.w
     this.r := ptePerm.r
-
+    this.v := ptePerm.v
     this.pm.assign_ap(pm)
     this
   }
@@ -139,13 +146,16 @@ class TlbEntry(pageNormal: Boolean, pageSuper: Boolean)(implicit p: Parameters)
   val tag = if (!pageNormal) UInt((vpnLen - vpnnLen).W)
             else UInt(vpnLen.W)
   val asid = UInt(asidLen.W)
+  val vmid = UInt(vmidLen.W)
   val level = if (!pageNormal) Some(UInt(1.W))
               else if (!pageSuper) None
               else Some(UInt(2.W))
   val ppn = if (!pageNormal) UInt((ppnLen - vpnnLen).W)
             else UInt(ppnLen.W)
+  val gvpn = if (!pageNormal) UInt((gvpnLen - vpnnLen).W)
+          else UInt(gvpnLen.W)
   val perm = new TlbPermBundle
-
+  val s2xlate = Bool()
   /** level usage:
    *  !PageSuper: page is only normal, level is None, match all the tag
    *  !PageNormal: page is only super, level is a Bool(), match high 9*2 parts
@@ -156,9 +166,10 @@ class TlbEntry(pageNormal: Boolean, pageSuper: Boolean)(implicit p: Parameters)
    *  bits1  0: need mid 9bits
    */
 
-  def hit(vpn: UInt, asid: UInt, nSets: Int = 1, ignoreAsid: Boolean = false): Bool = {
+  def hit(vpn: UInt, asid: UInt, s2xlate: Bool, nSets: Int = 1, ignoreAsid: Boolean = false, vmid: UInt = 0.U): Bool = {
     val asid_hit = if (ignoreAsid) true.B else (this.asid === asid)
-
+    val vmid_hit = Mux(s2xlate, this.vmid === vmid, true.B)
+    val s2xlate_hit = this.s2xlate === s2xlate && vmid_hit // entry is the page table of virtual machine whose id is vmid
     // NOTE: for timing, dont care low set index bits at hit check
     //       do not need store the low bits actually
     if (!pageSuper) asid_hit && drop_set_equal(vpn, tag, nSets)
@@ -166,7 +177,7 @@ class TlbEntry(pageNormal: Boolean, pageSuper: Boolean)(implicit p: Parameters)
       val tag_match_hi = tag(vpnnLen*2-1, vpnnLen) === vpn(vpnnLen*3-1, vpnnLen*2)
       val tag_match_mi = tag(vpnnLen-1, 0) === vpn(vpnnLen*2-1, vpnnLen)
       val tag_match = tag_match_hi && (level.get.asBool() || tag_match_mi)
-      asid_hit && tag_match
+      asid_hit && tag_match && s2xlate_hit
     }
     else {
       val tmp_level = level.get
@@ -174,13 +185,13 @@ class TlbEntry(pageNormal: Boolean, pageSuper: Boolean)(implicit p: Parameters)
       val tag_match_mi = tag(vpnnLen*2-1, vpnnLen) === vpn(vpnnLen*2-1, vpnnLen)
       val tag_match_lo = tag(vpnnLen-1, 0) === vpn(vpnnLen-1, 0) // if pageNormal is false, this will always be false
       val tag_match = tag_match_hi && (tmp_level(1) || tag_match_mi) && (tmp_level(0) || tag_match_lo)
-      asid_hit && tag_match
+      asid_hit && tag_match && s2xlate_hit
     }
   }
 
-  def apply(item: PtwResp, asid: UInt, pm: PMPConfig): TlbEntry = {
+  def apply(item: PtwResp, pm: PMPConfig): TlbEntry = {
     this.tag := {if (pageNormal) item.entry.tag else item.entry.tag(vpnLen-1, vpnnLen)}
-    this.asid := asid
+    this.asid := item.entry.asid
     val inner_level = item.entry.level.getOrElse(0.U)
     this.level.map(_ := { if (pageNormal && pageSuper) MuxLookup(inner_level, 0.U, Seq(
                                                         0.U -> 3.U,
@@ -190,7 +201,13 @@ class TlbEntry(pageNormal: Boolean, pageSuper: Boolean)(implicit p: Parameters)
                           else 0.U })
     this.ppn := { if (!pageNormal) item.entry.ppn(ppnLen-1, vpnnLen)
                   else item.entry.ppn }
+    this.gvpn := {
+      if (!pageNormal) item.entry.gvpn(gvpnLen - 1, vpnnLen)
+      else item.entry.gvpn
+    }
     this.perm.apply(item, pm)
+    this.s2xlate := item.entry.s2xlate
+    this.vmid := item.entry.vmid
     this
   }
 
@@ -243,10 +260,12 @@ class TlbStorageIO(nSets: Int, nWays: Int, ports: Int, nDups: Int = 1)(implicit
   val r = new Bundle {
     val req = Vec(ports, Flipped(DecoupledIO(new Bundle {
       val vpn = Output(UInt(vpnLen.W))
+      val s2xlate = Output(Bool())
     })))
     val resp = Vec(ports, ValidIO(new Bundle{
       val hit = Output(Bool())
       val ppn = Vec(nDups, Output(UInt(ppnLen.W)))
+      val gvpn = Vec(nDups, Output(UInt(gvpnLen.W)))
       val perm = Vec(nDups, Output(new TlbPermBundle()))
     }))
   }
@@ -265,9 +284,10 @@ class TlbStorageIO(nSets: Int, nWays: Int, ports: Int, nDups: Int = 1)(implicit
   }
   val access = Vec(ports, new ReplaceAccessBundle(nSets, nWays))
 
-  def r_req_apply(valid: Bool, vpn: UInt, i: Int): Unit = {
+  def r_req_apply(valid: Bool, vpn: UInt, i: Int, s2xlate: Bool): Unit = {
     this.r.req(i).valid := valid
     this.r.req(i).bits.vpn := vpn
+    this.r.req(i).bits.s2xlate := s2xlate
   }
 
   def r_resp_apply(i: Int) = {
@@ -287,14 +307,17 @@ class TlbStorageWrapperIO(ports: Int, q: TLBParameters, nDups: Int = 1)(implicit
   val r = new Bundle {
     val req = Vec(ports, Flipped(DecoupledIO(new Bundle {
       val vpn = Output(UInt(vpnLen.W))
+      val s2xlate = Output(Bool())
     })))
     val resp = Vec(ports, ValidIO(new Bundle{
       val hit = Output(Bool())
       val ppn = Vec(nDups, Output(UInt(ppnLen.W)))
+      val gvpn = Vec(nDups, Output(UInt(gvpnLen.W)))
       val perm = Vec(nDups, Output(new TlbPermBundle()))
       // below are dirty code for timing optimization
       val super_hit = Output(Bool())
       val super_ppn = Output(UInt(ppnLen.W))
+      val super_gvpn = Output(UInt(gvpnLen.W))
       val spm = Output(new TlbPMBundle)
     }))
   }
@@ -304,9 +327,10 @@ class TlbStorageWrapperIO(ports: Int, q: TLBParameters, nDups: Int = 1)(implicit
   }))
   val replace = if (q.outReplace) Flipped(new TlbReplaceIO(ports, q)) else null
 
-  def r_req_apply(valid: Bool, vpn: UInt, i: Int): Unit = {
+  def r_req_apply(valid: Bool, vpn: UInt, i: Int, s2xlate: Bool): Unit = {
     this.r.req(i).valid := valid
     this.r.req(i).bits.vpn := vpn
+    this.r.req(i).bits.s2xlate := s2xlate
   }
 
   def r_resp_apply(i: Int) = {
@@ -314,6 +338,10 @@ class TlbStorageWrapperIO(ports: Int, q: TLBParameters, nDups: Int = 1)(implicit
     this.r.resp(i).bits.super_hit, this.r.resp(i).bits.super_ppn, this.r.resp(i).bits.spm)
   }
 
+  def r_resp_gvpn_apply(i: Int) = {
+    (this.r.resp(i).bits.gvpn, this.r.resp(i).bits.super_gvpn)
+  }
+
   def w_apply(valid: Bool, data: PtwResp, data_replenish: PMPConfig): Unit = {
     this.w.valid := valid
     this.w.bits.data := data
@@ -370,6 +398,8 @@ class MemBlockidxBundle(implicit p: Parameters) extends TlbBundle {
 class TlbReq(implicit p: Parameters) extends TlbBundle {
   val vaddr = Output(UInt(VAddrBits.W))
   val cmd = Output(TlbCmd())
+  val hyperinst = Output(Bool())
+  val hlvx = Output(Bool())
   val size = Output(UInt(log2Ceil(log2Ceil(XLEN/8)+1).W))
   val kill = Output(Bool()) // Use for blocked tlb that need sync with other module like icache
   val memidx = Output(new MemBlockidxBundle)
@@ -391,10 +421,14 @@ class TlbExceptionBundle(implicit p: Parameters) extends TlbBundle {
   val ld = Output(Bool())
   val st = Output(Bool())
   val instr = Output(Bool())
+  val ldG = Output(Bool())
+  val stG = Output(Bool())
+  val instrG = Output(Bool())
 }
 
 class TlbResp(nDups: Int = 1)(implicit p: Parameters) extends TlbBundle {
   val paddr = Vec(nDups, Output(UInt(PAddrBits.W)))
+  val gpaddr = Vec(nDups, Output(UInt(GPAddrBits.W)))
   val miss = Output(Bool())
   val fast_miss = Output(Bool()) // without sram part for timing optimization
   val excp = Vec(nDups, new Bundle {
@@ -457,6 +491,14 @@ class MMUIOBaseBundle(implicit p: Parameters) extends TlbBundle {
     this.csr <> csr
     this.csr.satp := satp
   }
+
+  def base_connect(sfence: SfenceBundle, csr: TlbCsrBundle, satp: TlbSatpBundle, vsatp: TlbSatpBundle, hgatp: TlbSatpBundle) = {
+    this.sfence <> sfence
+    this.csr <> csr
+    this.csr.satp := satp
+    this.csr.vsatp := vsatp
+    this.csr.hgatp := hgatp
+  }
 }
 
 class TlbRefilltoMemIO()(implicit p: Parameters) extends TlbBundle {
@@ -541,6 +583,7 @@ class PteBundle(implicit p: Parameters) extends PtwBundle{
     pm.x := perm.x
     pm.w := perm.w
     pm.r := perm.r
+    pm.v := perm.v
     pm
   }
 
@@ -552,12 +595,14 @@ class PteBundle(implicit p: Parameters) extends PtwBundle{
 class PtwEntry(tagLen: Int, hasPerm: Boolean = false, hasLevel: Boolean = false)(implicit p: Parameters) extends PtwBundle {
   val tag = UInt(tagLen.W)
   val asid = UInt(asidLen.W)
+  val vmid = UInt(vmidLen.W)
   val ppn = UInt(ppnLen.W)
   val perm = if (hasPerm) Some(new PtePermBundle) else None
   val level = if (hasLevel) Some(UInt(log2Up(Level).W)) else None
   val prefetch = Bool()
   val v = Bool()
-
+  val s2xlate = Bool()
+  val gvpn = UInt(gvpnLen.W)
   def is_normalentry(): Bool = {
     if (!hasLevel) true.B
     else level.get === 2.U
@@ -572,28 +617,38 @@ class PtwEntry(tagLen: Int, hasPerm: Boolean = false, hasLevel: Boolean = false)
     )
   }
 
-  def hit(vpn: UInt, asid: UInt, allType: Boolean = false, ignoreAsid: Boolean = false) = {
+  def genGVPN(vpn: UInt): UInt = {
+    if (!hasLevel) gvpn
+    else MuxLookup(level.get, 0.U, Seq(
+      0.U -> Cat(gvpn(gvpn.getWidth - 1, vpnnLen * 2), vpn(vpnnLen * 2 - 1, 0)),
+      1.U -> Cat(gvpn(gvpn.getWidth - 1, vpnnLen), vpn(vpnnLen - 1, 0)),
+      2.U -> gvpn)
+    )
+  }
+
+  def hit(vpn: UInt, asid: UInt, allType: Boolean = false, ignoreAsid: Boolean = false, s2xlate: Bool) = {
     require(vpn.getWidth == vpnLen)
 //    require(this.asid.getWidth <= asid.getWidth)
     val asid_hit = if (ignoreAsid) true.B else (this.asid === asid)
+    val s2xlate_hit = this.s2xlate === s2xlate
     if (allType) {
       require(hasLevel)
       val hit0 = tag(tagLen - 1,    vpnnLen*2) === vpn(tagLen - 1, vpnnLen*2)
       val hit1 = tag(vpnnLen*2 - 1, vpnnLen)   === vpn(vpnnLen*2 - 1,  vpnnLen)
       val hit2 = tag(vpnnLen - 1,     0)         === vpn(vpnnLen - 1, 0)
 
-      asid_hit && Mux(level.getOrElse(0.U) === 2.U, hit2 && hit1 && hit0, Mux(level.getOrElse(0.U) === 1.U, hit1 && hit0, hit0))
+      s2xlate_hit && asid_hit && Mux(level.getOrElse(0.U) === 2.U, hit2 && hit1 && hit0, Mux(level.getOrElse(0.U) === 1.U, hit1 && hit0, hit0))
     } else if (hasLevel) {
       val hit0 = tag(tagLen - 1, tagLen - vpnnLen) === vpn(vpnLen - 1, vpnLen - vpnnLen)
       val hit1 = tag(tagLen - vpnnLen - 1, tagLen - vpnnLen * 2) === vpn(vpnLen - vpnnLen - 1, vpnLen - vpnnLen * 2)
 
-      asid_hit && Mux(level.getOrElse(0.U) === 0.U, hit0, hit0 && hit1)
+      s2xlate_hit && asid_hit && Mux(level.getOrElse(0.U) === 0.U, hit0, hit0 && hit1)
     } else {
-      asid_hit && tag === vpn(vpnLen - 1, vpnLen - tagLen)
+      s2xlate_hit && asid_hit && tag === vpn(vpnLen - 1, vpnLen - tagLen)
     }
   }
 
-  def refill(vpn: UInt, asid: UInt, pte: UInt, level: UInt = 0.U, prefetch: Bool, valid: Bool = false.B) {
+  def refill(vpn: UInt, asid: UInt, pte: UInt, level: UInt = 0.U, prefetch: Bool, valid: Bool = false.B, vmid: UInt = 0.U, s2xlate: Bool = false.B, gvpn: UInt = 0.U) {
     require(this.asid.getWidth <= asid.getWidth) // maybe equal is better, but ugly outside
 
     tag := vpn(vpnLen - 1, vpnLen - tagLen)
@@ -603,11 +658,14 @@ class PtwEntry(tagLen: Int, hasPerm: Boolean = false, hasLevel: Boolean = false)
     this.prefetch := prefetch
     this.v := valid
     this.level.map(_ := level)
+    this.vmid := vmid
+    this.s2xlate := s2xlate
+    this.gvpn := gvpn
   }
 
-  def genPtwEntry(vpn: UInt, asid: UInt, pte: UInt, level: UInt = 0.U, prefetch: Bool, valid: Bool = false.B) = {
+  def genPtwEntry(vpn: UInt, asid: UInt, pte: UInt, level: UInt = 0.U, prefetch: Bool, valid: Bool = false.B, vmid: UInt = 0.U, s2xlate: Bool = false.B, gvpn: UInt = 0.U) = {
     val e = Wire(new PtwEntry(tagLen, hasPerm, hasLevel))
-    e.refill(vpn, asid, pte, level, prefetch, valid)
+    e.refill(vpn, asid, pte, level, prefetch, valid, vmid, s2xlate, gvpn)
     e
   }
 
@@ -632,6 +690,7 @@ class PtwEntries(num: Int, tagLen: Int, level: Int, hasPerm: Boolean)(implicit p
   val vs   = Vec(num, Bool())
   val perms = if (hasPerm) Some(Vec(num, new PtePermBundle)) else None
   val prefetch = Bool()
+  val s2xlate = Bool() // this entry is guest machine page
   // println(s"PtwEntries: tag:1*${tagLen} ppns:${num}*${ppnLen} vs:${num}*1")
   // NOTE: vs is used for different usage:
   // for l3, which store the leaf(leaves), vs is page fault or not.
@@ -650,12 +709,13 @@ class PtwEntries(num: Int, tagLen: Int, level: Int, hasPerm: Boolean)(implicit p
     getVpnClip(vpn, level)(log2Up(num) - 1, 0)
   }
 
-  def hit(vpn: UInt, asid: UInt, ignoreAsid: Boolean = false) = {
+  def hit(vpn: UInt, asid: UInt, ignoreAsid: Boolean = false, s2xlate: Bool) = {
     val asid_hit = if (ignoreAsid) true.B else (this.asid === asid)
-    asid_hit && tag === tagClip(vpn) && (if (hasPerm) true.B else vs(sectorIdxClip(vpn, level)))
+    val s2xlate_hit = this.s2xlate === s2xlate
+    s2xlate_hit && asid_hit && tag === tagClip(vpn) && (if (hasPerm) true.B else vs(sectorIdxClip(vpn, level)))
   }
 
-  def genEntries(vpn: UInt, asid: UInt, data: UInt, levelUInt: UInt, prefetch: Bool) = {
+  def genEntries(vpn: UInt, asid: UInt, data: UInt, levelUInt: UInt, prefetch: Bool, s2xlate: Bool) = {
     require((data.getWidth / XLEN) == num,
       s"input data length must be multiple of pte length: data.length:${data.getWidth} num:${num}")
 
@@ -663,6 +723,7 @@ class PtwEntries(num: Int, tagLen: Int, level: Int, hasPerm: Boolean)(implicit p
     ps.tag := tagClip(vpn)
     ps.asid := asid
     ps.prefetch := prefetch
+    ps.s2xlate := s2xlate
     for (i <- 0 until num) {
       val pte = data((i+1)*XLEN-1, i*XLEN).asTypeOf(new PteBundle)
       ps.ppns(i) := pte.ppn
@@ -727,15 +788,19 @@ class PTWEntriesWithEcc(eccCode: Code, num: Int, tagLen: Int, level: Int, hasPer
     Cat(res).orR
   }
 
-  def gen(vpn: UInt, asid: UInt, data: UInt, levelUInt: UInt, prefetch: Bool) = {
-    this.entries := entries.genEntries(vpn, asid, data, levelUInt, prefetch)
+  def gen(vpn: UInt, asid: UInt, data: UInt, levelUInt: UInt, prefetch: Bool, s2xlate: Bool) = {
+    this.entries := entries.genEntries(vpn, asid, data, levelUInt, prefetch, s2xlate)
     this.encode()
   }
 }
 
 class PtwReq(implicit p: Parameters) extends PtwBundle {
   val vpn = UInt(vpnLen.W)
-
+  val gvpn = UInt(gvpnLen.W)
+  val cmd = TlbCmd() // for 2 stage translate
+  val hyperinst = Bool()
+  val hlvx = Bool()
+  val virt = Bool()
   override def toPrintable: Printable = {
     p"vpn:0x${Hexadecimal(vpn)}"
   }
@@ -749,17 +814,22 @@ class PtwResp(implicit p: Parameters) extends PtwBundle {
   val entry = new PtwEntry(tagLen = vpnLen, hasPerm = true, hasLevel = true)
   val pf = Bool()
   val af = Bool()
+  val gpf = Bool()
 
-  def apply(pf: Bool, af: Bool, level: UInt, pte: PteBundle, vpn: UInt, asid: UInt) = {
+  def apply(pf: Bool, af: Bool, gpf: Bool, level: UInt, pte: PteBundle, vpn: UInt, asid: UInt, gvpn: UInt, vmid:UInt, s2xlate: Bool) = {
     this.entry.level.map(_ := level)
     this.entry.tag := vpn
     this.entry.perm.map(_ := pte.getPerm())
     this.entry.ppn := pte.ppn
     this.entry.prefetch := DontCare
     this.entry.asid := asid
+    this.entry.vmid := vmid // valid when s2xlate is true
     this.entry.v := !pf
+    this.entry.s2xlate := s2xlate
+    this.entry.gvpn := gvpn
     this.pf := pf
     this.af := af
+    this.gpf := gpf
   }
 
   override def toPrintable: Printable = {
diff --git a/src/main/scala/xiangshan/cache/mmu/MMUConst.scala b/src/main/scala/xiangshan/cache/mmu/MMUConst.scala
index fb7a9ba27..ecc0ffda8 100644
--- a/src/main/scala/xiangshan/cache/mmu/MMUConst.scala
+++ b/src/main/scala/xiangshan/cache/mmu/MMUConst.scala
@@ -89,6 +89,7 @@ trait HasTlbConst extends HasXSParameter {
   val ppnLen  = PAddrBits - offLen
   val vpnnLen = 9
   val vpnLen  = VAddrBits - offLen
+  val gvpnLen = GPAddrBits - offLen
   val flagLen = 8
   val pteResLen = XLEN - 44 - 2 - flagLen
   val ppnHignLen = 44 - ppnLen
@@ -143,6 +144,7 @@ trait HasTlbConst extends HasXSParameter {
     val ptePerm = ptwResp.entry.perm.get.asTypeOf(new PtePermBundle().cloneType)
     tp.pf := ptwResp.pf
     tp.af := ptwResp.af
+    tp.gpf := ptwResp.gpf
     tp.d := ptePerm.d
     tp.a := ptePerm.a
     tp.g := ptePerm.g
@@ -150,6 +152,7 @@ trait HasTlbConst extends HasXSParameter {
     tp.x := ptePerm.x
     tp.w := ptePerm.w
     tp.r := ptePerm.r
+    tp.v := ptePerm.v // for H extention, but it will remove after rewrite mmu about H extention
     tp.pm := DontCare
     tp
   }
@@ -194,7 +197,8 @@ trait HasPtwConst extends HasTlbConst with MemoryOpConstants{
 
   // miss queue
   val MissQueueSize = l2tlbParams.ifilterSize + l2tlbParams.dfilterSize
-  val MemReqWidth = l2tlbParams.llptwsize + 1
+  val MemReqWidth = l2tlbParams.llptwsize + 2
+  val HptwReqID = l2tlbParams.llptwsize + 1
   val FsmReqID = l2tlbParams.llptwsize
   val bMemID = log2Up(MemReqWidth)
 
@@ -231,10 +235,31 @@ trait HasPtwConst extends HasTlbConst with MemoryOpConstants{
     Cat(ppn, off, 0.U(log2Up(XLEN/8).W))(PAddrBits-1, 0)
   }
 
+  def MakeGAddr(ppn: UInt, off: UInt) = {
+    require(off.getWidth == 9 || off.getWidth == 11)
+    (Cat(ppn, 0.U(offLen.W)) + Cat(off, 0.U(log2Up(XLEN / 8).W)))(GPAddrBits - 1, 0)
+  }
+
+  def MakeHPaddr(ppn:UInt, level: UInt, gpaddr:UInt) = {
+    val pg_mask0 = ((1 << 30) - 1).U(XLEN.W)
+    val pg_mask1 = ((1 << 21) - 1).U(XLEN.W)
+    val pg_base = Cat(ppn, 0.U(offLen.W))
+    Mux(level === 0.U, (pg_base & ~pg_mask0) | (gpaddr & pg_mask0) ,
+      Mux(level === 1.U, (pg_base & ~pg_mask0) | (gpaddr & pg_mask0),
+        Cat(ppn, gpaddr(offLen - 1, 0))))
+  }
   def getVpnn(vpn: UInt, idx: Int): UInt = {
     vpn(vpnnLen*(idx+1)-1, vpnnLen*idx)
   }
 
+  def getVpnn(vpn: UInt, idx: UInt): UInt = {
+    Mux(idx === 0.U, vpn(vpnnLen - 1, 0), Mux(idx === 1.U, vpn(vpnnLen * 2 - 1, vpnnLen), vpn(vpnnLen * 3 - 1, vpnnLen * 2)))
+  }
+
+  def getGVpnn(vpn: UInt, idx: UInt): UInt = {
+    Mux(idx === 0.U, vpn(vpnnLen - 1, 0), Mux(idx === 1.U, vpn(vpnnLen * 2 - 1, vpnnLen), vpn(vpnnLen * 3 + 1, vpnnLen * 2)))
+  }
+
   def getVpnClip(vpn: UInt, level: Int) = {
     // level 0  /* vpnn2 */
     // level 1  /* vpnn2 * vpnn1 */
diff --git a/src/main/scala/xiangshan/cache/mmu/PageTableCache.scala b/src/main/scala/xiangshan/cache/mmu/PageTableCache.scala
index f332dedce..bb61b9c78 100644
--- a/src/main/scala/xiangshan/cache/mmu/PageTableCache.scala
+++ b/src/main/scala/xiangshan/cache/mmu/PageTableCache.scala
@@ -1,774 +1,798 @@
-/***************************************************************************************
-* Copyright (c) 2020-2021 Institute of Computing Technology, Chinese Academy of Sciences
-* Copyright (c) 2020-2021 Peng Cheng Laboratory
-*
-* XiangShan is licensed under Mulan PSL v2.
-* You can use this software according to the terms and conditions of the Mulan PSL v2.
-* You may obtain a copy of Mulan PSL v2 at:
-*          http://license.coscl.org.cn/MulanPSL2
-*
-* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND,
-* EITHER EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT,
-* MERCHANTABILITY OR FIT FOR A PARTICULAR PURPOSE.
-*
-* See the Mulan PSL v2 for more details.
-***************************************************************************************/
-
-package xiangshan.cache.mmu
-
-import chipsalliance.rocketchip.config.Parameters
-import chisel3._
-import chisel3.util._
-import chisel3.internal.naming.chiselName
-import xiangshan._
-import xiangshan.cache.{HasDCacheParameters, MemoryOpConstants}
-import utils._
-import utility._
-import freechips.rocketchip.diplomacy.{LazyModule, LazyModuleImp}
-import freechips.rocketchip.tilelink._
-
-/* ptw cache caches the page table of all the three layers
- * ptw cache resp at next cycle
- * the cache should not be blocked
- * when miss queue if full, just block req outside
- */
-
-class PageCachePerPespBundle(implicit p: Parameters) extends PtwBundle {
-  val hit = Bool()
-  val pre = Bool()
-  val ppn = UInt(ppnLen.W)
-  val perm = new PtePermBundle()
-  val ecc = Bool()
-  val level = UInt(2.W)
-  val v = Bool()
-
-  def apply(hit: Bool, pre: Bool, ppn: UInt, perm: PtePermBundle = 0.U.asTypeOf(new PtePermBundle()),
-            ecc: Bool = false.B, level: UInt = 0.U, valid: Bool = true.B) {
-    this.hit := hit && !ecc
-    this.pre := pre
-    this.ppn := ppn
-    this.perm := perm
-    this.ecc := ecc && hit
-    this.level := level
-    this.v := valid
-  }
-}
-
-class PageCacheRespBundle(implicit p: Parameters) extends PtwBundle {
-  val l1 = new PageCachePerPespBundle
-  val l2 = new PageCachePerPespBundle
-  val l3 = new PageCachePerPespBundle
-  val sp = new PageCachePerPespBundle
-}
-
-class PtwCacheReq(implicit p: Parameters) extends PtwBundle {
-  val req_info = new L2TlbInnerBundle()
-  val isFirst = Bool()
-  val bypassed = Vec(3, Bool())
-}
-
-class PtwCacheIO()(implicit p: Parameters) extends MMUIOBaseBundle with HasPtwConst {
-  val req = Flipped(DecoupledIO(new PtwCacheReq()))
-  val resp = DecoupledIO(new Bundle {
-    val req_info = new L2TlbInnerBundle()
-    val isFirst = Bool()
-    val hit = Bool()
-    val prefetch = Bool() // is the entry fetched by prefetch
-    val bypassed = Bool()
-    val toFsm = new Bundle {
-      val l1Hit = Bool()
-      val l2Hit = Bool()
-      val ppn = UInt(ppnLen.W)
-    }
-    val toTlb = new PtwEntry(tagLen = vpnLen, hasPerm = true, hasLevel = true)
-  })
-  val refill = Flipped(ValidIO(new Bundle {
-    val ptes = UInt(blockBits.W)
-    val levelOH = new Bundle {
-      // NOTE: levelOH has (Level+1) bits, each stands for page cache entries
-      val sp = Bool()
-      val l3 = Bool()
-      val l2 = Bool()
-      val l1 = Bool()
-      def apply(levelUInt: UInt, valid: Bool) = {
-        sp := RegNext((levelUInt === 0.U || levelUInt === 1.U) && valid, false.B)
-        l3 := RegNext((levelUInt === 2.U) & valid, false.B)
-        l2 := RegNext((levelUInt === 1.U) & valid, false.B)
-        l1 := RegNext((levelUInt === 0.U) & valid, false.B)
-      }
-    }
-    // duplicate level and sel_pte for each page caches, for better fanout
-    val req_info_dup = Vec(3, new L2TlbInnerBundle())
-    val level_dup = Vec(3, UInt(log2Up(Level).W))
-    val sel_pte_dup = Vec(3, UInt(XLEN.W))
-  }))
-  val sfence_dup = Vec(4, Input(new SfenceBundle()))
-  val csr_dup = Vec(3, Input(new TlbCsrBundle()))
-}
-
-@chiselName
-class PtwCache()(implicit p: Parameters) extends XSModule with HasPtwConst with HasPerfEvents {
-  val io = IO(new PtwCacheIO)
-
-  val ecc = Code.fromString(l2tlbParams.ecc)
-  val l2EntryType = new PTWEntriesWithEcc(ecc, num = PtwL2SectorSize, tagLen = PtwL2TagLen, level = 1, hasPerm = false)
-  val l3EntryType = new PTWEntriesWithEcc(ecc, num = PtwL3SectorSize, tagLen = PtwL3TagLen, level = 2, hasPerm = true)
-
-  // TODO: four caches make the codes dirty, think about how to deal with it
-
-  val sfence_dup = io.sfence_dup
-  val refill = io.refill.bits
-  val refill_prefetch_dup = io.refill.bits.req_info_dup.map(a => from_pre(a.source))
-  val flush_dup = sfence_dup.zip(io.csr_dup).map(f => f._1.valid || f._2.satp.changed)
-  val flush = flush_dup(0)
-
-  // when refill, refuce to accept new req
-  val rwHarzad = if (sramSinglePort) io.refill.valid else false.B
-
-  // handle hand signal and req_info
-  // TODO: replace with FlushableQueue
-  val stageReq = Wire(Decoupled(new PtwCacheReq()))         // enq stage & read page cache valid
-  val stageDelay = Wire(Vec(2, Decoupled(new PtwCacheReq()))) // page cache resp
-  val stageCheck = Wire(Vec(2, Decoupled(new PtwCacheReq()))) // check hit & check ecc
-  val stageResp = Wire(Decoupled(new PtwCacheReq()))         // deq stage
-
-  val stageDelay_valid_1cycle = OneCycleValid(stageReq.fire, flush)      // catch ram data
-  val stageCheck_valid_1cycle = OneCycleValid(stageDelay(1).fire, flush) // replace & perf counter
-  val stageResp_valid_1cycle_dup = Wire(Vec(2, Bool()))
-  stageResp_valid_1cycle_dup.map(_ := OneCycleValid(stageCheck(1).fire, flush))  // ecc flush
-
-  stageReq <> io.req
-  PipelineConnect(stageReq, stageDelay(0), stageDelay(1).ready, flush, rwHarzad)
-  InsideStageConnect(stageDelay(0), stageDelay(1), stageDelay_valid_1cycle)
-  PipelineConnect(stageDelay(1), stageCheck(0), stageCheck(1).ready, flush)
-  InsideStageConnect(stageCheck(0), stageCheck(1), stageCheck_valid_1cycle)
-  PipelineConnect(stageCheck(1), stageResp, io.resp.ready, flush)
-  stageResp.ready := !stageResp.valid || io.resp.ready
-
-  // l1: level 0 non-leaf pte
-  val l1 = Reg(Vec(l2tlbParams.l1Size, new PtwEntry(tagLen = PtwL1TagLen)))
-  val l1v = RegInit(0.U(l2tlbParams.l1Size.W))
-  val l1g = Reg(UInt(l2tlbParams.l1Size.W))
-  val l1asids = l1.map(_.asid)
-
-  // l2: level 1 non-leaf pte
-  val l2 = Module(new SRAMTemplate(
-    l2EntryType,
-    set = l2tlbParams.l2nSets,
-    way = l2tlbParams.l2nWays,
-    singlePort = sramSinglePort
-  ))
-  val l2v = RegInit(0.U((l2tlbParams.l2nSets * l2tlbParams.l2nWays).W))
-  val l2g = Reg(UInt((l2tlbParams.l2nSets * l2tlbParams.l2nWays).W))
-  val l2asids = Reg(Vec(l2tlbParams.l2nSets, Vec(l2tlbParams.l2nWays, UInt(AsidLength.W))))
-  def getl2vSet(vpn: UInt) = {
-    require(log2Up(l2tlbParams.l2nWays) == log2Down(l2tlbParams.l2nWays))
-    val set = genPtwL2SetIdx(vpn)
-    require(set.getWidth == log2Up(l2tlbParams.l2nSets))
-    val l2vVec = l2v.asTypeOf(Vec(l2tlbParams.l2nSets, UInt(l2tlbParams.l2nWays.W)))
-    l2vVec(set)
-  }
-  def getl2asidSet(vpn: UInt) = {
-    require(log2Up(l2tlbParams.l2nWays) == log2Down(l2tlbParams.l2nWays))
-    val set = genPtwL2SetIdx(vpn)
-    require(set.getWidth == log2Up(l2tlbParams.l2nSets))
-    l2asids(set)
-  }
-
-  // l3: level 2 leaf pte of 4KB pages
-  val l3 = Module(new SRAMTemplate(
-    l3EntryType,
-    set = l2tlbParams.l3nSets,
-    way = l2tlbParams.l3nWays,
-    singlePort = sramSinglePort
-  ))
-  val l3v = RegInit(0.U((l2tlbParams.l3nSets * l2tlbParams.l3nWays).W))
-  val l3g = Reg(UInt((l2tlbParams.l3nSets * l2tlbParams.l3nWays).W))
-  val l3asids = Reg(Vec(l2tlbParams.l3nSets, Vec(l2tlbParams.l3nWays, UInt(AsidLength.W))))
-  def getl3vSet(vpn: UInt) = {
-    require(log2Up(l2tlbParams.l3nWays) == log2Down(l2tlbParams.l3nWays))
-    val set = genPtwL3SetIdx(vpn)
-    require(set.getWidth == log2Up(l2tlbParams.l3nSets))
-    val l3vVec = l3v.asTypeOf(Vec(l2tlbParams.l3nSets, UInt(l2tlbParams.l3nWays.W)))
-    l3vVec(set)
-  }
-  def getl3asidSet(vpn: UInt) = {
-    require(log2Up(l2tlbParams.l3nWays) == log2Down(l2tlbParams.l3nWays))
-    val set = genPtwL3SetIdx(vpn)
-    require(set.getWidth == log2Up(l2tlbParams.l3nSets))
-    l3asids(set)
-  }
-
-  // sp: level 0/1 leaf pte of 1GB/2MB super pages
-  val sp = Reg(Vec(l2tlbParams.spSize, new PtwEntry(tagLen = SPTagLen, hasPerm = true, hasLevel = true)))
-  val spv = RegInit(0.U(l2tlbParams.spSize.W))
-  val spg = Reg(UInt(l2tlbParams.spSize.W))
-  val spasids = sp.map(_.asid)
-
-  // Access Perf
-  val l1AccessPerf = Wire(Vec(l2tlbParams.l1Size, Bool()))
-  val l2AccessPerf = Wire(Vec(l2tlbParams.l2nWays, Bool()))
-  val l3AccessPerf = Wire(Vec(l2tlbParams.l3nWays, Bool()))
-  val spAccessPerf = Wire(Vec(l2tlbParams.spSize, Bool()))
-  l1AccessPerf.map(_ := false.B)
-  l2AccessPerf.map(_ := false.B)
-  l3AccessPerf.map(_ := false.B)
-  spAccessPerf.map(_ := false.B)
-
-
-
-  def vpn_match(vpn1: UInt, vpn2: UInt, level: Int) = {
-    vpn1(vpnnLen*3-1, vpnnLen*(2-level)+3) === vpn2(vpnnLen*3-1, vpnnLen*(2-level)+3)
-  }
-  // NOTE: not actually bypassed, just check if hit, re-access the page cache
-  def refill_bypass(vpn: UInt, level: Int) = {
-    io.refill.valid && (level.U === io.refill.bits.level_dup(0)) && vpn_match(io.refill.bits.req_info_dup(0).vpn, vpn, level),
-  }
-
-  // l1
-  val ptwl1replace = ReplacementPolicy.fromString(l2tlbParams.l1Replacer, l2tlbParams.l1Size)
-  val (l1Hit, l1HitPPN, l1Pre) = {
-    val hitVecT = l1.zipWithIndex.map { case (e, i) => e.hit(stageReq.bits.req_info.vpn, io.csr_dup(0).satp.asid) && l1v(i) }
-    val hitVec = hitVecT.map(RegEnable(_, stageReq.fire))
-
-    // stageDelay, but check for l1
-    val hitPPN = DataHoldBypass(ParallelPriorityMux(hitVec zip l1.map(_.ppn)), stageDelay_valid_1cycle)
-    val hitPre = DataHoldBypass(ParallelPriorityMux(hitVec zip l1.map(_.prefetch)), stageDelay_valid_1cycle)
-    val hit = DataHoldBypass(ParallelOR(hitVec), stageDelay_valid_1cycle)
-
-    when (hit && stageDelay_valid_1cycle) { ptwl1replace.access(OHToUInt(hitVec)) }
-
-    l1AccessPerf.zip(hitVec).map{ case (l, h) => l := h && stageDelay_valid_1cycle}
-    for (i <- 0 until l2tlbParams.l1Size) {
-      XSDebug(stageReq.fire, p"[l1] l1(${i.U}) ${l1(i)} hit:${l1(i).hit(stageReq.bits.req_info.vpn, io.csr_dup(0).satp.asid)}\n")
-    }
-    XSDebug(stageReq.fire, p"[l1] l1v:${Binary(l1v)} hitVecT:${Binary(VecInit(hitVecT).asUInt)}\n")
-    XSDebug(stageDelay(0).valid, p"[l1] l1Hit:${hit} l1HitPPN:0x${Hexadecimal(hitPPN)} hitVec:${VecInit(hitVec).asUInt}\n")
-
-    VecInit(hitVecT).suggestName(s"l1_hitVecT")
-    VecInit(hitVec).suggestName(s"l1_hitVec")
-
-    // synchronize with other entries with RegEnable
-    (RegEnable(hit, stageDelay(1).fire),
-     RegEnable(hitPPN, stageDelay(1).fire),
-     RegEnable(hitPre, stageDelay(1).fire))
-  }
-
-  // l2
-  val ptwl2replace = ReplacementPolicy.fromString(l2tlbParams.l2Replacer,l2tlbParams.l2nWays,l2tlbParams.l2nSets)
-  val (l2Hit, l2HitPPN, l2Pre, l2eccError) = {
-    val ridx = genPtwL2SetIdx(stageReq.bits.req_info.vpn)
-    l2.io.r.req.valid := stageReq.fire
-    l2.io.r.req.bits.apply(setIdx = ridx)
-    val vVec_req = getl2vSet(stageReq.bits.req_info.vpn)
-
-    // delay one cycle after sram read
-    val delay_vpn = stageDelay(0).bits.req_info.vpn
-    val data_resp = DataHoldBypass(l2.io.r.resp.data, stageDelay_valid_1cycle)
-    val vVec_delay = RegEnable(vVec_req, stageReq.fire)
-    val hitVec_delay = VecInit(data_resp.zip(vVec_delay.asBools).map { case (wayData, v) =>
-      wayData.entries.hit(delay_vpn, io.csr_dup(1).satp.asid) && v })
-
-    // check hit and ecc
-    val check_vpn = stageCheck(0).bits.req_info.vpn
-    val ramDatas = RegEnable(data_resp, stageDelay(1).fire)
-    val vVec = RegEnable(vVec_delay, stageDelay(1).fire).asBools()
-
-    val hitVec = RegEnable(hitVec_delay, stageDelay(1).fire)
-    val hitWayEntry = ParallelPriorityMux(hitVec zip ramDatas)
-    val hitWayData = hitWayEntry.entries
-    val hit = ParallelOR(hitVec)
-    val hitWay = ParallelPriorityMux(hitVec zip (0 until l2tlbParams.l2nWays).map(_.U(log2Up(l2tlbParams.l2nWays).W)))
-    val eccError = hitWayEntry.decode()
-
-    ridx.suggestName(s"l2_ridx")
-    ramDatas.suggestName(s"l2_ramDatas")
-    hitVec.suggestName(s"l2_hitVec")
-    hitWayData.suggestName(s"l2_hitWayData")
-    hitWay.suggestName(s"l2_hitWay")
-
-    when (hit && stageCheck_valid_1cycle) { ptwl2replace.access(genPtwL2SetIdx(check_vpn), hitWay) }
-
-    l2AccessPerf.zip(hitVec).map{ case (l, h) => l := h && stageCheck_valid_1cycle }
-    XSDebug(stageDelay_valid_1cycle, p"[l2] ridx:0x${Hexadecimal(ridx)}\n")
-    for (i <- 0 until l2tlbParams.l2nWays) {
-      XSDebug(stageCheck_valid_1cycle, p"[l2] ramDatas(${i.U}) ${ramDatas(i)}  l2v:${vVec(i)}  hit:${hit}\n")
-    }
-    XSDebug(stageCheck_valid_1cycle, p"[l2] l2Hit:${hit} l2HitPPN:0x${Hexadecimal(hitWayData.ppns(genPtwL2SectorIdx(check_vpn)))} hitVec:${Binary(hitVec.asUInt)} hitWay:${hitWay} vidx:${vVec}\n")
-
-    (hit, hitWayData.ppns(genPtwL2SectorIdx(check_vpn)), hitWayData.prefetch, eccError)
-  }
-
-  // l3
-  val ptwl3replace = ReplacementPolicy.fromString(l2tlbParams.l3Replacer,l2tlbParams.l3nWays,l2tlbParams.l3nSets)
-  val (l3Hit, l3HitData, l3Pre, l3eccError) = {
-    val ridx = genPtwL3SetIdx(stageReq.bits.req_info.vpn)
-    l3.io.r.req.valid := stageReq.fire
-    l3.io.r.req.bits.apply(setIdx = ridx)
-    val vVec_req = getl3vSet(stageReq.bits.req_info.vpn)
-
-    // delay one cycle after sram read
-    val delay_vpn = stageDelay(0).bits.req_info.vpn
-    val data_resp = DataHoldBypass(l3.io.r.resp.data, stageDelay_valid_1cycle)
-    val vVec_delay = RegEnable(vVec_req, stageReq.fire)
-    val hitVec_delay = VecInit(data_resp.zip(vVec_delay.asBools).map { case (wayData, v) =>
-      wayData.entries.hit(delay_vpn, io.csr_dup(2).satp.asid) && v })
-
-    // check hit and ecc
-    val check_vpn = stageCheck(0).bits.req_info.vpn
-    val ramDatas = RegEnable(data_resp, stageDelay(1).fire)
-    val vVec = RegEnable(vVec_delay, stageDelay(1).fire).asBools()
-
-    val hitVec = RegEnable(hitVec_delay, stageDelay(1).fire)
-    val hitWayEntry = ParallelPriorityMux(hitVec zip ramDatas)
-    val hitWayData = hitWayEntry.entries
-    val hitWayEcc = hitWayEntry.ecc
-    val hit = ParallelOR(hitVec)
-    val hitWay = ParallelPriorityMux(hitVec zip (0 until l2tlbParams.l3nWays).map(_.U(log2Up(l2tlbParams.l3nWays).W)))
-    val eccError = hitWayEntry.decode()
-
-    when (hit && stageCheck_valid_1cycle) { ptwl3replace.access(genPtwL3SetIdx(check_vpn), hitWay) }
-
-    l3AccessPerf.zip(hitVec).map{ case (l, h) => l := h && stageCheck_valid_1cycle }
-    XSDebug(stageReq.fire, p"[l3] ridx:0x${Hexadecimal(ridx)}\n")
-    for (i <- 0 until l2tlbParams.l3nWays) {
-      XSDebug(stageCheck_valid_1cycle, p"[l3] ramDatas(${i.U}) ${ramDatas(i)}  l3v:${vVec(i)}  hit:${hitVec(i)}\n")
-    }
-    XSDebug(stageCheck_valid_1cycle, p"[l3] l3Hit:${hit} l3HitData:${hitWayData} hitVec:${Binary(hitVec.asUInt)} hitWay:${hitWay} v:${vVec}\n")
-
-    ridx.suggestName(s"l3_ridx")
-    ramDatas.suggestName(s"l3_ramDatas")
-    hitVec.suggestName(s"l3_hitVec")
-    hitWay.suggestName(s"l3_hitWay")
-
-    (hit, hitWayData, hitWayData.prefetch, eccError)
-  }
-  val l3HitPPN = l3HitData.ppns(genPtwL3SectorIdx(stageCheck(0).bits.req_info.vpn))
-  val l3HitPerm = l3HitData.perms.getOrElse(0.U.asTypeOf(Vec(PtwL3SectorSize, new PtePermBundle)))(genPtwL3SectorIdx(stageCheck(0).bits.req_info.vpn))
-  val l3HitValid = l3HitData.vs(genPtwL3SectorIdx(stageCheck(0).bits.req_info.vpn))
-
-  // super page
-  val spreplace = ReplacementPolicy.fromString(l2tlbParams.spReplacer, l2tlbParams.spSize)
-  val (spHit, spHitData, spPre, spValid) = {
-    val hitVecT = sp.zipWithIndex.map { case (e, i) => e.hit(stageReq.bits.req_info.vpn, io.csr_dup(0).satp.asid) && spv(i) }
-    val hitVec = hitVecT.map(RegEnable(_, stageReq.fire))
-    val hitData = ParallelPriorityMux(hitVec zip sp)
-    val hit = ParallelOR(hitVec)
-
-    when (hit && stageDelay_valid_1cycle) { spreplace.access(OHToUInt(hitVec)) }
-
-    spAccessPerf.zip(hitVec).map{ case (s, h) => s := h && stageDelay_valid_1cycle }
-    for (i <- 0 until l2tlbParams.spSize) {
-      XSDebug(stageReq.fire, p"[sp] sp(${i.U}) ${sp(i)} hit:${sp(i).hit(stageReq.bits.req_info.vpn, io.csr_dup(0).satp.asid)} spv:${spv(i)}\n")
-    }
-    XSDebug(stageDelay_valid_1cycle, p"[sp] spHit:${hit} spHitData:${hitData} hitVec:${Binary(VecInit(hitVec).asUInt)}\n")
-
-    VecInit(hitVecT).suggestName(s"sp_hitVecT")
-    VecInit(hitVec).suggestName(s"sp_hitVec")
-
-    (RegEnable(hit, stageDelay(1).fire),
-     RegEnable(hitData, stageDelay(1).fire),
-     RegEnable(hitData.prefetch, stageDelay(1).fire),
-     RegEnable(hitData.v, stageDelay(1).fire()))
-  }
-  val spHitPerm = spHitData.perm.getOrElse(0.U.asTypeOf(new PtePermBundle))
-  val spHitLevel = spHitData.level.getOrElse(0.U)
-
-  val check_res = Wire(new PageCacheRespBundle)
-  check_res.l1.apply(l1Hit, l1Pre, l1HitPPN)
-  check_res.l2.apply(l2Hit, l2Pre, l2HitPPN, ecc = l2eccError)
-  check_res.l3.apply(l3Hit, l3Pre, l3HitPPN, l3HitPerm, l3eccError, valid = l3HitValid)
-  check_res.sp.apply(spHit, spPre, spHitData.ppn, spHitPerm, false.B, spHitLevel, spValid)
-
-  val resp_res = Reg(new PageCacheRespBundle)
-  when (stageCheck(1).fire) { resp_res := check_res }
-
-  // stageResp bypass
-  val bypassed = Wire(Vec(3, Bool()))
-  bypassed.indices.foreach(i =>
-    bypassed(i) := stageResp.bits.bypassed(i) ||
-      ValidHoldBypass(refill_bypass(stageResp.bits.req_info.vpn, i),
-        OneCycleValid(stageCheck(1).fire, false.B) || io.refill.valid)
-  )
-
-  io.resp.bits.req_info   := stageResp.bits.req_info
-  io.resp.bits.isFirst  := stageResp.bits.isFirst
-  io.resp.bits.hit      := resp_res.l3.hit || resp_res.sp.hit
-  io.resp.bits.bypassed := bypassed(2) || (bypassed(1) && !resp_res.l2.hit) || (bypassed(0) && !resp_res.l1.hit)
-  io.resp.bits.prefetch := resp_res.l3.pre && resp_res.l3.hit || resp_res.sp.pre && resp_res.sp.hit
-  io.resp.bits.toFsm.l1Hit := resp_res.l1.hit
-  io.resp.bits.toFsm.l2Hit := resp_res.l2.hit
-  io.resp.bits.toFsm.ppn   := Mux(resp_res.l2.hit, resp_res.l2.ppn, resp_res.l1.ppn)
-  io.resp.bits.toTlb.tag   := stageResp.bits.req_info.vpn
-  io.resp.bits.toTlb.asid  := io.csr_dup(0).satp.asid // DontCare
-  io.resp.bits.toTlb.ppn   := Mux(resp_res.l3.hit, resp_res.l3.ppn, resp_res.sp.ppn)
-  io.resp.bits.toTlb.perm.map(_ := Mux(resp_res.l3.hit, resp_res.l3.perm, resp_res.sp.perm))
-  io.resp.bits.toTlb.level.map(_ := Mux(resp_res.l3.hit, 2.U, resp_res.sp.level))
-  io.resp.bits.toTlb.prefetch := from_pre(stageResp.bits.req_info.source)
-  io.resp.bits.toTlb.v := Mux(resp_res.sp.hit, resp_res.sp.v, resp_res.l3.v)
-  io.resp.valid := stageResp.valid
-  XSError(stageResp.valid && resp_res.l3.hit && resp_res.sp.hit, "normal page and super page both hit")
-  XSError(stageResp.valid && io.resp.bits.hit && bypassed(2), "page cache, bypassed but hit")
-
-  // refill Perf
-  val l1RefillPerf = Wire(Vec(l2tlbParams.l1Size, Bool()))
-  val l2RefillPerf = Wire(Vec(l2tlbParams.l2nWays, Bool()))
-  val l3RefillPerf = Wire(Vec(l2tlbParams.l3nWays, Bool()))
-  val spRefillPerf = Wire(Vec(l2tlbParams.spSize, Bool()))
-  l1RefillPerf.map(_ := false.B)
-  l2RefillPerf.map(_ := false.B)
-  l3RefillPerf.map(_ := false.B)
-  spRefillPerf.map(_ := false.B)
-
-  // refill
-  l2.io.w.req <> DontCare
-  l3.io.w.req <> DontCare
-  l2.io.w.req.valid := false.B
-  l3.io.w.req.valid := false.B
-
-  val memRdata = refill.ptes
-  val memPtes = (0 until (l2tlbParams.blockBytes/(XLEN/8))).map(i => memRdata((i+1)*XLEN-1, i*XLEN).asTypeOf(new PteBundle))
-  val memSelData = io.refill.bits.sel_pte_dup
-  val memPte = memSelData.map(a => a.asTypeOf(new PteBundle))
-
-  // TODO: handle sfenceLatch outsize
-  when (!flush_dup(0) && refill.levelOH.l1 && !memPte(0).isLeaf() && !memPte(0).isPf(refill.level_dup(0)) && !memPte(0).isAf()) {
-    // val refillIdx = LFSR64()(log2Up(l2tlbParams.l1Size)-1,0) // TODO: may be LRU
-    val refillIdx = replaceWrapper(l1v, ptwl1replace.way)
-    refillIdx.suggestName(s"PtwL1RefillIdx")
-    val rfOH = UIntToOH(refillIdx)
-    l1(refillIdx).refill(
-      refill.req_info_dup(0).vpn,
-      io.csr_dup(0).satp.asid,
-      memSelData(0),
-      0.U,
-      refill_prefetch_dup(0)
-    )
-    ptwl1replace.access(refillIdx)
-    l1v := l1v | rfOH
-    l1g := (l1g & ~rfOH) | Mux(memPte(0).perm.g, rfOH, 0.U)
-
-    for (i <- 0 until l2tlbParams.l1Size) {
-      l1RefillPerf(i) := i.U === refillIdx
-    }
-
-    XSDebug(p"[l1 refill] refillIdx:${refillIdx} refillEntry:${l1(refillIdx).genPtwEntry(refill.req_info_dup(0).vpn, io.csr_dup(0).satp.asid, memSelData(0), 0.U, prefetch = refill_prefetch_dup(0))}\n")
-    XSDebug(p"[l1 refill] l1v:${Binary(l1v)}->${Binary(l1v | rfOH)} l1g:${Binary(l1g)}->${Binary((l1g & ~rfOH) | Mux(memPte(0).perm.g, rfOH, 0.U))}\n")
-
-    refillIdx.suggestName(s"l1_refillIdx")
-    rfOH.suggestName(s"l1_rfOH")
-  }
-
-  when (!flush_dup(1) && refill.levelOH.l2 && !memPte(1).isLeaf() && !memPte(1).isPf(refill.level_dup(1)) && !memPte(1).isAf()) {
-    val refillIdx = genPtwL2SetIdx(refill.req_info_dup(1).vpn)
-    val victimWay = replaceWrapper(getl2vSet(refill.req_info_dup(1).vpn), ptwl2replace.way(refillIdx))
-    val victimWayOH = UIntToOH(victimWay)
-    val rfvOH = UIntToOH(Cat(refillIdx, victimWay))
-    val wdata = Wire(l2EntryType)
-    wdata.gen(
-      vpn = refill.req_info_dup(1).vpn,
-      asid = io.csr_dup(1).satp.asid,
-      data = memRdata,
-      levelUInt = 1.U,
-      refill_prefetch_dup(1)
-    )
-    l2.io.w.apply(
-      valid = true.B,
-      setIdx = refillIdx,
-      data = wdata,
-      waymask = victimWayOH
-    )
-    ptwl2replace.access(refillIdx, victimWay)
-    l2v := l2v | rfvOH
-    l2g := l2g & ~rfvOH | Mux(Cat(memPtes.map(_.perm.g)).andR, rfvOH, 0.U)
-
-    for (i <- 0 until l2tlbParams.l2nWays) {
-      l2RefillPerf(i) := i.U === victimWay
-    }
-
-    XSDebug(p"[l2 refill] refillIdx:0x${Hexadecimal(refillIdx)} victimWay:${victimWay} victimWayOH:${Binary(victimWayOH)} rfvOH(in UInt):${Cat(refillIdx, victimWay)}\n")
-    XSDebug(p"[l2 refill] refilldata:0x${wdata}\n")
-    XSDebug(p"[l2 refill] l2v:${Binary(l2v)} -> ${Binary(l2v | rfvOH)}\n")
-    XSDebug(p"[l2 refill] l2g:${Binary(l2g)} -> ${Binary(l2g & ~rfvOH | Mux(Cat(memPtes.map(_.perm.g)).andR, rfvOH, 0.U))}\n")
-
-    refillIdx.suggestName(s"l2_refillIdx")
-    victimWay.suggestName(s"l2_victimWay")
-    victimWayOH.suggestName(s"l2_victimWayOH")
-    rfvOH.suggestName(s"l2_rfvOH")
-  }
-
-  when (!flush_dup(2) && refill.levelOH.l3 && !memPte(2).isAf()) {
-    val refillIdx = genPtwL3SetIdx(refill.req_info_dup(2).vpn)
-    val victimWay = replaceWrapper(getl3vSet(refill.req_info_dup(2).vpn), ptwl3replace.way(refillIdx))
-    val victimWayOH = UIntToOH(victimWay)
-    val rfvOH = UIntToOH(Cat(refillIdx, victimWay))
-    val wdata = Wire(l3EntryType)
-    wdata.gen(
-      vpn = refill.req_info_dup(2).vpn,
-      asid = io.csr_dup(2).satp.asid,
-      data = memRdata,
-      levelUInt = 2.U,
-      refill_prefetch_dup(2)
-    )
-    l3.io.w.apply(
-      valid = true.B,
-      setIdx = refillIdx,
-      data = wdata,
-      waymask = victimWayOH
-    )
-    ptwl3replace.access(refillIdx, victimWay)
-    l3v := l3v | rfvOH
-    l3g := l3g & ~rfvOH | Mux(Cat(memPtes.map(_.perm.g)).andR, rfvOH, 0.U)
-
-    for (i <- 0 until l2tlbParams.l3nWays) {
-      l3RefillPerf(i) := i.U === victimWay
-    }
-
-    XSDebug(p"[l3 refill] refillIdx:0x${Hexadecimal(refillIdx)} victimWay:${victimWay} victimWayOH:${Binary(victimWayOH)} rfvOH(in UInt):${Cat(refillIdx, victimWay)}\n")
-    XSDebug(p"[l3 refill] refilldata:0x${wdata}\n")
-    XSDebug(p"[l3 refill] l3v:${Binary(l3v)} -> ${Binary(l3v | rfvOH)}\n")
-    XSDebug(p"[l3 refill] l3g:${Binary(l3g)} -> ${Binary(l3g & ~rfvOH | Mux(Cat(memPtes.map(_.perm.g)).andR, rfvOH, 0.U))}\n")
-
-    refillIdx.suggestName(s"l3_refillIdx")
-    victimWay.suggestName(s"l3_victimWay")
-    victimWayOH.suggestName(s"l3_victimWayOH")
-    rfvOH.suggestName(s"l3_rfvOH")
-  }
-
-
-  // misc entries: super & invalid
-  when (!flush_dup(0) && refill.levelOH.sp && (memPte(0).isLeaf() || memPte(0).isPf(refill.level_dup(0))) && !memPte(0).isAf()) {
-    val refillIdx = spreplace.way// LFSR64()(log2Up(l2tlbParams.spSize)-1,0) // TODO: may be LRU
-    val rfOH = UIntToOH(refillIdx)
-    sp(refillIdx).refill(
-      refill.req_info_dup(0).vpn,
-      io.csr_dup(0).satp.asid,
-      memSelData(0),
-      refill.level_dup(2),
-      refill_prefetch_dup(0),
-      !memPte(0).isPf(refill.level_dup(0)),
-    )
-    spreplace.access(refillIdx)
-    spv := spv | rfOH
-    spg := spg & ~rfOH | Mux(memPte(0).perm.g, rfOH, 0.U)
-
-    for (i <- 0 until l2tlbParams.spSize) {
-      spRefillPerf(i) := i.U === refillIdx
-    }
-
-    XSDebug(p"[sp refill] refillIdx:${refillIdx} refillEntry:${sp(refillIdx).genPtwEntry(refill.req_info_dup(0).vpn, io.csr_dup(0).satp.asid, memSelData(0), refill.level_dup(0), refill_prefetch_dup(0))}\n")
-    XSDebug(p"[sp refill] spv:${Binary(spv)}->${Binary(spv | rfOH)} spg:${Binary(spg)}->${Binary(spg & ~rfOH | Mux(memPte(0).perm.g, rfOH, 0.U))}\n")
-
-    refillIdx.suggestName(s"sp_refillIdx")
-    rfOH.suggestName(s"sp_rfOH")
-  }
-
-  val l2eccFlush = resp_res.l2.ecc && stageResp_valid_1cycle_dup(0) // RegNext(l2eccError, init = false.B)
-  val l3eccFlush = resp_res.l3.ecc && stageResp_valid_1cycle_dup(1) // RegNext(l3eccError, init = false.B)
-  val eccVpn = stageResp.bits.req_info.vpn
-
-  XSError(l2eccFlush, "l2tlb.cache.l2 ecc error. Should not happen at sim stage")
-  XSError(l3eccFlush, "l2tlb.cache.l3 ecc error. Should not happen at sim stage")
-  when (l2eccFlush) {
-    val flushSetIdxOH = UIntToOH(genPtwL2SetIdx(eccVpn))
-    val flushMask = VecInit(flushSetIdxOH.asBools.map { a => Fill(l2tlbParams.l2nWays, a.asUInt) }).asUInt
-    l2v := l2v & ~flushMask
-    l2g := l2g & ~flushMask
-  }
-
-  when (l3eccFlush) {
-    val flushSetIdxOH = UIntToOH(genPtwL3SetIdx(eccVpn))
-    val flushMask = VecInit(flushSetIdxOH.asBools.map { a => Fill(l2tlbParams.l3nWays, a.asUInt) }).asUInt
-    l3v := l3v & ~flushMask
-    l3g := l3g & ~flushMask
-  }
-
-  // sfence
-  when (sfence_dup(3).valid) {
-    val sfence_vpn = sfence_dup(3).bits.addr(sfence_dup(3).bits.addr.getWidth-1, offLen)
-
-    when (sfence_dup(3).bits.rs1/*va*/) {
-      when (sfence_dup(3).bits.rs2) {
-        // all va && all asid
-        l3v := 0.U
-      } .otherwise {
-        // all va && specific asid except global
-        l3v := l3v & l3g
-      }
-    } .otherwise {
-      // val flushMask = UIntToOH(genTlbL2Idx(sfence.bits.addr(sfence.bits.addr.getWidth-1, offLen)))
-      val flushSetIdxOH = UIntToOH(genPtwL3SetIdx(sfence_vpn))
-      // val flushMask = VecInit(flushSetIdxOH.asBools.map(Fill(l2tlbParams.l3nWays, _.asUInt))).asUInt
-      val flushMask = VecInit(flushSetIdxOH.asBools.map { a => Fill(l2tlbParams.l3nWays, a.asUInt) }).asUInt
-      flushSetIdxOH.suggestName(s"sfence_nrs1_flushSetIdxOH")
-      flushMask.suggestName(s"sfence_nrs1_flushMask")
-
-      when (sfence_dup(3).bits.rs2) {
-        // specific leaf of addr && all asid
-        l3v := l3v & ~flushMask
-      } .otherwise {
-        // specific leaf of addr && specific asid
-        l3v := l3v & (~flushMask | l3g)
-      }
-    }
-  }
-
-  when (sfence_dup(0).valid) {
-    val l1asidhit = VecInit(l1asids.map(_ === sfence_dup(0).bits.asid)).asUInt
-    val spasidhit = VecInit(spasids.map(_ === sfence_dup(0).bits.asid)).asUInt
-    val sfence_vpn = sfence_dup(0).bits.addr(sfence_dup(0).bits.addr.getWidth-1, offLen)
-
-    when (sfence_dup(0).bits.rs1/*va*/) {
-      when (sfence_dup(0).bits.rs2) {
-        // all va && all asid
-        l1v := 0.U
-        l2v := 0.U
-        spv := 0.U
-      } .otherwise {
-        // all va && specific asid except global
-
-        l1v := l1v & (~l1asidhit | l1g)
-        l2v := l2v & l2g
-        spv := spv & (~spasidhit | spg)
-      }
-    } .otherwise {
-      // val flushMask = UIntToOH(genTlbL2Idx(sfence.bits.addr(sfence.bits.addr.getWidth-1, offLen)))
-      val flushSetIdxOH = UIntToOH(genPtwL3SetIdx(sfence_vpn))
-      // val flushMask = VecInit(flushSetIdxOH.asBools.map(Fill(l2tlbParams.l3nWays, _.asUInt))).asUInt
-      val flushMask = VecInit(flushSetIdxOH.asBools.map { a => Fill(l2tlbParams.l3nWays, a.asUInt) }).asUInt
-      flushSetIdxOH.suggestName(s"sfence_nrs1_flushSetIdxOH")
-      flushMask.suggestName(s"sfence_nrs1_flushMask")
-
-      when (sfence_dup(0).bits.rs2) {
-        // specific leaf of addr && all asid
-        spv := spv & (~VecInit(sp.map(_.hit(sfence_vpn, sfence_dup(0).bits.asid, ignoreAsid = true))).asUInt)
-      } .otherwise {
-        // specific leaf of addr && specific asid
-        spv := spv & (~VecInit(sp.map(_.hit(sfence_vpn, sfence_dup(0).bits.asid))).asUInt | spg)
-      }
-    }
-  }
-
-  def InsideStageConnect(in: DecoupledIO[PtwCacheReq], out: DecoupledIO[PtwCacheReq], inFire: Bool): Unit = {
-    in.ready := !in.valid || out.ready
-    out.valid := in.valid
-    out.bits := in.bits
-    out.bits.bypassed.zip(in.bits.bypassed).zipWithIndex.map{ case (b, i) =>
-      val bypassed_reg = Reg(Bool())
-      val bypassed_wire = refill_bypass(in.bits.req_info.vpn, i) && io.refill.valid
-      when (inFire) { bypassed_reg := bypassed_wire }
-      .elsewhen (io.refill.valid) { bypassed_reg := bypassed_reg || bypassed_wire }
-
-      b._1 := b._2 || (bypassed_wire || (bypassed_reg && !inFire))
-    }
-  }
-
-  // Perf Count
-  val resp_l3 = resp_res.l3.hit
-  val resp_sp = resp_res.sp.hit
-  val resp_l1_pre = resp_res.l1.pre
-  val resp_l2_pre = resp_res.l2.pre
-  val resp_l3_pre = resp_res.l3.pre
-  val resp_sp_pre = resp_res.sp.pre
-  val base_valid_access_0 = !from_pre(io.resp.bits.req_info.source) && io.resp.fire()
-  XSPerfAccumulate("access", base_valid_access_0)
-  XSPerfAccumulate("l1_hit", base_valid_access_0 && io.resp.bits.toFsm.l1Hit && !io.resp.bits.toFsm.l2Hit && !io.resp.bits.hit)
-  XSPerfAccumulate("l2_hit", base_valid_access_0 && io.resp.bits.toFsm.l2Hit && !io.resp.bits.hit)
-  XSPerfAccumulate("l3_hit", base_valid_access_0 && resp_l3)
-  XSPerfAccumulate("sp_hit", base_valid_access_0 && resp_sp)
-  XSPerfAccumulate("pte_hit",base_valid_access_0 && io.resp.bits.hit)
-
-  XSPerfAccumulate("l1_hit_pre", base_valid_access_0 && resp_l1_pre && io.resp.bits.toFsm.l1Hit && !io.resp.bits.toFsm.l2Hit && !io.resp.bits.hit)
-  XSPerfAccumulate("l2_hit_pre", base_valid_access_0 && resp_l2_pre && io.resp.bits.toFsm.l2Hit && !io.resp.bits.hit)
-  XSPerfAccumulate("l3_hit_pre", base_valid_access_0 && resp_l3_pre && resp_l3)
-  XSPerfAccumulate("sp_hit_pre", base_valid_access_0 && resp_sp_pre && resp_sp)
-  XSPerfAccumulate("pte_hit_pre",base_valid_access_0 && (resp_l3_pre && resp_l3 || resp_sp_pre && resp_sp) && io.resp.bits.hit)
-
-  val base_valid_access_1 = from_pre(io.resp.bits.req_info.source) && io.resp.fire()
-  XSPerfAccumulate("pre_access", base_valid_access_1)
-  XSPerfAccumulate("pre_l1_hit", base_valid_access_1 && io.resp.bits.toFsm.l1Hit && !io.resp.bits.toFsm.l2Hit && !io.resp.bits.hit)
-  XSPerfAccumulate("pre_l2_hit", base_valid_access_1 && io.resp.bits.toFsm.l2Hit && !io.resp.bits.hit)
-  XSPerfAccumulate("pre_l3_hit", base_valid_access_1 && resp_l3)
-  XSPerfAccumulate("pre_sp_hit", base_valid_access_1 && resp_sp)
-  XSPerfAccumulate("pre_pte_hit",base_valid_access_1 && io.resp.bits.hit)
-
-  XSPerfAccumulate("pre_l1_hit_pre", base_valid_access_1 && resp_l1_pre && io.resp.bits.toFsm.l1Hit && !io.resp.bits.toFsm.l2Hit && !io.resp.bits.hit)
-  XSPerfAccumulate("pre_l2_hit_pre", base_valid_access_1 && resp_l2_pre && io.resp.bits.toFsm.l2Hit && !io.resp.bits.hit)
-  XSPerfAccumulate("pre_l3_hit_pre", base_valid_access_1 && resp_l3_pre && resp_l3)
-  XSPerfAccumulate("pre_sp_hit_pre", base_valid_access_1 && resp_sp_pre && resp_sp)
-  XSPerfAccumulate("pre_pte_hit_pre",base_valid_access_1 && (resp_l3_pre && resp_l3 || resp_sp_pre && resp_sp) && io.resp.bits.hit)
-
-  val base_valid_access_2 = stageResp.bits.isFirst && !from_pre(io.resp.bits.req_info.source) && io.resp.fire()
-  XSPerfAccumulate("access_first", base_valid_access_2)
-  XSPerfAccumulate("l1_hit_first", base_valid_access_2 && io.resp.bits.toFsm.l1Hit && !io.resp.bits.toFsm.l2Hit && !io.resp.bits.hit)
-  XSPerfAccumulate("l2_hit_first", base_valid_access_2 && io.resp.bits.toFsm.l2Hit && !io.resp.bits.hit)
-  XSPerfAccumulate("l3_hit_first", base_valid_access_2 && resp_l3)
-  XSPerfAccumulate("sp_hit_first", base_valid_access_2 && resp_sp)
-  XSPerfAccumulate("pte_hit_first",base_valid_access_2 && io.resp.bits.hit)
-
-  XSPerfAccumulate("l1_hit_pre_first", base_valid_access_2 && resp_l1_pre && io.resp.bits.toFsm.l1Hit && !io.resp.bits.toFsm.l2Hit && !io.resp.bits.hit)
-  XSPerfAccumulate("l2_hit_pre_first", base_valid_access_2 && resp_l2_pre && io.resp.bits.toFsm.l2Hit && !io.resp.bits.hit)
-  XSPerfAccumulate("l3_hit_pre_first", base_valid_access_2 && resp_l3_pre && resp_l3)
-  XSPerfAccumulate("sp_hit_pre_first", base_valid_access_2 && resp_sp_pre && resp_sp)
-  XSPerfAccumulate("pte_hit_pre_first",base_valid_access_2 && (resp_l3_pre && resp_l3 || resp_sp_pre && resp_sp) && io.resp.bits.hit)
-
-  val base_valid_access_3 = stageResp.bits.isFirst && from_pre(io.resp.bits.req_info.source) && io.resp.fire()
-  XSPerfAccumulate("pre_access_first", base_valid_access_3)
-  XSPerfAccumulate("pre_l1_hit_first", base_valid_access_3 && io.resp.bits.toFsm.l1Hit && !io.resp.bits.toFsm.l2Hit && !io.resp.bits.hit)
-  XSPerfAccumulate("pre_l2_hit_first", base_valid_access_3 && io.resp.bits.toFsm.l2Hit && !io.resp.bits.hit)
-  XSPerfAccumulate("pre_l3_hit_first", base_valid_access_3 && resp_l3)
-  XSPerfAccumulate("pre_sp_hit_first", base_valid_access_3 && resp_sp)
-  XSPerfAccumulate("pre_pte_hit_first", base_valid_access_3 && io.resp.bits.hit)
-
-  XSPerfAccumulate("pre_l1_hit_pre_first", base_valid_access_3 && resp_l1_pre && io.resp.bits.toFsm.l1Hit && !io.resp.bits.toFsm.l2Hit && !io.resp.bits.hit)
-  XSPerfAccumulate("pre_l2_hit_pre_first", base_valid_access_3 && resp_l2_pre && io.resp.bits.toFsm.l2Hit && !io.resp.bits.hit)
-  XSPerfAccumulate("pre_l3_hit_pre_first", base_valid_access_3 && resp_l3_pre && resp_l3)
-  XSPerfAccumulate("pre_sp_hit_pre_first", base_valid_access_3 && resp_sp_pre && resp_sp)
-  XSPerfAccumulate("pre_pte_hit_pre_first",base_valid_access_3 && (resp_l3_pre && resp_l3 || resp_sp_pre && resp_sp) && io.resp.bits.hit)
-
-  XSPerfAccumulate("rwHarzad", io.req.valid && !io.req.ready)
-  XSPerfAccumulate("out_blocked", io.resp.valid && !io.resp.ready)
-  l1AccessPerf.zipWithIndex.map{ case (l, i) => XSPerfAccumulate(s"L1AccessIndex${i}", l) }
-  l2AccessPerf.zipWithIndex.map{ case (l, i) => XSPerfAccumulate(s"L2AccessIndex${i}", l) }
-  l3AccessPerf.zipWithIndex.map{ case (l, i) => XSPerfAccumulate(s"L3AccessIndex${i}", l) }
-  spAccessPerf.zipWithIndex.map{ case (l, i) => XSPerfAccumulate(s"SPAccessIndex${i}", l) }
-  l1RefillPerf.zipWithIndex.map{ case (l, i) => XSPerfAccumulate(s"L1RefillIndex${i}", l) }
-  l2RefillPerf.zipWithIndex.map{ case (l, i) => XSPerfAccumulate(s"L2RefillIndex${i}", l) }
-  l3RefillPerf.zipWithIndex.map{ case (l, i) => XSPerfAccumulate(s"L3RefillIndex${i}", l) }
-  spRefillPerf.zipWithIndex.map{ case (l, i) => XSPerfAccumulate(s"SPRefillIndex${i}", l) }
-
-  XSPerfAccumulate("l1Refill", Cat(l1RefillPerf).orR)
-  XSPerfAccumulate("l2Refill", Cat(l2RefillPerf).orR)
-  XSPerfAccumulate("l3Refill", Cat(l3RefillPerf).orR)
-  XSPerfAccumulate("spRefill", Cat(spRefillPerf).orR)
-  XSPerfAccumulate("l1Refill_pre", Cat(l1RefillPerf).orR && refill_prefetch_dup(0))
-  XSPerfAccumulate("l2Refill_pre", Cat(l2RefillPerf).orR && refill_prefetch_dup(0))
-  XSPerfAccumulate("l3Refill_pre", Cat(l3RefillPerf).orR && refill_prefetch_dup(0))
-  XSPerfAccumulate("spRefill_pre", Cat(spRefillPerf).orR && refill_prefetch_dup(0))
-
-  // debug
-  XSDebug(sfence_dup(0).valid, p"[sfence] original v and g vector:\n")
-  XSDebug(sfence_dup(0).valid, p"[sfence] l1v:${Binary(l1v)}\n")
-  XSDebug(sfence_dup(0).valid, p"[sfence] l2v:${Binary(l2v)}\n")
-  XSDebug(sfence_dup(0).valid, p"[sfence] l3v:${Binary(l3v)}\n")
-  XSDebug(sfence_dup(0).valid, p"[sfence] l3g:${Binary(l3g)}\n")
-  XSDebug(sfence_dup(0).valid, p"[sfence] spv:${Binary(spv)}\n")
-  XSDebug(RegNext(sfence_dup(0).valid), p"[sfence] new v and g vector:\n")
-  XSDebug(RegNext(sfence_dup(0).valid), p"[sfence] l1v:${Binary(l1v)}\n")
-  XSDebug(RegNext(sfence_dup(0).valid), p"[sfence] l2v:${Binary(l2v)}\n")
-  XSDebug(RegNext(sfence_dup(0).valid), p"[sfence] l3v:${Binary(l3v)}\n")
-  XSDebug(RegNext(sfence_dup(0).valid), p"[sfence] l3g:${Binary(l3g)}\n")
-  XSDebug(RegNext(sfence_dup(0).valid), p"[sfence] spv:${Binary(spv)}\n")
-
-  val perfEvents = Seq(
-    ("access           ", base_valid_access_0             ),
-    ("l1_hit           ", l1Hit                           ),
-    ("l2_hit           ", l2Hit                           ),
-    ("l3_hit           ", l3Hit                           ),
-    ("sp_hit           ", spHit                           ),
-    ("pte_hit          ", l3Hit || spHit                  ),
-    ("rwHarzad         ",  io.req.valid && !io.req.ready  ),
-    ("out_blocked      ",  io.resp.valid && !io.resp.ready),
-  )
-  generatePerfEvent()
-}
+///***************************************************************************************
+//* Copyright (c) 2020-2021 Institute of Computing Technology, Chinese Academy of Sciences
+//* Copyright (c) 2020-2021 Peng Cheng Laboratory
+//*
+//* XiangShan is licensed under Mulan PSL v2.
+//* You can use this software according to the terms and conditions of the Mulan PSL v2.
+//* You may obtain a copy of Mulan PSL v2 at:
+//*          http://license.coscl.org.cn/MulanPSL2
+//*
+//* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND,
+//* EITHER EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT,
+//* MERCHANTABILITY OR FIT FOR A PARTICULAR PURPOSE.
+//*
+//* See the Mulan PSL v2 for more details.
+//***************************************************************************************/
+//
+//package xiangshan.cache.mmu
+//
+//import chipsalliance.rocketchip.config.Parameters
+//import chisel3._
+//import chisel3.util._
+//import chisel3.internal.naming.chiselName
+//import xiangshan._
+//import xiangshan.cache.{HasDCacheParameters, MemoryOpConstants}
+//import utils._
+//import utility._
+//import freechips.rocketchip.diplomacy.{LazyModule, LazyModuleImp}
+//import freechips.rocketchip.tilelink._
+//
+///* ptw cache caches the page table of all the three layers
+// * ptw cache resp at next cycle
+// * the cache should not be blocked
+// * when miss queue if full, just block req outside
+// */
+//
+//class PageCachePerPespBundle(implicit p: Parameters) extends PtwBundle {
+//  val hit = Bool()
+//  val pre = Bool()
+//  val ppn = UInt(ppnLen.W)
+//  val perm = new PtePermBundle()
+//  val ecc = Bool()
+//  val level = UInt(2.W)
+//  val v = Bool()
+//
+//  def apply(hit: Bool, pre: Bool, ppn: UInt, perm: PtePermBundle = 0.U.asTypeOf(new PtePermBundle()),
+//            ecc: Bool = false.B, level: UInt = 0.U, valid: Bool = true.B) {
+//    this.hit := hit && !ecc
+//    this.pre := pre
+//    this.ppn := ppn
+//    this.perm := perm
+//    this.ecc := ecc && hit
+//    this.level := level
+//    this.v := valid
+//  }
+//}
+//
+//class PageCacheRespBundle(implicit p: Parameters) extends PtwBundle {
+//  val l1 = new PageCachePerPespBundle
+//  val l2 = new PageCachePerPespBundle
+//  val l3 = new PageCachePerPespBundle
+//  val sp = new PageCachePerPespBundle
+//}
+//
+//class PtwCacheReq(implicit p: Parameters) extends PtwBundle {
+//  val req_info = new L2TlbInnerBundle()
+//  val isFirst = Bool()
+//  val bypassed = Vec(3, Bool())
+//}
+//
+//class PtwCacheIO()(implicit p: Parameters) extends MMUIOBaseBundle with HasPtwConst {
+//  val req = Flipped(DecoupledIO(new PtwCacheReq()))
+//  val resp = DecoupledIO(new Bundle {
+//    val req_info = new L2TlbInnerBundle()
+//    val isFirst = Bool()
+//    val hit = Bool()
+//    val prefetch = Bool() // is the entry fetched by prefetch
+//    val bypassed = Bool()
+//    val toFsm = new Bundle {
+//      val l1Hit = Bool()
+//      val l2Hit = Bool()
+//      val ppn = UInt(ppnLen.W)
+//    }
+//    val toTlb = new PtwEntry(tagLen = vpnLen, hasPerm = true, hasLevel = true)
+//  })
+//  val refill = Flipped(ValidIO(new Bundle {
+//    val ptes = UInt(blockBits.W)
+//    val hyper = Bool()
+//    val levelOH = new Bundle {
+//      // NOTE: levelOH has (Level+1) bits, each stands for page cache entries
+//      val sp = Bool()
+//      val l3 = Bool()
+//      val l2 = Bool()
+//      val l1 = Bool()
+//      def apply(levelUInt: UInt, valid: Bool) = {
+//        sp := RegNext((levelUInt === 0.U || levelUInt === 1.U) && valid, false.B)
+//        l3 := RegNext((levelUInt === 2.U) & valid, false.B)
+//        l2 := RegNext((levelUInt === 1.U) & valid, false.B)
+//        l1 := RegNext((levelUInt === 0.U) & valid, false.B)
+//      }
+//    }
+//    // duplicate level and sel_pte for each page caches, for better fanout
+//    val req_info_dup = Vec(3, new L2TlbInnerBundle())
+//    val level_dup = Vec(3, UInt(log2Up(Level).W))
+//    val sel_pte_dup = Vec(3, UInt(XLEN.W))
+//  }))
+//  val sfence_dup = Vec(4, Input(new SfenceBundle()))
+//  val csr_dup = Vec(3, Input(new TlbCsrBundle()))
+//  val hptw = new Bundle {
+//    val req = DecoupledIO(new Bundle {
+//      val gpaddr = UInt(XLEN.W)
+//      val sfence = new SfenceBundle
+//      val hgatp = new TlbSatpBundle
+//    })
+//    val resp = Flipped(Valid(new Bundle {
+//      val resp = Output(UInt(XLEN.W))
+//      val level = Output(UInt(log2Up(Level).W))
+//      val af = Output(Bool())
+//      val pf = Output(Bool())
+//    }))
+//  }
+//}
+//
+//@chiselName
+//class PtwCache()(implicit p: Parameters) extends XSModule with HasPtwConst with HasPerfEvents {
+//  val io = IO(new PtwCacheIO)
+//
+//  val ecc = Code.fromString(l2tlbParams.ecc)
+//  val l2EntryType = new PTWEntriesWithEcc(ecc, num = PtwL2SectorSize, tagLen = PtwL2TagLen, level = 1, hasPerm = false)
+//  val l3EntryType = new PTWEntriesWithEcc(ecc, num = PtwL3SectorSize, tagLen = PtwL3TagLen, level = 2, hasPerm = true)
+//
+//  // TODO: four caches make the codes dirty, think about how to deal with it
+//
+//  val sfence_dup = io.sfence_dup
+//  val refill = io.refill.bits
+//  val refill_prefetch_dup = io.refill.bits.req_info_dup.map(a => from_pre(a.source))
+//  val flush_dup = sfence_dup.zip(io.csr_dup).map(f => f._1.valid || f._2.satp.changed)
+//  val flush = flush_dup(0)
+//
+//  // when refill, refuce to accept new req
+//  val rwHarzad = if (sramSinglePort) io.refill.valid else false.B
+//
+//  // handle hand signal and req_info
+//  // TODO: replace with FlushableQueue
+//  val stageReq = Wire(Decoupled(new PtwCacheReq()))         // enq stage & read page cache valid
+//  val stageDelay = Wire(Vec(2, Decoupled(new PtwCacheReq()))) // page cache resp
+//  val stageCheck = Wire(Vec(2, Decoupled(new PtwCacheReq()))) // check hit & check ecc
+//  val stageResp = Wire(Decoupled(new PtwCacheReq()))         // deq stage
+//
+//  val stageDelay_valid_1cycle = OneCycleValid(stageReq.fire, flush)      // catch ram data
+//  val stageCheck_valid_1cycle = OneCycleValid(stageDelay(1).fire, flush) // replace & perf counter
+//  val stageResp_valid_1cycle_dup = Wire(Vec(2, Bool()))
+//  stageResp_valid_1cycle_dup.map(_ := OneCycleValid(stageCheck(1).fire, flush))  // ecc flush
+//
+//  stageReq <> io.req
+//  PipelineConnect(stageReq, stageDelay(0), stageDelay(1).ready, flush, rwHarzad)
+//  InsideStageConnect(stageDelay(0), stageDelay(1), stageDelay_valid_1cycle)
+//  PipelineConnect(stageDelay(1), stageCheck(0), stageCheck(1).ready, flush)
+//  InsideStageConnect(stageCheck(0), stageCheck(1), stageCheck_valid_1cycle)
+//  PipelineConnect(stageCheck(1), stageResp, io.resp.ready, flush)
+//  stageResp.ready := !stageResp.valid || io.resp.ready
+//
+//  // l1: level 0 non-leaf pte
+//  val l1 = Reg(Vec(l2tlbParams.l1Size, new PtwEntry(tagLen = PtwL1TagLen)))
+//  val l1v = RegInit(0.U(l2tlbParams.l1Size.W))
+//  val l1g = Reg(UInt(l2tlbParams.l1Size.W))
+//  val l1asids = l1.map(_.asid)
+//
+//  // l2: level 1 non-leaf pte
+//  val l2 = Module(new SRAMTemplate(
+//    l2EntryType,
+//    set = l2tlbParams.l2nSets,
+//    way = l2tlbParams.l2nWays,
+//    singlePort = sramSinglePort
+//  ))
+//  val l2v = RegInit(0.U((l2tlbParams.l2nSets * l2tlbParams.l2nWays).W))
+//  val l2g = Reg(UInt((l2tlbParams.l2nSets * l2tlbParams.l2nWays).W))
+//  val l2asids = Reg(Vec(l2tlbParams.l2nSets, Vec(l2tlbParams.l2nWays, UInt(AsidLength.W))))
+//  def getl2vSet(vpn: UInt) = {
+//    require(log2Up(l2tlbParams.l2nWays) == log2Down(l2tlbParams.l2nWays))
+//    val set = genPtwL2SetIdx(vpn)
+//    require(set.getWidth == log2Up(l2tlbParams.l2nSets))
+//    val l2vVec = l2v.asTypeOf(Vec(l2tlbParams.l2nSets, UInt(l2tlbParams.l2nWays.W)))
+//    l2vVec(set)
+//  }
+//  def getl2asidSet(vpn: UInt) = {
+//    require(log2Up(l2tlbParams.l2nWays) == log2Down(l2tlbParams.l2nWays))
+//    val set = genPtwL2SetIdx(vpn)
+//    require(set.getWidth == log2Up(l2tlbParams.l2nSets))
+//    l2asids(set)
+//  }
+//
+//  // l3: level 2 leaf pte of 4KB pages
+//  val l3 = Module(new SRAMTemplate(
+//    l3EntryType,
+//    set = l2tlbParams.l3nSets,
+//    way = l2tlbParams.l3nWays,
+//    singlePort = sramSinglePort
+//  ))
+//  val l3v = RegInit(0.U((l2tlbParams.l3nSets * l2tlbParams.l3nWays).W))
+//  val l3g = Reg(UInt((l2tlbParams.l3nSets * l2tlbParams.l3nWays).W))
+//  val l3asids = Reg(Vec(l2tlbParams.l3nSets, Vec(l2tlbParams.l3nWays, UInt(AsidLength.W))))
+//  def getl3vSet(vpn: UInt) = {
+//    require(log2Up(l2tlbParams.l3nWays) == log2Down(l2tlbParams.l3nWays))
+//    val set = genPtwL3SetIdx(vpn)
+//    require(set.getWidth == log2Up(l2tlbParams.l3nSets))
+//    val l3vVec = l3v.asTypeOf(Vec(l2tlbParams.l3nSets, UInt(l2tlbParams.l3nWays.W)))
+//    l3vVec(set)
+//  }
+//  def getl3asidSet(vpn: UInt) = {
+//    require(log2Up(l2tlbParams.l3nWays) == log2Down(l2tlbParams.l3nWays))
+//    val set = genPtwL3SetIdx(vpn)
+//    require(set.getWidth == log2Up(l2tlbParams.l3nSets))
+//    l3asids(set)
+//  }
+//
+//  // sp: level 0/1 leaf pte of 1GB/2MB super pages
+//  val sp = Reg(Vec(l2tlbParams.spSize, new PtwEntry(tagLen = SPTagLen, hasPerm = true, hasLevel = true)))
+//  val spv = RegInit(0.U(l2tlbParams.spSize.W))
+//  val spg = Reg(UInt(l2tlbParams.spSize.W))
+//  val spasids = sp.map(_.asid)
+//
+//  // Access Perf
+//  val l1AccessPerf = Wire(Vec(l2tlbParams.l1Size, Bool()))
+//  val l2AccessPerf = Wire(Vec(l2tlbParams.l2nWays, Bool()))
+//  val l3AccessPerf = Wire(Vec(l2tlbParams.l3nWays, Bool()))
+//  val spAccessPerf = Wire(Vec(l2tlbParams.spSize, Bool()))
+//  l1AccessPerf.map(_ := false.B)
+//  l2AccessPerf.map(_ := false.B)
+//  l3AccessPerf.map(_ := false.B)
+//  spAccessPerf.map(_ := false.B)
+//
+//
+//
+//  def vpn_match(vpn1: UInt, vpn2: UInt, level: Int) = {
+//    vpn1(vpnnLen*3-1, vpnnLen*(2-level)+3) === vpn2(vpnnLen*3-1, vpnnLen*(2-level)+3)
+//  }
+//  // NOTE: not actually bypassed, just check if hit, re-access the page cache
+//  def refill_bypass(vpn: UInt, level: Int) = {
+//    io.refill.valid && (level.U === io.refill.bits.level_dup(0)) && vpn_match(io.refill.bits.req_info_dup(0).vpn, vpn, level),
+//  }
+//
+//  // if the req is hypervisor inst or in virtMode, we don't used ptw cache
+//  val s2xlate = stageReq.bits.req_info.virt || stageReq.bits.req_info.hyperinst
+//
+//  // l1
+//  val ptwl1replace = ReplacementPolicy.fromString(l2tlbParams.l1Replacer, l2tlbParams.l1Size)
+//  val (l1Hit, l1HitPPN, l1Pre) = {
+//    val hitVecT = l1.zipWithIndex.map { case (e, i) => e.hit(stageReq.bits.req_info.vpn, Mux(s2xlate, io.csr_dup(0).vsatp.asid, io.csr_dup(0).satp.asid), s2xlate) && l1v(i) }
+//    val hitVec = hitVecT.map(RegEnable(_, stageReq.fire))
+//
+//    // stageDelay, but check for l1
+//    val hitPPN = DataHoldBypass(ParallelPriorityMux(hitVec zip l1.map(_.ppn)), stageDelay_valid_1cycle)
+//    val hitPre = DataHoldBypass(ParallelPriorityMux(hitVec zip l1.map(_.prefetch)), stageDelay_valid_1cycle)
+//    val hit = DataHoldBypass(ParallelOR(hitVec), stageDelay_valid_1cycle)
+//
+//    when (hit && stageDelay_valid_1cycle) { ptwl1replace.access(OHToUInt(hitVec)) }
+//
+//    l1AccessPerf.zip(hitVec).map{ case (l, h) => l := h && stageDelay_valid_1cycle}
+//    for (i <- 0 until l2tlbParams.l1Size) {
+//      XSDebug(stageReq.fire, p"[l1] l1(${i.U}) ${l1(i)} hit:${l1(i).hit(stageReq.bits.req_info.vpn, io.csr_dup(0).satp.asid)}\n")
+//    }
+//    XSDebug(stageReq.fire, p"[l1] l1v:${Binary(l1v)} hitVecT:${Binary(VecInit(hitVecT).asUInt)}\n")
+//    XSDebug(stageDelay(0).valid, p"[l1] l1Hit:${hit} l1HitPPN:0x${Hexadecimal(hitPPN)} hitVec:${VecInit(hitVec).asUInt}\n")
+//
+//    VecInit(hitVecT).suggestName(s"l1_hitVecT")
+//    VecInit(hitVec).suggestName(s"l1_hitVec")
+//
+//    // synchronize with other entries with RegEnable
+//    (RegEnable(hit, stageDelay(1).fire),
+//     RegEnable(hitPPN, stageDelay(1).fire),
+//     RegEnable(hitPre, stageDelay(1).fire))
+//  }
+//
+//  // l2
+//  val ptwl2replace = ReplacementPolicy.fromString(l2tlbParams.l2Replacer,l2tlbParams.l2nWays,l2tlbParams.l2nSets)
+//  val (l2Hit, l2HitPPN, l2Pre, l2eccError) = {
+//    val ridx = genPtwL2SetIdx(stageReq.bits.req_info.vpn)
+//    l2.io.r.req.valid := stageReq.fire
+//    l2.io.r.req.bits.apply(setIdx = ridx)
+//    val vVec_req = getl2vSet(stageReq.bits.req_info.vpn)
+//
+//    // delay one cycle after sram read
+//    val delay_vpn = stageDelay(0).bits.req_info.vpn
+//    val data_resp = DataHoldBypass(l2.io.r.resp.data, stageDelay_valid_1cycle)
+//    val vVec_delay = RegEnable(vVec_req, stageReq.fire)
+//    val hitVec_delay = VecInit(data_resp.zip(vVec_delay.asBools).map { case (wayData, v) =>
+//      wayData.entries.hit(delay_vpn, Mux(s2xlate, io.csr_dup(1).vsatp.asid, io.csr_dup(1).satp.asid), s2xlate) && v })
+//
+//    // check hit and ecc
+//    val check_vpn = stageCheck(0).bits.req_info.vpn
+//    val ramDatas = RegEnable(data_resp, stageDelay(1).fire)
+//    val vVec = RegEnable(vVec_delay, stageDelay(1).fire).asBools()
+//
+//    val hitVec = RegEnable(hitVec_delay, stageDelay(1).fire)
+//    val hitWayEntry = ParallelPriorityMux(hitVec zip ramDatas)
+//    val hitWayData = hitWayEntry.entries
+//    val hit = ParallelOR(hitVec)
+//    val hitWay = ParallelPriorityMux(hitVec zip (0 until l2tlbParams.l2nWays).map(_.U(log2Up(l2tlbParams.l2nWays).W)))
+//    val eccError = hitWayEntry.decode()
+//
+//    ridx.suggestName(s"l2_ridx")
+//    ramDatas.suggestName(s"l2_ramDatas")
+//    hitVec.suggestName(s"l2_hitVec")
+//    hitWayData.suggestName(s"l2_hitWayData")
+//    hitWay.suggestName(s"l2_hitWay")
+//
+//    when (hit && stageCheck_valid_1cycle) { ptwl2replace.access(genPtwL2SetIdx(check_vpn), hitWay) }
+//
+//    l2AccessPerf.zip(hitVec).map{ case (l, h) => l := h && stageCheck_valid_1cycle }
+//    XSDebug(stageDelay_valid_1cycle, p"[l2] ridx:0x${Hexadecimal(ridx)}\n")
+//    for (i <- 0 until l2tlbParams.l2nWays) {
+//      XSDebug(stageCheck_valid_1cycle, p"[l2] ramDatas(${i.U}) ${ramDatas(i)}  l2v:${vVec(i)}  hit:${hit}\n")
+//    }
+//    XSDebug(stageCheck_valid_1cycle, p"[l2] l2Hit:${hit} l2HitPPN:0x${Hexadecimal(hitWayData.ppns(genPtwL2SectorIdx(check_vpn)))} hitVec:${Binary(hitVec.asUInt)} hitWay:${hitWay} vidx:${vVec}\n")
+//
+//    (hit, hitWayData.ppns(genPtwL2SectorIdx(check_vpn)), hitWayData.prefetch, eccError)
+//  }
+//
+//  // l3
+//  val ptwl3replace = ReplacementPolicy.fromString(l2tlbParams.l3Replacer,l2tlbParams.l3nWays,l2tlbParams.l3nSets)
+//  val (l3Hit, l3HitData, l3Pre, l3eccError) = {
+//    val ridx = genPtwL3SetIdx(stageReq.bits.req_info.vpn)
+//    l3.io.r.req.valid := stageReq.fire
+//    l3.io.r.req.bits.apply(setIdx = ridx)
+//    val vVec_req = getl3vSet(stageReq.bits.req_info.vpn)
+//
+//    // delay one cycle after sram read
+//    val delay_vpn = stageDelay(0).bits.req_info.vpn
+//    val data_resp = DataHoldBypass(l3.io.r.resp.data, stageDelay_valid_1cycle)
+//    val vVec_delay = RegEnable(vVec_req, stageReq.fire)
+//    val hitVec_delay = VecInit(data_resp.zip(vVec_delay.asBools).map { case (wayData, v) =>
+//      wayData.entries.hit(delay_vpn, Mux(s2xlate, io.csr_dup(2).vsatp.asid, io.csr_dup(2).satp.asid), s2xlate) && v})
+//
+//    // check hit and ecc
+//    val check_vpn = stageCheck(0).bits.req_info.vpn
+//    val ramDatas = RegEnable(data_resp, stageDelay(1).fire)
+//    val vVec = RegEnable(vVec_delay, stageDelay(1).fire).asBools()
+//
+//    val hitVec = RegEnable(hitVec_delay, stageDelay(1).fire)
+//    val hitWayEntry = ParallelPriorityMux(hitVec zip ramDatas)
+//    val hitWayData = hitWayEntry.entries
+//    val hitWayEcc = hitWayEntry.ecc
+//    val hit = ParallelOR(hitVec)
+//    val hitWay = ParallelPriorityMux(hitVec zip (0 until l2tlbParams.l3nWays).map(_.U(log2Up(l2tlbParams.l3nWays).W)))
+//    val eccError = hitWayEntry.decode()
+//
+//    when (hit && stageCheck_valid_1cycle) { ptwl3replace.access(genPtwL3SetIdx(check_vpn), hitWay) }
+//
+//    l3AccessPerf.zip(hitVec).map{ case (l, h) => l := h && stageCheck_valid_1cycle }
+//    XSDebug(stageReq.fire, p"[l3] ridx:0x${Hexadecimal(ridx)}\n")
+//    for (i <- 0 until l2tlbParams.l3nWays) {
+//      XSDebug(stageCheck_valid_1cycle, p"[l3] ramDatas(${i.U}) ${ramDatas(i)}  l3v:${vVec(i)}  hit:${hitVec(i)}\n")
+//    }
+//    XSDebug(stageCheck_valid_1cycle, p"[l3] l3Hit:${hit} l3HitData:${hitWayData} hitVec:${Binary(hitVec.asUInt)} hitWay:${hitWay} v:${vVec}\n")
+//
+//    ridx.suggestName(s"l3_ridx")
+//    ramDatas.suggestName(s"l3_ramDatas")
+//    hitVec.suggestName(s"l3_hitVec")
+//    hitWay.suggestName(s"l3_hitWay")
+//
+//    (hit, hitWayData, hitWayData.prefetch, eccError)
+//  }
+//  val l3HitPPN = l3HitData.ppns(genPtwL3SectorIdx(stageCheck(0).bits.req_info.vpn))
+//  val l3HitPerm = l3HitData.perms.getOrElse(0.U.asTypeOf(Vec(PtwL3SectorSize, new PtePermBundle)))(genPtwL3SectorIdx(stageCheck(0).bits.req_info.vpn))
+//  val l3HitValid = l3HitData.vs(genPtwL3SectorIdx(stageCheck(0).bits.req_info.vpn))
+//
+//  // super page
+//  val spreplace = ReplacementPolicy.fromString(l2tlbParams.spReplacer, l2tlbParams.spSize)
+//  val (spHit, spHitData, spPre, spValid) = {
+//    val hitVecT = sp.zipWithIndex.map { case (e, i) => e.hit(stageReq.bits.req_info.vpn, Mux(s2xlate, io.csr_dup(0).vsatp.asid, io.csr_dup(0).satp.asid), s2xlate) && spv(i) }
+//    val hitVec = hitVecT.map(x => RegEnable(x, stageReq.fire))
+//    val hitData = ParallelPriorityMux(hitVec zip sp)
+//    val hit = ParallelOR(hitVec)
+//
+//    when (hit && stageDelay_valid_1cycle) { spreplace.access(OHToUInt(hitVec)) }
+//
+//    spAccessPerf.zip(hitVec).map{ case (s, h) => s := h && stageDelay_valid_1cycle }
+//    for (i <- 0 until l2tlbParams.spSize) {
+//      XSDebug(stageReq.fire, p"[sp] sp(${i.U}) ${sp(i)} hit:${sp(i).hit(stageReq.bits.req_info.vpn, io.csr_dup(0).satp.asid)} spv:${spv(i)}\n")
+//    }
+//    XSDebug(stageDelay_valid_1cycle, p"[sp] spHit:${hit} spHitData:${hitData} hitVec:${Binary(VecInit(hitVec).asUInt)}\n")
+//
+//    VecInit(hitVecT).suggestName(s"sp_hitVecT")
+//    VecInit(hitVec).suggestName(s"sp_hitVec")
+//
+//    (RegEnable(hit, stageDelay(1).fire),
+//     RegEnable(hitData, stageDelay(1).fire),
+//     RegEnable(hitData.prefetch, stageDelay(1).fire),
+//     RegEnable(hitData.v, stageDelay(1).fire()))
+//  }
+//  val spHitPerm = spHitData.perm.getOrElse(0.U.asTypeOf(new PtePermBundle))
+//  val spHitLevel = spHitData.level.getOrElse(0.U)
+//
+//  val check_res = Wire(new PageCacheRespBundle)
+//  check_res.l1.apply(l1Hit, l1Pre, l1HitPPN)
+//  check_res.l2.apply(l2Hit, l2Pre, l2HitPPN, ecc = l2eccError)
+//  check_res.l3.apply(l3Hit, l3Pre, l3HitPPN, l3HitPerm, l3eccError, valid = l3HitValid)
+//  check_res.sp.apply(spHit, spPre, spHitData.ppn, spHitPerm, false.B, spHitLevel, spValid)
+//
+//  val resp_res = Reg(new PageCacheRespBundle)
+//  when (stageCheck(1).fire) { resp_res := check_res }
+//
+//  // stageResp bypass
+//  val bypassed = Wire(Vec(3, Bool()))
+//  bypassed.indices.foreach(i =>
+//    bypassed(i) := stageResp.bits.bypassed(i) ||
+//      ValidHoldBypass(refill_bypass(stageResp.bits.req_info.vpn, i),
+//        OneCycleValid(stageCheck(1).fire, false.B) || io.refill.valid)
+//  )
+//
+//  io.resp.bits.req_info   := stageResp.bits.req_info
+//  io.resp.bits.isFirst  := stageResp.bits.isFirst
+//  io.resp.bits.hit      := resp_res.l3.hit || resp_res.sp.hit
+//  io.resp.bits.bypassed := bypassed(2) || (bypassed(1) && !resp_res.l2.hit) || (bypassed(0) && !resp_res.l1.hit)
+//  io.resp.bits.prefetch := resp_res.l3.pre && resp_res.l3.hit || resp_res.sp.pre && resp_res.sp.hit
+//  io.resp.bits.toFsm.l1Hit := resp_res.l1.hit
+//  io.resp.bits.toFsm.l2Hit := resp_res.l2.hit
+//  io.resp.bits.toFsm.ppn   := Mux(resp_res.l2.hit, resp_res.l2.ppn, resp_res.l1.ppn)
+//  io.resp.bits.toTlb.tag   := stageResp.bits.req_info.vpn
+//  io.resp.bits.toTlb.asid  := io.csr_dup(0).satp.asid // DontCare
+//  io.resp.bits.toTlb.ppn   := Mux(resp_res.l3.hit, resp_res.l3.ppn, resp_res.sp.ppn)
+//  io.resp.bits.toTlb.perm.map(_ := Mux(resp_res.l3.hit, resp_res.l3.perm, resp_res.sp.perm))
+//  io.resp.bits.toTlb.level.map(_ := Mux(resp_res.l3.hit, 2.U, resp_res.sp.level))
+//  io.resp.bits.toTlb.prefetch := from_pre(stageResp.bits.req_info.source)
+//  io.resp.bits.toTlb.v := Mux(resp_res.sp.hit, resp_res.sp.v, resp_res.l3.v)
+//  io.resp.bits.toTlb.gvpn := DontCare // now page cache don't store guest page
+//  io.resp.bits.toTlb.vmid := DontCare
+//  io.resp.bits.toTlb.s2xlate := DontCare
+//  io.resp.valid := stageResp.valid
+//  XSError(stageResp.valid && resp_res.l3.hit && resp_res.sp.hit, "normal page and super page both hit")
+//  XSError(stageResp.valid && io.resp.bits.hit && bypassed(2), "page cache, bypassed but hit")
+//
+//  // refill Perf
+//  val l1RefillPerf = Wire(Vec(l2tlbParams.l1Size, Bool()))
+//  val l2RefillPerf = Wire(Vec(l2tlbParams.l2nWays, Bool()))
+//  val l3RefillPerf = Wire(Vec(l2tlbParams.l3nWays, Bool()))
+//  val spRefillPerf = Wire(Vec(l2tlbParams.spSize, Bool()))
+//  l1RefillPerf.map(_ := false.B)
+//  l2RefillPerf.map(_ := false.B)
+//  l3RefillPerf.map(_ := false.B)
+//  spRefillPerf.map(_ := false.B)
+//
+//  // refill
+//  l2.io.w.req <> DontCare
+//  l3.io.w.req <> DontCare
+//  l2.io.w.req.valid := false.B
+//  l3.io.w.req.valid := false.B
+//
+//  val memRdata = refill.ptes
+//  val memPtes = (0 until (l2tlbParams.blockBytes/(XLEN/8))).map(i => memRdata((i+1)*XLEN-1, i*XLEN).asTypeOf(new PteBundle))
+//  val memSelData = io.refill.bits.sel_pte_dup
+//  val memPte = memSelData.map(a => a.asTypeOf(new PteBundle))
+//
+//  // TODO: handle sfenceLatch outsize
+//  when (!flush_dup(0) && refill.levelOH.l1 && !memPte(0).isLeaf() && !memPte(0).isPf(refill.level_dup(0)) && !memPte(0).isAf()) {
+//    // val refillIdx = LFSR64()(log2Up(l2tlbParams.l1Size)-1,0) // TODO: may be LRU
+//    val refillIdx = replaceWrapper(l1v, ptwl1replace.way)
+//    refillIdx.suggestName(s"PtwL1RefillIdx")
+//    val rfOH = UIntToOH(refillIdx)
+//    l1(refillIdx).refill(
+//      refill.req_info_dup(0).vpn,
+//      io.csr_dup(0).satp.asid,
+//      memSelData(0),
+//      0.U,
+//      refill_prefetch_dup(0),
+//      s2xlate = refill.req_info_dup(0).hyperinst || refill.req_info_dup(0).virt,
+//    )
+//    ptwl1replace.access(refillIdx)
+//    l1v := l1v | rfOH
+//    l1g := (l1g & ~rfOH) | Mux(memPte(0).perm.g, rfOH, 0.U)
+//
+//    for (i <- 0 until l2tlbParams.l1Size) {
+//      l1RefillPerf(i) := i.U === refillIdx
+//    }
+//
+//    XSDebug(p"[l1 refill] refillIdx:${refillIdx} refillEntry:${l1(refillIdx).genPtwEntry(refill.req_info_dup(0).vpn, io.csr_dup(0).satp.asid, memSelData(0), 0.U, prefetch = refill_prefetch_dup(0))}\n")
+//    XSDebug(p"[l1 refill] l1v:${Binary(l1v)}->${Binary(l1v | rfOH)} l1g:${Binary(l1g)}->${Binary((l1g & ~rfOH) | Mux(memPte(0).perm.g, rfOH, 0.U))}\n")
+//
+//    refillIdx.suggestName(s"l1_refillIdx")
+//    rfOH.suggestName(s"l1_rfOH")
+//  }
+//
+//  when (!flush_dup(1) && refill.levelOH.l2 && !memPte(1).isLeaf() && !memPte(1).isPf(refill.level_dup(1)) && !memPte(1).isAf()) {
+//    val refillIdx = genPtwL2SetIdx(refill.req_info_dup(1).vpn)
+//    val victimWay = replaceWrapper(getl2vSet(refill.req_info_dup(1).vpn), ptwl2replace.way(refillIdx))
+//    val victimWayOH = UIntToOH(victimWay)
+//    val rfvOH = UIntToOH(Cat(refillIdx, victimWay))
+//    val wdata = Wire(l2EntryType)
+//    wdata.gen(
+//      vpn = refill.req_info_dup(1).vpn,
+//      asid = io.csr_dup(1).satp.asid,
+//      data = memRdata,
+//      levelUInt = 1.U,
+//      refill_prefetch_dup(1),
+//      s2xlate = refill.req_info_dup(1).hyperinst || refill.req_info_dup(1).virt
+//    )
+//    l2.io.w.apply(
+//      valid = true.B,
+//      setIdx = refillIdx,
+//      data = wdata,
+//      waymask = victimWayOH
+//    )
+//    ptwl2replace.access(refillIdx, victimWay)
+//    l2v := l2v | rfvOH
+//    l2g := l2g & ~rfvOH | Mux(Cat(memPtes.map(_.perm.g)).andR, rfvOH, 0.U)
+//
+//    for (i <- 0 until l2tlbParams.l2nWays) {
+//      l2RefillPerf(i) := i.U === victimWay
+//    }
+//
+//    XSDebug(p"[l2 refill] refillIdx:0x${Hexadecimal(refillIdx)} victimWay:${victimWay} victimWayOH:${Binary(victimWayOH)} rfvOH(in UInt):${Cat(refillIdx, victimWay)}\n")
+//    XSDebug(p"[l2 refill] refilldata:0x${wdata}\n")
+//    XSDebug(p"[l2 refill] l2v:${Binary(l2v)} -> ${Binary(l2v | rfvOH)}\n")
+//    XSDebug(p"[l2 refill] l2g:${Binary(l2g)} -> ${Binary(l2g & ~rfvOH | Mux(Cat(memPtes.map(_.perm.g)).andR, rfvOH, 0.U))}\n")
+//
+//    refillIdx.suggestName(s"l2_refillIdx")
+//    victimWay.suggestName(s"l2_victimWay")
+//    victimWayOH.suggestName(s"l2_victimWayOH")
+//    rfvOH.suggestName(s"l2_rfvOH")
+//  }
+//
+//  when (!flush_dup(2) && refill.levelOH.l3 && !memPte(2).isAf()) {
+//    val refillIdx = genPtwL3SetIdx(refill.req_info_dup(2).vpn)
+//    val victimWay = replaceWrapper(getl3vSet(refill.req_info_dup(2).vpn), ptwl3replace.way(refillIdx))
+//    val victimWayOH = UIntToOH(victimWay)
+//    val rfvOH = UIntToOH(Cat(refillIdx, victimWay))
+//    val wdata = Wire(l3EntryType)
+//    wdata.gen(
+//      vpn = refill.req_info_dup(2).vpn,
+//      asid = io.csr_dup(2).satp.asid,
+//      data = memRdata,
+//      levelUInt = 2.U,
+//      refill_prefetch_dup(2),
+//      s2xlate = refill.req_info_dup(2).hyperinst || refill.req_info_dup(2).virt
+//    )
+//    l3.io.w.apply(
+//      valid = true.B,
+//      setIdx = refillIdx,
+//      data = wdata,
+//      waymask = victimWayOH
+//    )
+//    ptwl3replace.access(refillIdx, victimWay)
+//    l3v := l3v | rfvOH
+//    l3g := l3g & ~rfvOH | Mux(Cat(memPtes.map(_.perm.g)).andR, rfvOH, 0.U)
+//
+//    for (i <- 0 until l2tlbParams.l3nWays) {
+//      l3RefillPerf(i) := i.U === victimWay
+//    }
+//
+//    XSDebug(p"[l3 refill] refillIdx:0x${Hexadecimal(refillIdx)} victimWay:${victimWay} victimWayOH:${Binary(victimWayOH)} rfvOH(in UInt):${Cat(refillIdx, victimWay)}\n")
+//    XSDebug(p"[l3 refill] refilldata:0x${wdata}\n")
+//    XSDebug(p"[l3 refill] l3v:${Binary(l3v)} -> ${Binary(l3v | rfvOH)}\n")
+//    XSDebug(p"[l3 refill] l3g:${Binary(l3g)} -> ${Binary(l3g & ~rfvOH | Mux(Cat(memPtes.map(_.perm.g)).andR, rfvOH, 0.U))}\n")
+//
+//    refillIdx.suggestName(s"l3_refillIdx")
+//    victimWay.suggestName(s"l3_victimWay")
+//    victimWayOH.suggestName(s"l3_victimWayOH")
+//    rfvOH.suggestName(s"l3_rfvOH")
+//  }
+//
+//
+//  // misc entries: super & invalid
+//  when (!flush_dup(0) && refill.levelOH.sp && (memPte(0).isLeaf() || memPte(0).isPf(refill.level_dup(0))) && !memPte(0).isAf()) {
+//    val refillIdx = spreplace.way// LFSR64()(log2Up(l2tlbParams.spSize)-1,0) // TODO: may be LRU
+//    val rfOH = UIntToOH(refillIdx)
+//    sp(refillIdx).refill(
+//      refill.req_info_dup(0).vpn,
+//      io.csr_dup(0).satp.asid,
+//      memSelData(0),
+//      refill.level_dup(2),
+//      refill_prefetch_dup(0),
+//      !memPte(0).isPf(refill.level_dup(0)),
+//      s2xlate = refill.req_info_dup(0).virt || refill.req_info_dup(0).hyperinst
+//    )
+//    spreplace.access(refillIdx)
+//    spv := spv | rfOH
+//    spg := spg & ~rfOH | Mux(memPte(0).perm.g, rfOH, 0.U)
+//
+//    for (i <- 0 until l2tlbParams.spSize) {
+//      spRefillPerf(i) := i.U === refillIdx
+//    }
+//
+//    XSDebug(p"[sp refill] refillIdx:${refillIdx} refillEntry:${sp(refillIdx).genPtwEntry(refill.req_info_dup(0).vpn, io.csr_dup(0).satp.asid, memSelData(0), refill.level_dup(0), refill_prefetch_dup(0))}\n")
+//    XSDebug(p"[sp refill] spv:${Binary(spv)}->${Binary(spv | rfOH)} spg:${Binary(spg)}->${Binary(spg & ~rfOH | Mux(memPte(0).perm.g, rfOH, 0.U))}\n")
+//
+//    refillIdx.suggestName(s"sp_refillIdx")
+//    rfOH.suggestName(s"sp_rfOH")
+//  }
+//
+//  val l2eccFlush = resp_res.l2.ecc && stageResp_valid_1cycle_dup(0) // RegNext(l2eccError, init = false.B)
+//  val l3eccFlush = resp_res.l3.ecc && stageResp_valid_1cycle_dup(1) // RegNext(l3eccError, init = false.B)
+//  val eccVpn = stageResp.bits.req_info.vpn
+//
+//  XSError(l2eccFlush, "l2tlb.cache.l2 ecc error. Should not happen at sim stage")
+//  XSError(l3eccFlush, "l2tlb.cache.l3 ecc error. Should not happen at sim stage")
+//  when (l2eccFlush) {
+//    val flushSetIdxOH = UIntToOH(genPtwL2SetIdx(eccVpn))
+//    val flushMask = VecInit(flushSetIdxOH.asBools.map { a => Fill(l2tlbParams.l2nWays, a.asUInt) }).asUInt
+//    l2v := l2v & ~flushMask
+//    l2g := l2g & ~flushMask
+//  }
+//
+//  when (l3eccFlush) {
+//    val flushSetIdxOH = UIntToOH(genPtwL3SetIdx(eccVpn))
+//    val flushMask = VecInit(flushSetIdxOH.asBools.map { a => Fill(l2tlbParams.l3nWays, a.asUInt) }).asUInt
+//    l3v := l3v & ~flushMask
+//    l3g := l3g & ~flushMask
+//  }
+//
+//  // sfence
+//  when (sfence_dup(3).valid) {
+//    val sfence_vpn = sfence_dup(3).bits.addr(sfence_dup(3).bits.addr.getWidth-1, offLen)
+//
+//    when (sfence_dup(3).bits.rs1/*va*/) {
+//      when (sfence_dup(3).bits.rs2) {
+//        // all va && all asid
+//        l3v := 0.U
+//      } .otherwise {
+//        // all va && specific asid except global
+//        l3v := l3v & l3g
+//      }
+//    } .otherwise {
+//      // val flushMask = UIntToOH(genTlbL2Idx(sfence.bits.addr(sfence.bits.addr.getWidth-1, offLen)))
+//      val flushSetIdxOH = UIntToOH(genPtwL3SetIdx(sfence_vpn))
+//      // val flushMask = VecInit(flushSetIdxOH.asBools.map(Fill(l2tlbParams.l3nWays, _.asUInt))).asUInt
+//      val flushMask = VecInit(flushSetIdxOH.asBools.map { a => Fill(l2tlbParams.l3nWays, a.asUInt) }).asUInt
+//      flushSetIdxOH.suggestName(s"sfence_nrs1_flushSetIdxOH")
+//      flushMask.suggestName(s"sfence_nrs1_flushMask")
+//
+//      when (sfence_dup(3).bits.rs2) {
+//        // specific leaf of addr && all asid
+//        l3v := l3v & ~flushMask
+//      } .otherwise {
+//        // specific leaf of addr && specific asid
+//        l3v := l3v & (~flushMask | l3g)
+//      }
+//    }
+//  }
+//
+//  when (sfence_dup(0).valid) {
+//    val l1asidhit = VecInit(l1asids.map(_ === sfence_dup(0).bits.asid)).asUInt
+//    val spasidhit = VecInit(spasids.map(_ === sfence_dup(0).bits.asid)).asUInt
+//    val sfence_vpn = sfence_dup(0).bits.addr(sfence_dup(0).bits.addr.getWidth-1, offLen)
+//
+//    when (sfence_dup(0).bits.rs1/*va*/) {
+//      when (sfence_dup(0).bits.rs2) {
+//        // all va && all asid
+//        l1v := 0.U
+//        l2v := 0.U
+//        spv := 0.U
+//      } .otherwise {
+//        // all va && specific asid except global
+//
+//        l1v := l1v & (~l1asidhit | l1g)
+//        l2v := l2v & l2g
+//        spv := spv & (~spasidhit | spg)
+//      }
+//    } .otherwise {
+//      // val flushMask = UIntToOH(genTlbL2Idx(sfence.bits.addr(sfence.bits.addr.getWidth-1, offLen)))
+//      val flushSetIdxOH = UIntToOH(genPtwL3SetIdx(sfence_vpn))
+//      // val flushMask = VecInit(flushSetIdxOH.asBools.map(Fill(l2tlbParams.l3nWays, _.asUInt))).asUInt
+//      val flushMask = VecInit(flushSetIdxOH.asBools.map { a => Fill(l2tlbParams.l3nWays, a.asUInt) }).asUInt
+//      flushSetIdxOH.suggestName(s"sfence_nrs1_flushSetIdxOH")
+//      flushMask.suggestName(s"sfence_nrs1_flushMask")
+//
+//      when (sfence_dup(0).bits.rs2) {
+//        // specific leaf of addr && all asid
+//        spv := spv & (~VecInit(sp.map(_.hit(sfence_vpn, sfence_dup(0).bits.asid, ignoreAsid = true))).asUInt)
+//      } .otherwise {
+//        // specific leaf of addr && specific asid
+//        spv := spv & (~VecInit(sp.map(_.hit(sfence_vpn, sfence_dup(0).bits.asid))).asUInt | spg)
+//      }
+//    }
+//  }
+//
+//  def InsideStageConnect(in: DecoupledIO[PtwCacheReq], out: DecoupledIO[PtwCacheReq], inFire: Bool): Unit = {
+//    in.ready := !in.valid || out.ready
+//    out.valid := in.valid
+//    out.bits := in.bits
+//    out.bits.bypassed.zip(in.bits.bypassed).zipWithIndex.map{ case (b, i) =>
+//      val bypassed_reg = Reg(Bool())
+//      val bypassed_wire = refill_bypass(in.bits.req_info.vpn, i) && io.refill.valid
+//      when (inFire) { bypassed_reg := bypassed_wire }
+//      .elsewhen (io.refill.valid) { bypassed_reg := bypassed_reg || bypassed_wire }
+//
+//      b._1 := b._2 || (bypassed_wire || (bypassed_reg && !inFire))
+//    }
+//  }
+//
+//  // Perf Count
+//  val resp_l3 = resp_res.l3.hit
+//  val resp_sp = resp_res.sp.hit
+//  val resp_l1_pre = resp_res.l1.pre
+//  val resp_l2_pre = resp_res.l2.pre
+//  val resp_l3_pre = resp_res.l3.pre
+//  val resp_sp_pre = resp_res.sp.pre
+//  val base_valid_access_0 = !from_pre(io.resp.bits.req_info.source) && io.resp.fire()
+//  XSPerfAccumulate("access", base_valid_access_0)
+//  XSPerfAccumulate("l1_hit", base_valid_access_0 && io.resp.bits.toFsm.l1Hit && !io.resp.bits.toFsm.l2Hit && !io.resp.bits.hit)
+//  XSPerfAccumulate("l2_hit", base_valid_access_0 && io.resp.bits.toFsm.l2Hit && !io.resp.bits.hit)
+//  XSPerfAccumulate("l3_hit", base_valid_access_0 && resp_l3)
+//  XSPerfAccumulate("sp_hit", base_valid_access_0 && resp_sp)
+//  XSPerfAccumulate("pte_hit",base_valid_access_0 && io.resp.bits.hit)
+//
+//  XSPerfAccumulate("l1_hit_pre", base_valid_access_0 && resp_l1_pre && io.resp.bits.toFsm.l1Hit && !io.resp.bits.toFsm.l2Hit && !io.resp.bits.hit)
+//  XSPerfAccumulate("l2_hit_pre", base_valid_access_0 && resp_l2_pre && io.resp.bits.toFsm.l2Hit && !io.resp.bits.hit)
+//  XSPerfAccumulate("l3_hit_pre", base_valid_access_0 && resp_l3_pre && resp_l3)
+//  XSPerfAccumulate("sp_hit_pre", base_valid_access_0 && resp_sp_pre && resp_sp)
+//  XSPerfAccumulate("pte_hit_pre",base_valid_access_0 && (resp_l3_pre && resp_l3 || resp_sp_pre && resp_sp) && io.resp.bits.hit)
+//
+//  val base_valid_access_1 = from_pre(io.resp.bits.req_info.source) && io.resp.fire()
+//  XSPerfAccumulate("pre_access", base_valid_access_1)
+//  XSPerfAccumulate("pre_l1_hit", base_valid_access_1 && io.resp.bits.toFsm.l1Hit && !io.resp.bits.toFsm.l2Hit && !io.resp.bits.hit)
+//  XSPerfAccumulate("pre_l2_hit", base_valid_access_1 && io.resp.bits.toFsm.l2Hit && !io.resp.bits.hit)
+//  XSPerfAccumulate("pre_l3_hit", base_valid_access_1 && resp_l3)
+//  XSPerfAccumulate("pre_sp_hit", base_valid_access_1 && resp_sp)
+//  XSPerfAccumulate("pre_pte_hit",base_valid_access_1 && io.resp.bits.hit)
+//
+//  XSPerfAccumulate("pre_l1_hit_pre", base_valid_access_1 && resp_l1_pre && io.resp.bits.toFsm.l1Hit && !io.resp.bits.toFsm.l2Hit && !io.resp.bits.hit)
+//  XSPerfAccumulate("pre_l2_hit_pre", base_valid_access_1 && resp_l2_pre && io.resp.bits.toFsm.l2Hit && !io.resp.bits.hit)
+//  XSPerfAccumulate("pre_l3_hit_pre", base_valid_access_1 && resp_l3_pre && resp_l3)
+//  XSPerfAccumulate("pre_sp_hit_pre", base_valid_access_1 && resp_sp_pre && resp_sp)
+//  XSPerfAccumulate("pre_pte_hit_pre",base_valid_access_1 && (resp_l3_pre && resp_l3 || resp_sp_pre && resp_sp) && io.resp.bits.hit)
+//
+//  val base_valid_access_2 = stageResp.bits.isFirst && !from_pre(io.resp.bits.req_info.source) && io.resp.fire()
+//  XSPerfAccumulate("access_first", base_valid_access_2)
+//  XSPerfAccumulate("l1_hit_first", base_valid_access_2 && io.resp.bits.toFsm.l1Hit && !io.resp.bits.toFsm.l2Hit && !io.resp.bits.hit)
+//  XSPerfAccumulate("l2_hit_first", base_valid_access_2 && io.resp.bits.toFsm.l2Hit && !io.resp.bits.hit)
+//  XSPerfAccumulate("l3_hit_first", base_valid_access_2 && resp_l3)
+//  XSPerfAccumulate("sp_hit_first", base_valid_access_2 && resp_sp)
+//  XSPerfAccumulate("pte_hit_first",base_valid_access_2 && io.resp.bits.hit)
+//
+//  XSPerfAccumulate("l1_hit_pre_first", base_valid_access_2 && resp_l1_pre && io.resp.bits.toFsm.l1Hit && !io.resp.bits.toFsm.l2Hit && !io.resp.bits.hit)
+//  XSPerfAccumulate("l2_hit_pre_first", base_valid_access_2 && resp_l2_pre && io.resp.bits.toFsm.l2Hit && !io.resp.bits.hit)
+//  XSPerfAccumulate("l3_hit_pre_first", base_valid_access_2 && resp_l3_pre && resp_l3)
+//  XSPerfAccumulate("sp_hit_pre_first", base_valid_access_2 && resp_sp_pre && resp_sp)
+//  XSPerfAccumulate("pte_hit_pre_first",base_valid_access_2 && (resp_l3_pre && resp_l3 || resp_sp_pre && resp_sp) && io.resp.bits.hit)
+//
+//  val base_valid_access_3 = stageResp.bits.isFirst && from_pre(io.resp.bits.req_info.source) && io.resp.fire()
+//  XSPerfAccumulate("pre_access_first", base_valid_access_3)
+//  XSPerfAccumulate("pre_l1_hit_first", base_valid_access_3 && io.resp.bits.toFsm.l1Hit && !io.resp.bits.toFsm.l2Hit && !io.resp.bits.hit)
+//  XSPerfAccumulate("pre_l2_hit_first", base_valid_access_3 && io.resp.bits.toFsm.l2Hit && !io.resp.bits.hit)
+//  XSPerfAccumulate("pre_l3_hit_first", base_valid_access_3 && resp_l3)
+//  XSPerfAccumulate("pre_sp_hit_first", base_valid_access_3 && resp_sp)
+//  XSPerfAccumulate("pre_pte_hit_first", base_valid_access_3 && io.resp.bits.hit)
+//
+//  XSPerfAccumulate("pre_l1_hit_pre_first", base_valid_access_3 && resp_l1_pre && io.resp.bits.toFsm.l1Hit && !io.resp.bits.toFsm.l2Hit && !io.resp.bits.hit)
+//  XSPerfAccumulate("pre_l2_hit_pre_first", base_valid_access_3 && resp_l2_pre && io.resp.bits.toFsm.l2Hit && !io.resp.bits.hit)
+//  XSPerfAccumulate("pre_l3_hit_pre_first", base_valid_access_3 && resp_l3_pre && resp_l3)
+//  XSPerfAccumulate("pre_sp_hit_pre_first", base_valid_access_3 && resp_sp_pre && resp_sp)
+//  XSPerfAccumulate("pre_pte_hit_pre_first",base_valid_access_3 && (resp_l3_pre && resp_l3 || resp_sp_pre && resp_sp) && io.resp.bits.hit)
+//
+//  XSPerfAccumulate("rwHarzad", io.req.valid && !io.req.ready)
+//  XSPerfAccumulate("out_blocked", io.resp.valid && !io.resp.ready)
+//  l1AccessPerf.zipWithIndex.map{ case (l, i) => XSPerfAccumulate(s"L1AccessIndex${i}", l) }
+//  l2AccessPerf.zipWithIndex.map{ case (l, i) => XSPerfAccumulate(s"L2AccessIndex${i}", l) }
+//  l3AccessPerf.zipWithIndex.map{ case (l, i) => XSPerfAccumulate(s"L3AccessIndex${i}", l) }
+//  spAccessPerf.zipWithIndex.map{ case (l, i) => XSPerfAccumulate(s"SPAccessIndex${i}", l) }
+//  l1RefillPerf.zipWithIndex.map{ case (l, i) => XSPerfAccumulate(s"L1RefillIndex${i}", l) }
+//  l2RefillPerf.zipWithIndex.map{ case (l, i) => XSPerfAccumulate(s"L2RefillIndex${i}", l) }
+//  l3RefillPerf.zipWithIndex.map{ case (l, i) => XSPerfAccumulate(s"L3RefillIndex${i}", l) }
+//  spRefillPerf.zipWithIndex.map{ case (l, i) => XSPerfAccumulate(s"SPRefillIndex${i}", l) }
+//
+//  XSPerfAccumulate("l1Refill", Cat(l1RefillPerf).orR)
+//  XSPerfAccumulate("l2Refill", Cat(l2RefillPerf).orR)
+//  XSPerfAccumulate("l3Refill", Cat(l3RefillPerf).orR)
+//  XSPerfAccumulate("spRefill", Cat(spRefillPerf).orR)
+//  XSPerfAccumulate("l1Refill_pre", Cat(l1RefillPerf).orR && refill_prefetch_dup(0))
+//  XSPerfAccumulate("l2Refill_pre", Cat(l2RefillPerf).orR && refill_prefetch_dup(0))
+//  XSPerfAccumulate("l3Refill_pre", Cat(l3RefillPerf).orR && refill_prefetch_dup(0))
+//  XSPerfAccumulate("spRefill_pre", Cat(spRefillPerf).orR && refill_prefetch_dup(0))
+//
+//  // debug
+//  XSDebug(sfence_dup(0).valid, p"[sfence] original v and g vector:\n")
+//  XSDebug(sfence_dup(0).valid, p"[sfence] l1v:${Binary(l1v)}\n")
+//  XSDebug(sfence_dup(0).valid, p"[sfence] l2v:${Binary(l2v)}\n")
+//  XSDebug(sfence_dup(0).valid, p"[sfence] l3v:${Binary(l3v)}\n")
+//  XSDebug(sfence_dup(0).valid, p"[sfence] l3g:${Binary(l3g)}\n")
+//  XSDebug(sfence_dup(0).valid, p"[sfence] spv:${Binary(spv)}\n")
+//  XSDebug(RegNext(sfence_dup(0).valid), p"[sfence] new v and g vector:\n")
+//  XSDebug(RegNext(sfence_dup(0).valid), p"[sfence] l1v:${Binary(l1v)}\n")
+//  XSDebug(RegNext(sfence_dup(0).valid), p"[sfence] l2v:${Binary(l2v)}\n")
+//  XSDebug(RegNext(sfence_dup(0).valid), p"[sfence] l3v:${Binary(l3v)}\n")
+//  XSDebug(RegNext(sfence_dup(0).valid), p"[sfence] l3g:${Binary(l3g)}\n")
+//  XSDebug(RegNext(sfence_dup(0).valid), p"[sfence] spv:${Binary(spv)}\n")
+//
+//  val perfEvents = Seq(
+//    ("access           ", base_valid_access_0             ),
+//    ("l1_hit           ", l1Hit                           ),
+//    ("l2_hit           ", l2Hit                           ),
+//    ("l3_hit           ", l3Hit                           ),
+//    ("sp_hit           ", spHit                           ),
+//    ("pte_hit          ", l3Hit || spHit                  ),
+//    ("rwHarzad         ",  io.req.valid && !io.req.ready  ),
+//    ("out_blocked      ",  io.resp.valid && !io.resp.ready),
+//  )
+//  generatePerfEvent()
+//}
diff --git a/src/main/scala/xiangshan/cache/mmu/PageTableWalker.scala b/src/main/scala/xiangshan/cache/mmu/PageTableWalker.scala
index 9b8eb9eef..2b189eb33 100644
--- a/src/main/scala/xiangshan/cache/mmu/PageTableWalker.scala
+++ b/src/main/scala/xiangshan/cache/mmu/PageTableWalker.scala
@@ -34,6 +34,336 @@ import xiangshan.backend.fu.{PMPReqBundle, PMPRespBundle}
   */
 
 
+/**HPTW, hypervisor PTW
+ *  guest physical address translation, guest physical address -> host physical address
+ */
+class HPTWIO()(implicit p: Parameters) extends MMUIOBaseBundle with HasPtwConst {
+  val ptw = new Bundle {
+    val req = Flipped(DecoupledIO(new Bundle {
+      val id = UInt(log2Up(l2tlbParams.llptwsize).W)
+      val gpaddr = UInt(XLEN.W)
+      val sfence = new SfenceBundle
+      val hgatp = new TlbSatpBundle
+      val hlvx = Bool()
+      val mxr = Bool() //in the future, it will move to TLB.scala
+      val cmd = TlbCmd()
+    }))
+    val resp = Valid(new Bundle {
+      val resp = Output(UInt(XLEN.W))
+      val level = Output(UInt(log2Up(Level).W))
+      val af = Output(Bool())
+      val pf = Output(Bool())
+    })
+  }
+  val llptw = new Bundle {
+    val req = Flipped(DecoupledIO(new Bundle {
+      val id = UInt(log2Up(l2tlbParams.llptwsize).W)
+      val gpaddr = UInt(XLEN.W)
+      val sfence = new SfenceBundle
+      val hgatp = new TlbSatpBundle
+      val hlvx = Bool()
+    }))
+    val resp = Valid(new Bundle {
+      val hpaddr = Output(UInt(XLEN.W))
+      val id = Output(UInt(log2Up(l2tlbParams.llptwsize).W))
+      val af = Output(Bool())
+      val pf = Output(Bool())
+    })
+  }
+  val pageCache = new Bundle {
+    val req = Flipped(DecoupledIO(new Bundle {
+      val gpaddr = UInt(XLEN.W)
+      val sfence = new SfenceBundle
+      val hgatp = new TlbSatpBundle
+      val hlvx = Bool()
+    }))
+    val resp = Valid(new Bundle {
+      val resp = Output(UInt(XLEN.W))
+      val level = Output(UInt(log2Up(Level).W))
+      val af = Output(Bool())
+      val pf = Output(Bool())
+    })
+  }
+  val mem = new Bundle {
+    val req = DecoupledIO(new L2TlbMemReqBundle())
+    val resp = Flipped(ValidIO(UInt(XLEN.W)))
+    val mask = Input(Bool())
+  }
+  val pmp = new Bundle {
+    val req = ValidIO(new PMPReqBundle())
+    val resp = Flipped(new PMPRespBundle())
+  }
+}
+
+@chiselName
+class HPTW()(implicit p: Parameters) extends XSModule with HasPtwConst {
+  val io = IO(new HPTWIO)
+  val mxr = io.ptw.req.bits.mxr
+  val hgatp =  io.ptw.req.bits.hgatp
+  val sfence = io.ptw.req.bits.sfence
+  val flush = sfence.valid || hgatp.changed
+
+  val level = RegInit(0.U(log2Up(Level).W))
+  val gpaddr = Reg(UInt(XLEN.W))
+  val vpn = gpaddr(40, 12)
+  val levelNext = level + 1.U
+  val Pg_base = MakeGAddr(hgatp.ppn, getGVpnn(vpn, 2.U))
+  val Pte = io.mem.resp.bits.asTypeOf(new PteBundle().cloneType)
+  val p_Pte = MakeAddr(Pte.ppn, getVpnn(vpn, 2.U - level))
+  val mem_addr = Mux(level === 0.U, Pg_base, p_Pte)
+  val hlvx = RegInit(false.B)
+  val cmd = Reg(TlbCmd())
+
+  // s/w register
+  val s_pmp_check = RegInit(true.B)
+  val s_mem_req = RegInit(true.B)
+  val w_mem_resp = RegInit(true.B)
+  val mem_addr_update = RegInit(false.B)
+  val idle = RegInit(true.B)
+  val finish = WireInit(false.B)
+  val sent_to_pmp = idle === false.B && (s_pmp_check === false.B || mem_addr_update) && !finish
+
+  val pageFault = Pte.isPf(level) || (Pte.isLeaf() && hlvx  && !Pte.perm.x) || (Pte.isLeaf() && TlbCmd.isWrite(cmd) && !(Pte.perm.r && Pte.perm.w))
+  val accessFault = RegEnable(io.pmp.resp.ld || io.pmp.resp.mmio, sent_to_pmp)
+
+  val ppn_af = Pte.isAf()
+  val find_pte = Pte.isLeaf() || ppn_af || pageFault
+
+
+  val resp_valid = idle === false.B && mem_addr_update && ((w_mem_resp && find_pte) || (s_pmp_check && accessFault))
+
+
+  val id = Reg(UInt(log2Up(l2tlbParams.llptwsize).W))
+  io.ptw.req.ready := idle
+  io.llptw.req.ready := idle
+
+  io.ptw.resp.valid := resp_valid
+  io.ptw.resp.bits.resp := io.mem.resp.bits
+  io.ptw.resp.bits.level := level
+  io.ptw.resp.bits.af := accessFault || ppn_af
+  io.ptw.resp.bits.pf := pageFault && !accessFault && !ppn_af
+
+  io.pageCache := DontCare
+  io.llptw := DontCare
+
+  io.pmp.req.valid := DontCare
+  io.pmp.req.bits.addr := mem_addr
+  io.pmp.req.bits.size := 3.U
+  io.pmp.req.bits.cmd := TlbCmd.read
+
+  io.mem.req.valid := s_mem_req === false.B && !io.mem.mask && !accessFault && s_pmp_check
+  io.mem.req.bits.addr := mem_addr
+  io.mem.req.bits.id := HptwReqID.U(bMemID.W)
+
+  when (idle){
+    when(io.pageCache.req.fire()){
+      level := 0.U
+      idle := false.B
+      gpaddr := io.pageCache.req.bits.gpaddr
+      hlvx := io.pageCache.req.bits.hlvx
+      accessFault := false.B
+      s_pmp_check := false.B
+    }.elsewhen (io.ptw.req.fire()){
+      level := 0.U
+      idle := false.B
+      gpaddr := io.ptw.req.bits.gpaddr
+      hlvx := io.ptw.req.bits.hlvx
+      cmd := io.ptw.req.bits.cmd
+      accessFault := false.B
+      s_pmp_check := false.B
+      id := io.ptw.req.bits.id
+    }.elsewhen(io.llptw.req.fire()){
+      level := 0.U
+      idle := false.B
+      gpaddr := io.llptw.req.bits.gpaddr
+      hlvx := io.llptw.req.bits.hlvx
+      accessFault := false.B
+      s_pmp_check := false.B
+      id := io.llptw.req.bits.id
+    }
+  }
+
+  when(sent_to_pmp && mem_addr_update === false.B){
+    s_mem_req := false.B
+    s_pmp_check := true.B
+  }
+
+  when(accessFault && idle === false.B){
+    s_pmp_check := true.B
+    s_mem_req := true.B
+    w_mem_resp := true.B
+    mem_addr_update := true.B
+  }
+
+  when(io.mem.req.fire()){
+    s_mem_req := true.B
+    w_mem_resp := false.B
+  }
+
+  when(io.mem.resp.fire() && w_mem_resp === false.B){
+    w_mem_resp := true.B
+    mem_addr_update := true.B
+  }
+
+  when(mem_addr_update){
+    when(!(find_pte || accessFault)){
+      level := levelNext
+      s_mem_req := false.B
+      mem_addr_update := false.B
+    }.elsewhen(resp_valid){
+      when(io.ptw.resp.fire()){
+        idle := true.B
+        mem_addr_update := false.B
+        accessFault := false.B
+      }
+      finish := true.B
+    }
+  }
+
+}
+
+//@chiselName
+//class HPTW()(implicit p: Parameters) extends XSModule with HasPtwConst {
+//  val io = IO(new HPTWIO)
+//  val fromptw = RegInit(false.B)
+//  val frompageCache = RegInit(false.B)
+//  val hgatp = Mux(frompageCache, io.pageCache.req.bits.hgatp, Mux(fromptw, io.ptw.req.bits.hgatp, io.llptw.req.bits.hgatp))
+//  val sfence = Mux(frompageCache, io.pageCache.req.bits.sfence, Mux(fromptw, io.ptw.req.bits.sfence, io.llptw.req.bits.sfence))
+//  val flush = sfence.valid || hgatp.changed
+//
+//  val level = RegInit(0.U(log2Up(Level).W))
+//  val gpaddr = Reg(UInt(XLEN.W))
+//  val hlvx = RegInit(false.B)
+//  val vpn = gpaddr(40, 12)
+//  val levelNext = level + 1.U
+//  val Pg_base = MakeGAddr(hgatp.ppn, getGVpnn(vpn, 2.U))
+//  val Pte = io.mem.resp.bits.asTypeOf(new PteBundle().cloneType)
+//  val p_Pte = MakeAddr(Pte.ppn, getVpnn(vpn, 2.U - level))
+//  val mem_addr = Mux(level === 0.U, Pg_base, p_Pte)
+//
+//  // s/w register
+//  val s_pmp_check = RegInit(true.B)
+//  val s_mem_req = RegInit(true.B)
+//  val w_mem_resp = RegInit(true.B)
+//  val mem_addr_update = RegInit(false.B)
+//  val idle = RegInit(true.B)
+//  val finish = WireInit(false.B)
+//  val sent_to_pmp = idle === false.B && (s_pmp_check === false.B || mem_addr_update) && !finish
+//
+//  val pageFault = Pte.isPf(level) || (Pte.isLeaf() && hlvx  && !Pte.perm.x)
+//  val accessFault = RegEnable(io.pmp.resp.ld || io.pmp.resp.mmio, sent_to_pmp)
+//
+//  val ppn_af = Pte.isAf()
+//  val find_pte = Pte.isLeaf() || ppn_af || pageFault
+//
+//
+//  val resp_valid = idle === false.B && mem_addr_update && ((w_mem_resp && find_pte) || (s_pmp_check && accessFault))
+//
+//
+//  val id = Reg(UInt(log2Up(l2tlbParams.llptwsize).W))
+//  io.ptw.req.ready := idle
+//  io.llptw.req.ready := idle
+//
+//  io.pageCache.resp.valid := frompageCache && resp_valid
+//  io.ptw.resp.valid := fromptw && resp_valid
+//  io.llptw.resp.valid := !fromptw && !frompageCache && resp_valid
+//  io.pageCache.resp.bits.resp := io.mem.resp.bits
+//  io.pageCache.resp.bits.level := level
+//  io.pageCache.resp.bits.af := accessFault || ppn_af
+//  io.pageCache.resp.bits.pf := pageFault && !accessFault && !ppn_af
+//
+//  io.ptw.resp.bits.resp := io.mem.resp.bits
+//  io.ptw.resp.bits.level := level
+//  io.ptw.resp.bits.af := accessFault || ppn_af
+//  io.ptw.resp.bits.pf := pageFault && !accessFault && !ppn_af
+//
+//  io.llptw.resp.bits.hpaddr := Cat(io.mem.resp.bits.asTypeOf(new PteBundle().cloneType).ppn, gpaddr(offLen - 1, 0))
+//  io.llptw.resp.bits.af := accessFault || ppn_af
+//  io.llptw.resp.bits.pf := pageFault && !accessFault && !ppn_af
+//  io.llptw.resp.bits.id := id
+//
+//  io.pmp.req.valid := DontCare
+//  io.pmp.req.bits.addr := mem_addr
+//  io.pmp.req.bits.size := 3.U
+//  io.pmp.req.bits.cmd := TlbCmd.read
+//
+//  io.mem.req.valid := s_mem_req === false.B && !io.mem.mask && !accessFault && s_pmp_check
+//  io.mem.req.bits.addr := mem_addr
+//  io.mem.req.bits.id := HptwReqID.U(bMemID.W)
+//
+//  when (idle){
+//    when(io.pageCache.req.fire()){
+//      level := 0.U
+//      frompageCache := true.B
+//      fromptw := false.B
+//      idle := false.B
+//      gpaddr := io.ptw.req.bits.gpaddr
+//      hlvx := io.ptw.req.bits.hlvx
+//      accessFault := false.B
+//      s_pmp_check := false.B
+//    }.elsewhen (io.ptw.req.fire()){
+//      level := 0.U
+//      frompageCache := false.B
+//      fromptw := true.B
+//      idle := false.B
+//      gpaddr := io.ptw.req.bits.gpaddr
+//      hlvx := io.ptw.req.bits.hlvx
+//      accessFault := false.B
+//      s_pmp_check := false.B
+//      id := io.ptw.req.bits.id
+//    }.elsewhen(io.llptw.req.fire()){
+//      level := 0.U
+//      fromptw := false.B
+//      frompageCache := false.B
+//      idle := false.B
+//      gpaddr := io.llptw.req.bits.gpaddr
+//      hlvx := io.ptw.req.bits.hlvx
+//      accessFault := false.B
+//      s_pmp_check := false.B
+//      id := io.llptw.req.bits.id
+//    }
+//  }
+//
+//  when(sent_to_pmp && mem_addr_update === false.B){
+//    s_mem_req := false.B
+//    s_pmp_check := true.B
+//  }
+//
+//  when(accessFault && idle === false.B){
+//    s_pmp_check := true.B
+//    s_mem_req := true.B
+//    w_mem_resp := true.B
+//    mem_addr_update := true.B
+//  }
+//
+//  when(io.mem.req.fire()){
+//    s_mem_req := true.B
+//    w_mem_resp := false.B
+//  }
+//
+//  when(io.mem.resp.fire() && w_mem_resp === false.B){
+//    w_mem_resp := true.B
+//    mem_addr_update := true.B
+//  }
+//
+//  when(mem_addr_update){
+//    when(!(find_pte || accessFault)){
+//      level := levelNext
+//      s_mem_req := false.B
+//      mem_addr_update := false.B
+//    }.elsewhen(resp_valid){
+//      when(io.ptw.resp.fire() || io.llptw.resp.fire()){
+//        idle := true.B
+//        mem_addr_update := false.B
+//        accessFault := false.B
+//      }
+//      finish := true.B
+//    }
+//  }
+//
+//}
+
+
 /** PTW : page table walker
   * a finite state machine
   * only take 1GB and 2MB page walks
@@ -47,6 +377,7 @@ class PTWIO()(implicit p: Parameters) extends MMUIOBaseBundle with HasPtwConst {
   }))
   val resp = DecoupledIO(new Bundle {
     val source = UInt(bSourceWidth.W)
+    val hlvx = Bool() // for difftest
     val resp = new PtwResp
   })
 
@@ -54,11 +385,30 @@ class PTWIO()(implicit p: Parameters) extends MMUIOBaseBundle with HasPtwConst {
   // NOTE: llptw change from "connect to llptw" to "connect to page cache"
   // to avoid corner case that caused duplicate entries
 
+  val hptw = new Bundle {
+      val req = DecoupledIO(new Bundle {
+        val id = UInt(log2Up(l2tlbParams.llptwsize).W)
+        val gpaddr = UInt(XLEN.W)
+        val sfence = new SfenceBundle
+        val hgatp = new TlbSatpBundle
+        val hlvx = Bool()
+        val mxr = Bool() //in the future, it will move to TLB.scala
+        val cmd = TlbCmd()
+      })
+    val resp = Flipped(Valid(new Bundle {
+      val resp = Output(UInt(XLEN.W))
+      val level = Output(UInt(log2Up(Level).W))
+      val af = Output(Bool())
+      val pf = Output(Bool())
+    }))
+  }
+
   val mem = new Bundle {
     val req = DecoupledIO(new L2TlbMemReqBundle())
     val resp = Flipped(ValidIO(UInt(XLEN.W)))
     val mask = Input(Bool())
   }
+
   val pmp = new Bundle {
     val req = ValidIO(new PMPReqBundle())
     val resp = Flipped(new PMPRespBundle())
@@ -75,75 +425,147 @@ class PTW()(implicit p: Parameters) extends XSModule with HasPtwConst with HasPe
   val io = IO(new PTWIO)
   val sfence = io.sfence
   val mem = io.mem
-  val satp = io.csr.satp
-  val flush = io.sfence.valid || io.csr.satp.changed
+  val hgatp = io.csr.hgatp
+  val hyperInst = RegInit(false.B)
+  val hlvx = RegInit(false.B)
+  val virt = RegInit(false.B) //virt will change when a exception is raised
+
+  val satp = Mux((virt || hyperInst), io.csr.vsatp, io.csr.satp)
+  val flush = io.sfence.valid || satp.changed
+  val onlyS1xlate = satp.mode =/= 0.U && hgatp.mode === 0.U
+  val onlyS2xlate = satp.mode === 0.U && hgatp.mode =/= 0.U
+  val s2xlate = (virt || hyperInst) && !onlyS1xlate
 
   val level = RegInit(0.U(log2Up(Level).W))
   val af_level = RegInit(0.U(log2Up(Level).W)) // access fault return this level
   val ppn = Reg(UInt(ppnLen.W))
   val vpn = Reg(UInt(vpnLen.W))
+  val gvpn = Reg(UInt(gvpnLen.W)) // for the cases: 1. satp == 0 and hgatp != 0 and exec hlv; 2. virtmode == 1, vsatp == 0 and hgatp != 0
+  val cmd = Reg(TlbCmd())
+
   val levelNext = level + 1.U
-  val l1Hit = Reg(Bool())
-  val memPte = mem.resp.bits.asTypeOf(new PteBundle().cloneType)
+  val pte = mem.resp.bits.asTypeOf(new PteBundle().cloneType)
+  val hptw_pte = io.hptw.resp.bits.resp.asTypeOf(new PteBundle().cloneType)
+  val hptw_level = io.hptw.resp.bits.level
 
   // s/w register
   val s_pmp_check = RegInit(true.B)
   val s_mem_req = RegInit(true.B)
-  val s_llptw_req = RegInit(true.B)
   val w_mem_resp = RegInit(true.B)
+  val s_hptw_req = RegInit(true.B)
+  val w_hptw_resp = RegInit(true.B)
+  val s_last_hptw_req = RegInit(true.B)
+  val w_last_hptw_resp = RegInit(true.B)
   // for updating "level"
   val mem_addr_update = RegInit(false.B)
-
   val idle = RegInit(true.B)
   val finish = WireInit(false.B)
   val sent_to_pmp = idle === false.B && (s_pmp_check === false.B || mem_addr_update) && !finish
 
-  val pageFault = memPte.isPf(level)
+  val pageFault = Mux(onlyS2xlate, false.B, pte.isPf(level))
   val accessFault = RegEnable(io.pmp.resp.ld || io.pmp.resp.mmio, sent_to_pmp)
 
-  val ppn_af = memPte.isAf()
-  val find_pte = memPte.isLeaf() || ppn_af || pageFault
-  val to_find_pte = level === 1.U && find_pte === false.B
+  val hptw_pageFault = RegInit(false.B)
+  val hptw_accessFault = RegInit(false.B)
+  val last_s2xlate = RegInit(false.B) //we need to return host pte other than guest pte
+
+  val ppn_af = Mux(s2xlate, false.B, pte.isAf())
+  val find_pte = pte.isLeaf() || ppn_af || pageFault || hptw_pageFault || onlyS2xlate
+
   val source = RegEnable(io.req.bits.req_info.source, io.req.fire())
+  val vaddr = Cat(gvpn, 0.U(offLen.W))
+  val pg_base = MakeAddr(satp.ppn, getVpnn(vpn, 2))
+  val p_pte = MakeAddr(pte.ppn, getVpnn(vpn, 2.U - level))
+
+  val mem_addr = Mux(af_level === 0.U, pg_base, p_pte)
 
-  val l1addr = MakeAddr(satp.ppn, getVpnn(vpn, 2))
-  val l2addr = MakeAddr(Mux(l1Hit, ppn, memPte.ppn), getVpnn(vpn, 1))
-  val mem_addr = Mux(af_level === 0.U, l1addr, l2addr)
+  val gpaddr = Mux(onlyS2xlate, Cat(gvpn, 0.U(offLen.W)), mem_addr)
+  val hpaddr = MakeHPaddr(hptw_pte.ppn, hptw_level, gpaddr)
 
   io.req.ready := idle
 
-  io.resp.valid := idle === false.B && mem_addr_update && ((w_mem_resp && find_pte) || (s_pmp_check && accessFault))
+  io.resp.valid := !idle && mem_addr_update && !last_s2xlate && ((w_mem_resp && find_pte) || (s_pmp_check && accessFault) || onlyS2xlate)
   io.resp.bits.source := source
-  io.resp.bits.resp.apply(pageFault && !accessFault && !ppn_af, accessFault || ppn_af, Mux(accessFault, af_level,level), memPte, vpn, satp.asid)
+  io.resp.bits.hlvx := hlvx
+  val ret_pte = Wire(new PteBundle)
+  ret_pte.reserved := hptw_pte.reserved
+  ret_pte.ppn_high := hptw_pte.ppn_high
+  ret_pte.ppn := hpaddr >> offLen
+  ret_pte.rsw := hptw_pte.rsw
+  ret_pte.perm := pte.perm
+  io.resp.bits.resp.apply(pageFault && !accessFault && !ppn_af, hptw_accessFault || accessFault || ppn_af, hptw_pageFault, Mux(accessFault, af_level, Mux(onlyS2xlate, hptw_level, level)), Mux(s2xlate, ret_pte, pte), vpn, satp.asid, gpaddr >> 12.U, hgatp.asid, s2xlate)
 
-  io.llptw.valid := s_llptw_req === false.B && to_find_pte && !accessFault
-  io.llptw.bits.req_info.source := source
-  io.llptw.bits.req_info.vpn := vpn
-  io.llptw.bits.ppn := memPte.ppn
 
   io.pmp.req.valid := DontCare // samecycle, do not use valid
-  io.pmp.req.bits.addr := mem_addr
+  io.pmp.req.bits.addr := Mux(s2xlate, hpaddr, mem_addr)
   io.pmp.req.bits.size := 3.U // TODO: fix it
   io.pmp.req.bits.cmd := TlbCmd.read
 
   mem.req.valid := s_mem_req === false.B && !mem.mask && !accessFault && s_pmp_check
-  mem.req.bits.addr := mem_addr
+  mem.req.bits.addr := Mux(s2xlate, hpaddr, mem_addr)
   mem.req.bits.id := FsmReqID.U(bMemID.W)
 
-  io.refill.req_info.vpn := vpn
-  io.refill.level := level
-  io.refill.req_info.source := source
+  io.llptw := DontCare
+  io.refill := DontCare
+
 
+  io.hptw.req.valid := s_hptw_req === false.B || s_last_hptw_req === false.B
+  io.hptw.req.bits.id := FsmReqID.U(bMemID.W)
+  io.hptw.req.bits.gpaddr := gpaddr
+  io.hptw.req.bits.sfence := io.sfence
+  io.hptw.req.bits.hgatp := io.csr.hgatp
+  io.hptw.req.bits.hlvx := hlvx
+  io.hptw.req.bits.mxr := io.csr.priv.mxr
+  io.hptw.req.bits.cmd := cmd
   when (io.req.fire()){
-    val req = io.req.bits
-    level := Mux(req.l1Hit, 1.U, 0.U)
-    af_level := Mux(req.l1Hit, 1.U, 0.U)
-    ppn := Mux(req.l1Hit, io.req.bits.ppn, satp.ppn)
+    level := 0.U
+    af_level := 0.U
+    ppn := Mux((io.csr.priv.virt || io.req.bits.req_info.hyperinst), io.csr.vsatp.ppn, io.csr.satp.ppn)
     vpn := io.req.bits.req_info.vpn
-    l1Hit := req.l1Hit
+    gvpn := io.req.bits.req_info.gvpn
+    hyperInst := io.req.bits.req_info.hyperinst
+    hlvx := io.req.bits.req_info.hlvx
+    virt := io.csr.priv.virt
+    cmd := io.req.bits.req_info.cmd
     accessFault := false.B
-    s_pmp_check := false.B
     idle := false.B
+    hptw_pageFault := false.B
+    when((io.req.bits.req_info.hyperinst || io.csr.priv.virt) && hgatp.mode =/= 0.U ){
+      last_s2xlate := true.B
+      s_hptw_req := false.B
+    }.otherwise{
+      s_pmp_check := false.B
+    }
+  }
+
+  when(io.hptw.req.fire() && s_hptw_req === false.B){
+    s_hptw_req := true.B
+    w_hptw_resp := false.B
+  }
+
+  when(io.hptw.resp.fire() && w_hptw_resp === false.B){
+    hptw_pageFault := io.hptw.resp.bits.pf
+    hptw_accessFault := io.hptw.resp.bits.af
+    w_hptw_resp := true.B
+    when(onlyS2xlate){
+      mem_addr_update := true.B
+      last_s2xlate := false.B
+    }.otherwise{
+      s_pmp_check := false.B
+    }
+  }
+
+  when(io.hptw.req.fire() && s_last_hptw_req === false.B){
+    w_last_hptw_resp := false.B
+    s_last_hptw_req := true.B
+  }
+
+  when(io.hptw.resp.fire() && w_last_hptw_resp === false.B){
+    hptw_pageFault := io.hptw.resp.bits.pf
+    hptw_accessFault := io.hptw.resp.bits.af
+    w_last_hptw_resp := true.B
+    mem_addr_update := true.B
+    last_s2xlate := false.B
   }
 
   when(sent_to_pmp && mem_addr_update === false.B){
@@ -155,8 +577,12 @@ class PTW()(implicit p: Parameters) extends XSModule with HasPtwConst with HasPe
     s_pmp_check := true.B
     s_mem_req := true.B
     w_mem_resp := true.B
-    s_llptw_req := true.B
+    s_hptw_req := true.B
+    w_hptw_resp := true.B
+    s_last_hptw_req := true.B
+    w_last_hptw_resp := true.B
     mem_addr_update := true.B
+    last_s2xlate := false.B
   }
 
   when (mem.req.fire()){
@@ -167,75 +593,327 @@ class PTW()(implicit p: Parameters) extends XSModule with HasPtwConst with HasPe
   when(mem.resp.fire() && w_mem_resp === false.B){
     w_mem_resp := true.B
     af_level := af_level + 1.U
-    s_llptw_req := false.B
     mem_addr_update := true.B
   }
 
   when(mem_addr_update){
-    when(level === 0.U && !(find_pte || accessFault)){
+    when(!(find_pte || accessFault)){
       level := levelNext
-      s_mem_req := false.B
-      s_llptw_req := true.B
-      mem_addr_update := false.B
-    }.elsewhen(io.llptw.valid){
-      when(io.llptw.fire()) {
-        idle := true.B
-        s_llptw_req := true.B
-        mem_addr_update := false.B
+      when (s2xlate){
+        s_hptw_req := false.B
+      }.otherwise{
+        s_mem_req := false.B
       }
-      finish := true.B
-    }.elsewhen(io.resp.valid){
-      when(io.resp.fire()) {
-        idle := true.B
-        s_llptw_req := true.B
+      mem_addr_update := false.B
+    }.elsewhen(find_pte){
+      when(s2xlate && last_s2xlate === true.B){
+        s_last_hptw_req := false.B
         mem_addr_update := false.B
-        accessFault := false.B
+      }.elsewhen(io.resp.valid){
+        when(io.resp.fire()) {
+          idle := true.B
+          mem_addr_update := false.B
+          accessFault := false.B
+        }
+        finish := true.B
       }
-      finish := true.B
     }
   }
 
-
   when (sfence.valid) {
     idle := true.B
     s_pmp_check := true.B
     s_mem_req := true.B
-    s_llptw_req := true.B
     w_mem_resp := true.B
     accessFault := false.B
     mem_addr_update := false.B
+    s_hptw_req := true.B
+    w_hptw_resp := true.B
+    s_last_hptw_req := true.B
+    w_last_hptw_resp := true.B
   }
-
-
   XSDebug(p"[ptw] level:${level} notFound:${pageFault}\n")
 
-  // perf
-  XSPerfAccumulate("fsm_count", io.req.fire())
-  for (i <- 0 until PtwWidth) {
-    XSPerfAccumulate(s"fsm_count_source${i}", io.req.fire() && io.req.bits.req_info.source === i.U)
-  }
-  XSPerfAccumulate("fsm_busy", !idle)
-  XSPerfAccumulate("fsm_idle", idle)
-  XSPerfAccumulate("resp_blocked", io.resp.valid && !io.resp.ready)
-  XSPerfAccumulate("ptw_ppn_af", io.resp.fire && ppn_af)
-  XSPerfAccumulate("mem_count", mem.req.fire())
-  XSPerfAccumulate("mem_cycle", BoolStopWatch(mem.req.fire, mem.resp.fire(), true))
-  XSPerfAccumulate("mem_blocked", mem.req.valid && !mem.req.ready)
+    // perf
+    XSPerfAccumulate("fsm_count", io.req.fire())
+    for (i <- 0 until PtwWidth) {
+      XSPerfAccumulate(s"fsm_count_source${i}", io.req.fire() && io.req.bits.req_info.source === i.U)
+    }
+    XSPerfAccumulate("fsm_busy", !idle)
+    XSPerfAccumulate("fsm_idle", idle)
+    XSPerfAccumulate("resp_blocked", io.resp.valid && !io.resp.ready)
+    XSPerfAccumulate("ptw_ppn_af", io.resp.fire && ppn_af)
+    XSPerfAccumulate("mem_count", mem.req.fire())
+    XSPerfAccumulate("mem_cycle", BoolStopWatch(mem.req.fire, mem.resp.fire(), true))
+    XSPerfAccumulate("mem_blocked", mem.req.valid && !mem.req.ready)
 
-  TimeOutAssert(!idle, timeOutThreshold, "page table walker time out")
+    TimeOutAssert(!idle, timeOutThreshold, "page table walker time out")
 
-  val perfEvents = Seq(
-    ("fsm_count         ", io.req.fire()                                     ),
-    ("fsm_busy          ", !idle                                             ),
-    ("fsm_idle          ", idle                                              ),
-    ("resp_blocked      ", io.resp.valid && !io.resp.ready                   ),
-    ("mem_count         ", mem.req.fire()                                    ),
-    ("mem_cycle         ", BoolStopWatch(mem.req.fire, mem.resp.fire(), true)),
-    ("mem_blocked       ", mem.req.valid && !mem.req.ready                   ),
-  )
-  generatePerfEvent()
+    val perfEvents = Seq(
+      ("fsm_count         ", io.req.fire()                                     ),
+      ("fsm_busy          ", !idle                                             ),
+      ("fsm_idle          ", idle                                              ),
+      ("resp_blocked      ", io.resp.valid && !io.resp.ready                   ),
+      ("mem_count         ", mem.req.fire()                                    ),
+      ("mem_cycle         ", BoolStopWatch(mem.req.fire, mem.resp.fire(), true)),
+      ("mem_blocked       ", mem.req.valid && !mem.req.ready                   ),
+    )
+    generatePerfEvent()
 }
 
+//@chiselName
+//class PTW()(implicit p: Parameters) extends XSModule with HasPtwConst with HasPerfEvents {
+//  val io = IO(new PTWIO)
+//  val sfence = io.sfence
+//  val mem = io.mem
+//  val virt = io.csr.priv.virt
+//  val hyperInst = RegInit(false.B)
+//  val hlvx = RegInit(false.B)
+//  val satp = Mux((virt || hyperInst), io.csr.vsatp, io.csr.satp)
+//  val hgatp = io.csr.hgatp
+//  val onlyS1xlate = satp.mode =/= 0.U && hgatp.mode === 0.U
+//  val onlyS2xlate = satp.mode === 0.U && hgatp.mode =/= 0.U
+//  val s2xlate = (virt || hyperInst) && !onlyS1xlate
+//  val flush = io.sfence.valid || satp.changed
+//
+//  val level = RegInit(0.U(log2Up(Level).W))
+//  val af_level = RegInit(0.U(log2Up(Level).W)) // access fault return this level
+//  val ppn = Reg(UInt(ppnLen.W))
+//  val vpn = Reg(UInt(vpnLen.W))
+//  val gvpn = Reg(UInt(gvpnLen.W)) // for the cases: 1. satp == 0 and hgatp != 0 and exec hlv; 2. virtmode == 1, vsatp == 0 and hgatp != 0
+//
+//  val levelNext = level + 1.U
+//  val l1Hit = Reg(Bool())
+//  val Pte = mem.resp.bits.asTypeOf(new PteBundle().cloneType)
+//  val hptw_Pte = io.hptw.resp.bits.resp.asTypeOf(new PteBundle().cloneType)
+//  val hptw_level = io.hptw.resp.bits.level
+//
+//  // s/w register
+//  val s_pmp_check = RegInit(true.B)
+//  val s_mem_req = RegInit(true.B)
+//  val s_llptw_req = RegInit(true.B)
+//  val w_mem_resp = RegInit(true.B)
+//  val s_hptw_req = RegInit(true.B)
+//  val w_hptw_resp = RegInit(true.B)
+//  val s_last_hptw_req = RegInit(true.B)
+//  val w_last_hptw_resp = RegInit(true.B)
+//  // for updating "level"
+//  val mem_addr_update = RegInit(false.B)
+//  val idle = RegInit(true.B)
+//  val finish = WireInit(false.B)
+//  val sent_to_pmp = idle === false.B && (s_pmp_check === false.B || mem_addr_update) && !finish
+//
+//  val pageFault = Pte.isPf(level) || (pte.isLeaf() && hlvx  && !pte.perm.x)
+//  val accessFault = RegEnable(io.pmp.resp.ld || io.pmp.resp.mmio, sent_to_pmp)
+//
+//  val hptw_pageFault = RegInit(false.B)
+//  val hptw_accessFault = RegInit(false.B)
+//  val last_s2xlate = RegInit(false.B) //we need to return host pte other than guest pte
+//
+//  val ppn_af = Mux(s2xlate, false.B, Pte.isAf())
+//  val find_pte = Pte.isLeaf() || ppn_af || pageFault || hptw_pageFault || onlyS2xlate
+//  val to_find_pte = level === 1.U && find_pte === false.B
+//  val source = RegEnable(io.req.bits.req_info.source, io.req.fire())
+//  val vaddr = Cat(gvpn, 0.U(offLen.W))
+//  val l1addr = MakeAddr(satp.ppn, getVpnn(vpn, 2))
+//  val l2addr = MakeAddr(Mux(l1Hit, ppn, Pte.ppn), getVpnn(vpn, 1))
+//  val mem_addr = Mux(af_level === 0.U, l1addr, l2addr)
+//
+//  val gpaddr = mem_addr
+//  val hpaddr = MakeHPaddr(hptw_Pte.ppn, hptw_level, gpaddr);
+//
+//  io.req.ready := idle
+//
+//  io.resp.valid := !idle && mem_addr_update && !last_s2xlate && ((w_mem_resp && find_pte) || (s_pmp_check && accessFault) || onlyS2xlate)
+//  io.resp.bits.source := source
+//  io.resp.bits.resp.apply(pageFault && !accessFault && !ppn_af, hptw_accessFault || accessFault || ppn_af, hptw_pageFault, Mux(accessFault, af_level,level), Mux(s2xlate, hptw_Pte, Pte), vpn, satp.asid, gpaddr >> 12.U, hgatp.asid, s2xlate)
+//
+//  io.llptw.valid := s_llptw_req === false.B && to_find_pte && !accessFault
+//  io.llptw.bits.req_info.source := source
+//  io.llptw.bits.req_info.vpn := vpn
+//  io.llptw.bits.req_info.gvpn := gvpn
+//  io.llptw.bits.ppn := Pte.ppn
+//  io.llptw.bits.req_info.hlvx := hlvx
+//  io.llptw.bits.req_info.hyperinst := hyperInst
+//  io.llptw.bits.req_info.virt := virt
+//
+//  io.pmp.req.valid := DontCare // samecycle, do not use valid
+//  io.pmp.req.bits.addr := Mux(s2xlate, hpaddr, mem_addr)
+//  io.pmp.req.bits.size := 3.U // TODO: fix it
+//  io.pmp.req.bits.cmd := TlbCmd.read
+//
+//  mem.req.valid := s_mem_req === false.B && !mem.mask && !accessFault && s_pmp_check
+//  mem.req.bits.addr := Mux(s2xlate, hpaddr, mem_addr)
+//  mem.req.bits.id := FsmReqID.U(bMemID.W)
+//
+//  io.refill.req_info.virt := virt
+//  io.refill.req_info.hyperinst := hyperInst
+//  io.refill.req_info.hlvx := hlvx
+//  io.refill.req_info.vpn := vpn
+//  io.refill.req_info.gvpn := gvpn
+//  io.refill.level := level
+//  io.refill.req_info.source := source
+//
+//  io.hptw.req.valid := s_hptw_req === false.B || s_last_hptw_req === false.B
+//  io.hptw.req.bits.id := FsmReqID.U(bMemID.W)
+//  io.hptw.req.bits.gpaddr := gpaddr
+//  io.hptw.req.bits.sfence := io.sfence
+//  io.hptw.req.bits.hgatp := io.csr.hgatp
+//  when (io.req.fire()){
+//    level := Mux(io.req.bits.l1Hit, 1.U, 0.U)
+//    af_level := Mux(io.req.bits.l1Hit, 1.U, 0.U)
+//    ppn := Mux(io.req.bits.l1Hit, io.req.bits.ppn, satp.ppn)
+//    vpn := io.req.bits.req_info.vpn
+//    gvpn := io.req.bits.req_info.gvpn
+//    hyperInst := io.req.bits.req_info.hyperinst
+//    hlvx := io.req.bits.req_info.hlvx
+//    l1Hit := io.req.bits.l1Hit
+//    accessFault := false.B
+//    idle := false.B
+//    when((io.req.bits.req_info.hyperinst || virt) && hgatp.mode =/= 0.U ){
+//      last_s2xlate := true.B
+//      s_hptw_req := false.B
+//    }.otherwise{
+//      s_pmp_check := false.B
+//    }
+//  }
+//
+//  when(io.hptw.req.fire() && s_hptw_req === false.B){
+//    s_hptw_req := true.B
+//    w_hptw_resp := false.B
+//  }
+//
+//  when(io.hptw.resp.fire() && w_hptw_resp === false.B){
+//    hptw_pageFault := io.hptw.resp.bits.pf
+//    hptw_accessFault := io.hptw.resp.bits.af
+//    w_hptw_resp := true.B
+//    when(onlyS2xlate){
+//      mem_addr_update := true.B
+//      last_s2xlate := false.B
+//    }.otherwise{
+//      s_pmp_check := false.B
+//    }
+//  }
+//
+//  when(io.hptw.req.fire() && s_last_hptw_req === false.B){
+//    w_last_hptw_resp := false.B
+//    s_last_hptw_req := true.B
+//  }
+//
+//  when(io.hptw.resp.fire() && w_last_hptw_resp === false.B){
+//    w_last_hptw_resp := true.B
+//    mem_addr_update := true.B
+//    last_s2xlate := false.B
+//  }
+//
+//  when(sent_to_pmp && mem_addr_update === false.B){
+//    s_mem_req := false.B
+//    s_pmp_check := true.B
+//  }
+//
+//  when(accessFault && idle === false.B){
+//    s_pmp_check := true.B
+//    s_mem_req := true.B
+//    w_mem_resp := true.B
+//    s_llptw_req := true.B
+//    s_hptw_req := true.B
+//    w_hptw_resp := true.B
+//    s_last_hptw_req := true.B
+//    w_last_hptw_resp := true.B
+//    mem_addr_update := true.B
+//    last_s2xlate := false.B
+//  }
+//
+//  when (mem.req.fire()){
+//    s_mem_req := true.B
+//    w_mem_resp := false.B
+//  }
+//
+//  when(mem.resp.fire() && w_mem_resp === false.B){
+//    w_mem_resp := true.B
+//    af_level := af_level + 1.U
+//    s_llptw_req := false.B
+//    mem_addr_update := true.B
+//  }
+//
+//  when(mem_addr_update){
+//    when(level === 0.U && !(find_pte || accessFault)){
+//      level := levelNext
+//      when (s2xlate){
+//        s_hptw_req := false.B
+//      }.otherwise{
+//        s_mem_req := false.B
+//      }
+//      s_llptw_req := true.B
+//      mem_addr_update := false.B
+//    }.elsewhen(io.llptw.valid){
+//      when(io.llptw.fire()) {
+//        idle := true.B
+//        s_llptw_req := true.B
+//        mem_addr_update := false.B
+//      }
+//      finish := true.B
+//    }.elsewhen(find_pte){
+//      when(s2xlate && last_s2xlate === true.B){
+//        s_last_hptw_req := false.B
+//        mem_addr_update := false.B
+//      }.elsewhen(io.resp.valid){
+//        when(io.resp.fire()) {
+//          idle := true.B
+//          s_llptw_req := true.B
+//          mem_addr_update := false.B
+//          accessFault := false.B
+//        }
+//        finish := true.B
+//      }
+//    }
+//  }
+//
+//
+//  when (sfence.valid) {
+//    idle := true.B
+//    s_pmp_check := true.B
+//    s_mem_req := true.B
+//    s_llptw_req := true.B
+//    w_mem_resp := true.B
+//    accessFault := false.B
+//    mem_addr_update := false.B
+//    s_hptw_req := true.B
+//    w_hptw_resp := true.B
+//    s_last_hptw_req := true.B
+//    w_last_hptw_resp := true.B
+//  }
+//
+//
+//  XSDebug(p"[ptw] level:${level} notFound:${pageFault}\n")
+//
+//  // perf
+//  XSPerfAccumulate("fsm_count", io.req.fire())
+//  for (i <- 0 until PtwWidth) {
+//    XSPerfAccumulate(s"fsm_count_source${i}", io.req.fire() && io.req.bits.req_info.source === i.U)
+//  }
+//  XSPerfAccumulate("fsm_busy", !idle)
+//  XSPerfAccumulate("fsm_idle", idle)
+//  XSPerfAccumulate("resp_blocked", io.resp.valid && !io.resp.ready)
+//  XSPerfAccumulate("ptw_ppn_af", io.resp.fire && ppn_af)
+//  XSPerfAccumulate("mem_count", mem.req.fire())
+//  XSPerfAccumulate("mem_cycle", BoolStopWatch(mem.req.fire, mem.resp.fire(), true))
+//  XSPerfAccumulate("mem_blocked", mem.req.valid && !mem.req.ready)
+//
+//  TimeOutAssert(!idle, timeOutThreshold, "page table walker time out")
+//
+//  val perfEvents = Seq(
+//    ("fsm_count         ", io.req.fire()                                     ),
+//    ("fsm_busy          ", !idle                                             ),
+//    ("fsm_idle          ", idle                                              ),
+//    ("resp_blocked      ", io.resp.valid && !io.resp.ready                   ),
+//    ("mem_count         ", mem.req.fire()                                    ),
+//    ("mem_cycle         ", BoolStopWatch(mem.req.fire, mem.resp.fire(), true)),
+//    ("mem_blocked       ", mem.req.valid && !mem.req.ready                   ),
+//  )
+//  generatePerfEvent()
+//}
+
 /*========================= LLPTW ==============================*/
 
 /** LLPTW : Last Level Page Table Walker
@@ -251,8 +929,11 @@ class LLPTWIO(implicit p: Parameters) extends MMUIOBaseBundle with HasPtwConst {
   val in = Flipped(DecoupledIO(new LLPTWInBundle()))
   val out = DecoupledIO(new Bundle {
     val req_info = Output(new L2TlbInnerBundle())
+    val gpaddr = UInt(XLEN.W)
+    val s2xlate = Output(Bool())
     val id = Output(UInt(bMemID.W))
     val af = Output(Bool())
+    val gpf = Output(Bool())
   })
   val mem = new Bundle {
     val req = DecoupledIO(new L2TlbMemReqBundle())
@@ -269,23 +950,44 @@ class LLPTWIO(implicit p: Parameters) extends MMUIOBaseBundle with HasPtwConst {
     val req = Valid(new PMPReqBundle())
     val resp = Flipped(new PMPRespBundle())
   }
+  val hptw = new Bundle {
+    val req = DecoupledIO(new Bundle {
+      val id = UInt(log2Up(l2tlbParams.llptwsize).W)
+      val gpaddr = UInt(XLEN.W)
+      val sfence = new SfenceBundle
+      val hgatp = new TlbSatpBundle
+    })
+    val resp = Flipped(Valid(new Bundle {
+      val hpaddr = Output(UInt(XLEN.W))
+      val id = Output(UInt(log2Up(l2tlbParams.llptwsize).W))
+      val af = Output(Bool())
+      val pf = Output(Bool())
+    }))
+  }
 }
 
 class LLPTWEntry(implicit p: Parameters) extends XSBundle with HasPtwConst {
   val req_info = new L2TlbInnerBundle()
+  val s2xlate = Bool()
   val ppn = UInt(ppnLen.W)
   val wait_id = UInt(log2Up(l2tlbParams.llptwsize).W)
   val af = Bool()
+  val gpf = Bool()
 }
 
 
 @chiselName
 class LLPTW(implicit p: Parameters) extends XSModule with HasPtwConst with HasPerfEvents {
   val io = IO(new LLPTWIO())
-
-  val flush = io.sfence.valid || io.csr.satp.changed
+  val virt = io.csr.priv.virt
+  val hyperInst = io.in.bits.req_info.hyperinst
+  val satp = Mux((virt || hyperInst), io.csr.vsatp, io.csr.satp)
+  val hgatp = io.csr.hgatp
+  val onlyS1xlate = satp.mode =/= 0.U && hgatp.mode === 0.U
+  val s2xlate = (virt || hyperInst) && !onlyS1xlate
+  val flush = io.sfence.valid || satp.changed
   val entries = Reg(Vec(l2tlbParams.llptwsize, new LLPTWEntry()))
-  val state_idle :: state_addr_check :: state_mem_req :: state_mem_waiting :: state_mem_out :: state_cache :: Nil = Enum(6)
+  val state_idle :: state_hptw_req :: state_hptw_resp :: state_addr_check :: state_mem_req :: state_mem_waiting :: state_mem_out :: state_last_hptw_req :: state_last_hptw_resp :: state_cache :: Nil = Enum(10)
   val state = RegInit(VecInit(Seq.fill(l2tlbParams.llptwsize)(state_idle)))
 
   val is_emptys = state.map(_ === state_idle)
@@ -293,6 +995,8 @@ class LLPTW(implicit p: Parameters) extends XSModule with HasPtwConst with HasPe
   val is_waiting = state.map(_ === state_mem_waiting)
   val is_having = state.map(_ === state_mem_out)
   val is_cache = state.map(_ === state_cache)
+  val is_hptw_req = state.map(_ === state_hptw_req)
+  val is_last_hptw_req = state.map(_ === state_last_hptw_req)
 
   val full = !ParallelOR(is_emptys).asBool()
   val enq_ptr = ParallelPriorityEncoder(is_emptys)
@@ -304,15 +1008,25 @@ class LLPTW(implicit p: Parameters) extends XSModule with HasPtwConst with HasPe
     mem_arb.io.in(i).valid := is_mems(i) && !io.mem.req_mask(i)
   }
 
+  val hyper_arb1 = Module(new RRArbiter(new LLPTWEntry(), l2tlbParams.llptwsize))
+  for (i <- 0 until l2tlbParams.llptwsize) {
+    hyper_arb1.io.in(i).bits := entries(i)
+    hyper_arb1.io.in(i).valid := is_hptw_req(i)
+  }
+  val hyper_arb2 = Module(new RRArbiter(new LLPTWEntry(), l2tlbParams.llptwsize))
+  for (i <- 0 until l2tlbParams.llptwsize) {
+    hyper_arb2.io.in(i).bits := entries(i)
+    hyper_arb2.io.in(i).valid := is_last_hptw_req(i)
+  }
   val cache_ptr = ParallelMux(is_cache, (0 until l2tlbParams.llptwsize).map(_.U(log2Up(l2tlbParams.llptwsize).W)))
 
   // duplicate req
   // to_wait: wait for the last to access mem, set to mem_resp
   // to_cache: the last is back just right now, set to mem_cache
   val dup_vec = state.indices.map(i =>
-    dup(io.in.bits.req_info.vpn, entries(i).req_info.vpn)
+    dup(io.in.bits.req_info.vpn, entries(i).req_info.vpn) && !entries(i).s2xlate
   )
-  val dup_req_fire = mem_arb.io.out.fire() && dup(io.in.bits.req_info.vpn, mem_arb.io.out.bits.req_info.vpn) // dup with the req fire entry
+  val dup_req_fire = !mem_arb.io.out.bits.s2xlate && mem_arb.io.out.fire() && dup(io.in.bits.req_info.vpn, mem_arb.io.out.bits.req_info.vpn) // dup with the req fire entry
   val dup_vec_wait = dup_vec.zip(is_waiting).map{case (d, w) => d && w} // dup with "mem_waiting" entres, sending mem req already
   val dup_vec_having = dup_vec.zipWithIndex.map{case (d, i) => d && is_having(i)} // dup with the "mem_out" entry recv the data just now
   val wait_id = Mux(dup_req_fire, mem_arb.io.chosen, ParallelMux(dup_vec_wait zip entries.map(_.wait_id)))
@@ -332,20 +1046,58 @@ class LLPTW(implicit p: Parameters) extends XSModule with HasPtwConst with HasPe
     // if prefetch req does not need mem access, just give it up.
     // so there will be at most 1 + FilterSize entries that needs re-access page cache
     // so 2 + FilterSize is enough to avoid dead-lock
-    state(enq_ptr) := enq_state
+    state(enq_ptr) := Mux(s2xlate, state_hptw_req, enq_state)
     entries(enq_ptr).req_info := io.in.bits.req_info
     entries(enq_ptr).ppn := io.in.bits.ppn
     entries(enq_ptr).wait_id := Mux(to_wait, wait_id, enq_ptr)
     entries(enq_ptr).af := false.B
+    entries(enq_ptr).gpf := false.B
+    entries(enq_ptr).s2xlate := s2xlate
     mem_resp_hit(enq_ptr) := to_mem_out
   }
 
+  val gpaddr = Reg(UInt(GPAddrBits.W))
+  when (hyper_arb1.io.out.fire()) {
+    for (i <- state.indices) {
+      when (state(i) === state_hptw_req && entries(i).ppn === hyper_arb1.io.out.bits.ppn){
+        state(i) := state_hptw_resp
+        gpaddr := MakeAddr(hyper_arb1.io.out.bits.ppn, getVpnn(hyper_arb1.io.out.bits.req_info.vpn, 0))
+        entries(i).wait_id := hyper_arb1.io.chosen
+      }
+    }
+  }
+  when(hyper_arb2.io.out.fire()) {
+    for (i <- state.indices) {
+      when(state(i) === state_last_hptw_req && entries(i).ppn === hyper_arb2.io.out.bits.ppn) {
+        state(i) := state_last_hptw_resp
+        gpaddr := MakeAddr(hyper_arb2.io.out.bits.ppn, getVpnn(hyper_arb2.io.out.bits.req_info.vpn, 0))
+        entries(i).wait_id := hyper_arb2.io.chosen
+      }
+    }
+  }
+  when (io.hptw.resp.fire()){
+    for (i <- state.indices){
+      when (state(i) === state_hptw_resp && io.hptw.resp.bits.id === entries(i).wait_id){
+        state(i) := Mux(io.hptw.resp.bits.af || io.hptw.resp.bits.pf, state_mem_out, state_addr_check)
+        entries(i).af := io.hptw.resp.bits.af
+        entries(i).gpf := io.hptw.resp.bits.pf
+      }
+      when(state(i) === state_last_hptw_resp && io.hptw.resp.bits.id === entries(i).wait_id) {
+        state(i) := state_mem_out
+        entries(i).af := io.hptw.resp.bits.af
+        entries(i).gpf := io.hptw.resp.bits.pf
+      }
+    }
+  }
+
   val enq_ptr_reg = RegNext(enq_ptr)
-  val need_addr_check = RegNext(enq_state === state_addr_check && io.in.fire() && !flush)
+  val need_addr_check = RegNext(enq_state === state_addr_check && io.in.fire() && !s2xlate && !flush)
   val last_enq_vpn = RegEnable(io.in.bits.req_info.vpn, io.in.fire())
-
+  val hptw_need_addr_check = RegNext(io.hptw.resp.fire())
   io.pmp.req.valid := need_addr_check
-  io.pmp.req.bits.addr := RegEnable(MakeAddr(io.in.bits.ppn, getVpnn(io.in.bits.req_info.vpn, 0)), io.in.fire())
+  val in_paddr = RegEnable(MakeAddr(io.in.bits.ppn, getVpnn(io.in.bits.req_info.vpn, 0)), io.in.fire())
+  val hptw_paddr = RegEnable(io.hptw.resp.bits.hpaddr, io.hptw.resp.fire())
+  io.pmp.req.bits.addr := Mux(hptw_need_addr_check, hptw_paddr, in_paddr)
   io.pmp.req.bits.cmd := TlbCmd.read
   io.pmp.req.bits.size := 3.U // TODO: fix it
   val pmp_resp_valid = io.pmp.req.valid // same cycle
@@ -369,11 +1121,13 @@ class LLPTW(implicit p: Parameters) extends XSModule with HasPtwConst with HasPe
   when (io.mem.resp.fire()) {
     state.indices.map{i =>
       when (state(i) === state_mem_waiting && io.mem.resp.bits.id === entries(i).wait_id) {
-        state(i) := state_mem_out
+        state(i) := Mux(s2xlate, state_last_hptw_req, state_mem_out)
         mem_resp_hit(i) := true.B
       }
     }
   }
+
+
   when (io.out.fire()) {
     assert(state(mem_ptr) === state_mem_out)
     state(mem_ptr) := state_idle
@@ -389,12 +1143,17 @@ class LLPTW(implicit p: Parameters) extends XSModule with HasPtwConst with HasPe
     state.map(_ := state_idle)
   }
 
-  io.in.ready := !full
+  // when hptw resp fire, we need pmp to check the paddr from hptw, and at this time
+  // we should avoid io.in.fire to use pmp
+  io.in.ready := !full || !io.hptw.resp.fire()
 
   io.out.valid := ParallelOR(is_having).asBool()
   io.out.bits.req_info := entries(mem_ptr).req_info
   io.out.bits.id := mem_ptr
+  io.out.bits.gpaddr := gpaddr
   io.out.bits.af := entries(mem_ptr).af
+  io.out.bits.gpf := entries(mem_ptr).gpf
+  io.out.bits.s2xlate := entries(mem_ptr).s2xlate
 
   io.mem.req.valid := mem_arb.io.out.valid && !flush
   io.mem.req.bits.addr := MakeAddr(mem_arb.io.out.bits.ppn, getVpnn(mem_arb.io.out.bits.req_info.vpn, 0))
@@ -407,6 +1166,16 @@ class LLPTW(implicit p: Parameters) extends XSModule with HasPtwConst with HasPe
   io.cache.valid := Cat(is_cache).orR
   io.cache.bits := ParallelMux(is_cache, entries.map(_.req_info))
 
+
+  io.hptw.req.valid := (hyper_arb1.io.out.valid || hyper_arb1.io.out.valid) && !flush
+  hyper_arb1.io.out.ready := io.hptw.req.ready
+  hyper_arb2.io.out.ready := io.hptw.req.ready
+  io.hptw.req.bits.gpaddr := Mux(hyper_arb1.io.out.valid,
+    MakeAddr(hyper_arb1.io.out.bits.ppn, getVpnn(hyper_arb1.io.out.bits.req_info.vpn, 0)),
+    MakeAddr(hyper_arb2.io.out.bits.ppn, getVpnn(hyper_arb2.io.out.bits.req_info.vpn, 0)))
+  io.hptw.req.bits.id := Mux(hyper_arb1.io.out.valid, hyper_arb1.io.chosen, hyper_arb2.io.chosen)
+  io.hptw.req.bits.sfence := io.sfence
+  io.hptw.req.bits.hgatp := io.csr.hgatp
   XSPerfAccumulate("llptw_in_count", io.in.fire())
   XSPerfAccumulate("llptw_in_block", io.in.valid && !io.in.ready)
   for (i <- 0 until 7) {
diff --git a/src/main/scala/xiangshan/cache/mmu/Repeater.scala b/src/main/scala/xiangshan/cache/mmu/Repeater.scala
index 0f40ca402..84ad96c8d 100644
--- a/src/main/scala/xiangshan/cache/mmu/Repeater.scala
+++ b/src/main/scala/xiangshan/cache/mmu/Repeater.scala
@@ -55,7 +55,7 @@ class PTWRepeater(Width: Int = 1, FenceDelay: Int)(implicit p: Parameters) exten
     arb.io.in <> io.tlb.req
     arb.io.out
   }
-  val (tlb, ptw, flush) = (io.tlb, io.ptw, DelayN(io.sfence.valid || io.csr.satp.changed, FenceDelay))
+  val (tlb, ptw, flush) = (io.tlb, io.ptw, DelayN(io.sfence.valid || io.csr.satp.changed || (io.csr.priv.virt && io.csr.vsatp.changed), FenceDelay))
   val req = RegEnable(req_in.bits, req_in.fire())
   val resp = RegEnable(ptw.resp.bits, ptw.resp.fire())
   val haveOne = BoolStopWatch(req_in.fire(), tlb.resp.fire() || flush)
@@ -98,7 +98,7 @@ class PTWRepeaterNB(Width: Int = 1, passReady: Boolean = false, FenceDelay: Int)
     arb.io.in <> io.tlb.req
     arb.io.out
   }
-  val (tlb, ptw, flush) = (io.tlb, io.ptw, DelayN(io.sfence.valid || io.csr.satp.changed, FenceDelay))
+  val (tlb, ptw, flush) = (io.tlb, io.ptw, DelayN(io.sfence.valid || io.csr.satp.changed || (io.csr.priv.virt && io.csr.vsatp.changed), FenceDelay))
   /* sent: tlb -> repeater -> ptw
    * recv: ptw -> repeater -> tlb
    * different from PTWRepeater
@@ -157,6 +157,11 @@ class PTWFilter(Width: Int, Size: Int, FenceDelay: Int)(implicit p: Parameters)
   val v = RegInit(VecInit(Seq.fill(Size)(false.B)))
   val ports = Reg(Vec(Size, Vec(Width, Bool()))) // record which port(s) the entry come from, may not able to cover all the ports
   val vpn = Reg(Vec(Size, UInt(vpnLen.W)))
+  val gvpn = Reg(Vec(Size, UInt(gvpnLen.W)))
+  val cmd = Reg(Vec(Size, TlbCmd()))
+  val hyperinst = Reg(Vec(Size, Bool()))
+  val hlvx = Reg(Vec(Size, Bool()))
+  val virt = Reg(Vec(Size, Bool()))
   val memidx = Reg(Vec(Size, new MemBlockidxBundle))
   val enqPtr = RegInit(0.U(log2Up(Size).W)) // Enq
   val issPtr = RegInit(0.U(log2Up(Size).W)) // Iss to Ptw
@@ -165,7 +170,7 @@ class PTWFilter(Width: Int, Size: Int, FenceDelay: Int)(implicit p: Parameters)
   val mayFullIss = RegInit(false.B)
   val counter = RegInit(0.U(log2Up(Size+1).W))
 
-  val flush = DelayN(io.sfence.valid || io.csr.satp.changed, FenceDelay)
+  val flush = DelayN(io.sfence.valid || io.csr.satp.changed || (io.csr.priv.virt && io.csr.vsatp.changed), FenceDelay)
   val tlb_req = WireInit(io.tlb.req) // NOTE: tlb_req is not io.tlb.req, see below codes, just use cloneType
   tlb_req.suggestName("tlb_req")
 
@@ -177,8 +182,8 @@ class PTWFilter(Width: Int, Size: Int, FenceDelay: Int)(implicit p: Parameters)
 
   val canEnqueue = Wire(Bool()) // NOTE: actually enqueue
   val ptwResp = RegEnable(io.ptw.resp.bits, io.ptw.resp.fire())
-  val ptwResp_OldMatchVec = vpn.zip(v).map{ case (pi, vi) =>
-    vi && io.ptw.resp.bits.entry.hit(pi, io.csr.satp.asid, true, true)}
+  val ptwResp_OldMatchVec = vpn.zip(v).zip(virt).zip(hyperinst).map{ case (((vpn, v), virt), hyperinst) =>
+    v && io.ptw.resp.bits.entry.hit(vpn, io.csr.satp.asid, true, true, virt || hyperinst)}
   val ptwResp_valid = RegNext(io.ptw.resp.fire() && Cat(ptwResp_OldMatchVec).orR, init = false.B)
   val oldMatchVec_early = io.tlb.req.map(a => vpn.zip(v).map{ case (pi, vi) => vi && pi === a.bits.vpn})
   val lastReqMatchVec_early = io.tlb.req.map(a => tlb_req.map{ b => b.valid && b.bits.vpn === a.bits.vpn && canEnqueue})
@@ -186,7 +191,7 @@ class PTWFilter(Width: Int, Size: Int, FenceDelay: Int)(implicit p: Parameters)
 
   (0 until Width) foreach { i =>
     tlb_req(i).valid := RegNext(io.tlb.req(i).valid &&
-      !(ptwResp_valid && ptwResp.entry.hit(io.tlb.req(i).bits.vpn, 0.U, true, true)) &&
+      !(ptwResp_valid && ptwResp.entry.hit(io.tlb.req(i).bits.vpn, 0.U, true, true, io.tlb.req(i).bits.virt || io.tlb.req(i).bits.hyperinst)) &&
       !Cat(lastReqMatchVec_early(i)).orR,
       init = false.B)
     tlb_req(i).bits := RegEnable(io.tlb.req(i).bits, io.tlb.req(i).valid)
@@ -197,7 +202,7 @@ class PTWFilter(Width: Int, Size: Int, FenceDelay: Int)(implicit p: Parameters)
     RegNext(newMatchVec_early(i)(j)) && tlb_req(j).valid
   ))
   val ptwResp_newMatchVec = tlb_req.map(a =>
-    ptwResp_valid && ptwResp.entry.hit(a.bits.vpn, 0.U, allType = true, true))
+    ptwResp_valid && ptwResp.entry.hit(a.bits.vpn, 0.U, allType = true, true, a.bits.virt || a.bits.hyperinst))
 
   val oldMatchVec2 = (0 until Width).map(i => oldMatchVec_early(i).map(RegNext(_)).map(_ & tlb_req(i).valid))
   val update_ports = v.indices.map(i => oldMatchVec2.map(j => j(i)))
@@ -239,20 +244,26 @@ class PTWFilter(Width: Int, Size: Int, FenceDelay: Int)(implicit p: Parameters)
 
   // tlb req flushed by ptw resp: last ptw resp && current ptw resp
   // the flushed tlb req will fakely enq, with a false valid
-  val tlb_req_flushed = reqs.map(a => io.ptw.resp.valid && io.ptw.resp.bits.entry.hit(a.bits.vpn, 0.U, true, true))
+  val tlb_req_flushed = reqs.map(a => io.ptw.resp.valid && io.ptw.resp.bits.entry.hit(a.bits.vpn, 0.U, true, true, a.bits.virt || a.bits.hyperinst))
 
   io.tlb.resp.valid := ptwResp_valid
   io.tlb.resp.bits.data.entry := ptwResp.entry
   io.tlb.resp.bits.data.pf := ptwResp.pf
   io.tlb.resp.bits.data.af := ptwResp.af
+  io.tlb.resp.bits.data.gpf := ptwResp.gpf
   io.tlb.resp.bits.data.memidx := memidx(OHToUInt(ptwResp_OldMatchVec))
   io.tlb.resp.bits.vector := resp_vector
 
   val issue_valid = v(issPtr) && !isEmptyIss && !inflight_full
-  val issue_filtered = ptwResp_valid && ptwResp.entry.hit(io.ptw.req(0).bits.vpn, io.csr.satp.asid, allType=true, ignoreAsid=true)
+  val issue_filtered = ptwResp_valid && ptwResp.entry.hit(io.ptw.req(0).bits.vpn, io.csr.satp.asid, allType=true, ignoreAsid=true, io.ptw.req(0).bits.virt || io.ptw.req(0).bits.hyperinst)
   val issue_fire_fake = issue_valid && (io.ptw.req(0).ready || (issue_filtered && false.B /*timing-opt*/))
   io.ptw.req(0).valid := issue_valid && !issue_filtered
   io.ptw.req(0).bits.vpn := vpn(issPtr)
+  io.ptw.req(0).bits.gvpn := gvpn(issPtr)
+  io.ptw.req(0).bits.virt := virt(issPtr)
+  io.ptw.req(0).bits.hlvx := hlvx(issPtr)
+  io.ptw.req(0).bits.hyperinst := hyperinst(issPtr)
+  io.ptw.req(0).bits.cmd := cmd(issPtr)
   io.ptw.resp.ready := true.B
 
   reqs.zipWithIndex.map{
@@ -260,6 +271,11 @@ class PTWFilter(Width: Int, Size: Int, FenceDelay: Int)(implicit p: Parameters)
       when (req.valid && canEnqueue) {
         v(enqPtrVec(i)) := !tlb_req_flushed(i)
         vpn(enqPtrVec(i)) := req.bits.vpn
+        gvpn(enqPtrVec(i)) := req.bits.gvpn
+        hlvx(enqPtrVec(i)) := req.bits.hlvx
+        hyperinst(enqPtrVec(i)) := req.bits.hyperinst
+        cmd(enqPtrVec(i)) := req.bits.cmd
+        virt(enqPtrVec(i)) := req.bits.virt
         memidx(enqPtrVec(i)) := req.bits.memidx
         ports(enqPtrVec(i)) := req_ports(i).asBools
       }
diff --git a/src/main/scala/xiangshan/cache/mmu/TLB.scala b/src/main/scala/xiangshan/cache/mmu/TLB.scala
index 48c988da3..12f3092c6 100644
--- a/src/main/scala/xiangshan/cache/mmu/TLB.scala
+++ b/src/main/scala/xiangshan/cache/mmu/TLB.scala
@@ -62,33 +62,40 @@ class TLB(Width: Int, nRespDups: Int = 1, Block: Seq[Boolean], q: TLBParameters)
   val sfence = DelayN(io.sfence, q.fenceDelay)
   val csr = io.csr
   val satp = DelayN(io.csr.satp, q.fenceDelay)
-  val flush_mmu = DelayN(sfence.valid || csr.satp.changed, q.fenceDelay)
-  val mmu_flush_pipe = DelayN(sfence.valid && sfence.bits.flushPipe, q.fenceDelay) // for svinval, won't flush pipe
+  val vsatp = DelayN(io.csr.vsatp, q.fenceDelay)
+  val hgatp = DelayN(io.csr.hgatp, q.fenceDelay)
   val flush_pipe = io.flushPipe
+  val isHyperInst = (0 until Width).map(i =>ValidHold(req(i).fire && !req(i).bits.kill && req(i).bits.hyperinst, resp(i).fire, flush_pipe(i)))
+  val onlyS2xlate = vsatp.mode === 0.U && hgatp.mode === 8.U
+  val flush_mmu = DelayN(sfence.valid || csr.satp.changed || csr.vsatp.changed || csr.hgatp.changed, q.fenceDelay)
+  val mmu_flush_pipe = DelayN((sfence.valid && sfence.bits.flushPipe), q.fenceDelay) // for svinval, won't flush pipe
 
   // ATTENTION: csr and flush from backend are delayed. csr should not be later than flush.
   // because, csr will influence tlb behavior.
   val ifecth = if (q.fetchi) true.B else false.B
-  val mode = if (q.useDmode) csr.priv.dmode else csr.priv.imode
+  val mode_tmp = if (q.useDmode) csr.priv.dmode else csr.priv.imode
+  val mode = (0 until Width).map(i => Mux(isHyperInst(i), csr.priv.spvp, mode_tmp))
+  val virt = csr.priv.virt
+  val sum = (0 until Width).map(i => Mux(virt || isHyperInst(i), io.csr.priv.vsum, io.csr.priv.sum))
+  val mxr = (0 until Width).map(i => Mux(virt || isHyperInst(i), io.csr.priv.vmxr || io.csr.priv.mxr, io.csr.priv.mxr))
   // val vmEnable = satp.mode === 8.U // && (mode < ModeM) // FIXME: fix me when boot xv6/linux...
-  val vmEnable = if (EnbaleTlbDebug) (satp.mode === 8.U)
-    else (satp.mode === 8.U && (mode < ModeM))
-  val portTranslateEnable = (0 until Width).map(i => vmEnable && !req(i).bits.no_translate)
+  val vmEnable = (0 until Width).map(i => if (EnbaleTlbDebug) (satp.mode === 8.U)
+    else (satp.mode === 8.U) && (mode(i) < ModeM))
+  val s2xlateEnable = (0 until Width).map(i => (isHyperInst(i) || virt) && (vsatp.mode === 8.U || hgatp.mode === 8.U) && (mode(i) < ModeM))
+  val portTranslateEnable = (0 until Width).map(i => (vmEnable(i) || s2xlateEnable(i)) && !req(i).bits.no_translate)
 
   val req_in = req
   val req_out = req.map(a => RegEnable(a.bits, a.fire()))
   val req_out_v = (0 until Width).map(i => ValidHold(req_in(i).fire && !req_in(i).bits.kill, resp(i).fire, flush_pipe(i)))
 
-  val refill = ptw.resp.fire() && !flush_mmu && vmEnable
-  refill_to_mem.valid := refill
-  refill_to_mem.memidx := ptw.resp.bits.memidx
-
+  val refill = (0 until Width).map(i => ptw.resp.fire() && !flush_mmu && (vmEnable(i) || ptw.resp.bits.entry.s2xlate))
+  io.refill_to_mem := DontCare
   val entries = Module(new TlbStorageWrapper(Width, q, nRespDups))
-  entries.io.base_connect(sfence, csr, satp)
+  entries.io.base_connect(sfence, csr, satp, vsatp, hgatp)
   if (q.outReplace) { io.replace <> entries.io.replace }
   for (i <- 0 until Width) {
-    entries.io.r_req_apply(io.requestor(i).req.valid, get_pn(req_in(i).bits.vaddr), i)
-    entries.io.w_apply(refill, ptw.resp.bits, io.ptw_replenish)
+    entries.io.r_req_apply(io.requestor(i).req.valid, get_pn(req_in(i).bits.vaddr), i, virt || req_in(i).bits.hyperinst)
+    entries.io.w_apply(refill(i), ptw.resp.bits, io.ptw_replenish)
     resp(i).bits.debug.isFirstIssue := RegNext(req(i).bits.debug.isFirstIssue)
     resp(i).bits.debug.robIdx := RegNext(req(i).bits.debug.robIdx)
   }
@@ -107,7 +114,7 @@ class TLB(Width: Int, nRespDups: Int = 1, Block: Seq[Boolean], q: TLBParameters)
   (0 until Width).foreach{i =>
     pmp_check(pmp_addr(i), req_out(i).size, req_out(i).cmd, i)
     for (d <- 0 until nRespDups) {
-      perm_check(perm(i)(d), req_out(i).cmd, static_pm(i), static_pm_v(i), i, d)
+      perm_check(perm(i)(d), req_out(i).cmd, static_pm(i), static_pm_v(i), i, d, req_out(i).hlvx, mode(i), mxr(i), sum(i), virt || req_out(i).hyperinst)
     }
   }
 
@@ -124,7 +131,8 @@ class TLB(Width: Int, nRespDups: Int = 1, Block: Seq[Boolean], q: TLBParameters)
   /************************  main body above | method/log/perf below ****************************/
   def TLBRead(i: Int) = {
     val (e_hit, e_ppn, e_perm, e_super_hit, e_super_ppn, static_pm) = entries.io.r_resp_apply(i)
-    val (p_hit, p_ppn, p_perm) = ptw_resp_bypass(get_pn(req_in(i).bits.vaddr))
+    val (e_gvpn, e_super_gvpn) = entries.io.r_resp_gvpn_apply(i)
+    val (p_hit, p_ppn, p_perm, p_gvpn) = ptw_resp_bypass(get_pn(req_in(i).bits.vaddr), virt || req_in(i).bits.hyperinst)
     val enable = portTranslateEnable(i)
 
     val hit = e_hit || p_hit
@@ -140,14 +148,18 @@ class TLB(Width: Int, nRespDups: Int = 1, Block: Seq[Boolean], q: TLBParameters)
     resp(i).bits.memidx := RegNext(req_in(i).bits.memidx)
 
     val ppn = WireInit(VecInit(Seq.fill(nRespDups)(0.U(ppnLen.W))))
+    val gvpn = WireInit(VecInit(Seq.fill(nRespDups)(0.U(gvpnLen.W))))
     val perm = WireInit(VecInit(Seq.fill(nRespDups)(0.U.asTypeOf(new TlbPermBundle))))
 
     for (d <- 0 until nRespDups) {
       ppn(d) := Mux(p_hit, p_ppn, e_ppn(d))
+      gvpn(d) := Mux(p_hit, p_gvpn, e_gvpn(d))
       perm(d) := Mux(p_hit, p_perm, e_perm(d))
 
       val paddr = Cat(ppn(d), get_off(req_out(i).vaddr))
+      val gpaddr = Cat(gvpn(d), get_off(req_out(i).vaddr))
       resp(i).bits.paddr(d) := Mux(enable, paddr, vaddr)
+      resp(i).bits.gpaddr(d) := gpaddr
     }
 
     XSDebug(req_out_v(i), p"(${i.U}) hit:${hit} miss:${miss} ppn:${Hexadecimal(ppn(0))} perm:${perm(0)}\n")
@@ -167,26 +179,33 @@ class TLB(Width: Int, nRespDups: Int = 1, Block: Seq[Boolean], q: TLBParameters)
     pmp(idx).bits.cmd := cmd
   }
 
-  def perm_check(perm: TlbPermBundle, cmd: UInt, spm: TlbPMBundle, spm_v: Bool, idx: Int, nDups: Int) = {
+  def perm_check(perm: TlbPermBundle, cmd: UInt, spm: TlbPMBundle, spm_v: Bool, idx: Int, nDups: Int, hlvx: Bool, mode: UInt, mxr: Bool, sum: Bool, s2xlate: Bool) = {
     // for timing optimization, pmp check is divided into dynamic and static
     // dynamic: superpage (or full-connected reg entries) -> check pmp when translation done
     // static: 4K pages (or sram entries) -> check pmp with pre-checked results
     val af = perm.af
     val pf = perm.pf
+    val gpf = perm.gpf
     val ldUpdate = !perm.a && TlbCmd.isRead(cmd) && !TlbCmd.isAmo(cmd) // update A/D through exception
     val stUpdate = (!perm.a || !perm.d) && (TlbCmd.isWrite(cmd) || TlbCmd.isAmo(cmd)) // update A/D through exception
     val instrUpdate = !perm.a && TlbCmd.isExec(cmd) // update A/D through exception
-    val modeCheck = !(mode === ModeU && !perm.u || mode === ModeS && perm.u && (!io.csr.priv.sum || ifecth))
-    val ldPermFail = !(modeCheck && (perm.r || io.csr.priv.mxr && perm.x))
+    val modeCheck = !(mode === ModeU && !perm.u || mode === ModeS && perm.u && (!sum || ifecth))
+    val ldPermFail = !(modeCheck && Mux(hlvx, perm.x, perm.r || mxr && perm.x))
     val stPermFail = !(modeCheck && perm.w)
     val instrPermFail = !(modeCheck && perm.x)
-    val ldPf = (ldPermFail || pf) && (TlbCmd.isRead(cmd) && !TlbCmd.isAmo(cmd))
-    val stPf = (stPermFail || pf) && (TlbCmd.isWrite(cmd) || TlbCmd.isAmo(cmd))
-    val instrPf = (instrPermFail || pf) && TlbCmd.isExec(cmd)
+    val ldPf = (Mux(onlyS2xlate && s2xlate, false.B, ldPermFail && perm.v) || pf) && (TlbCmd.isRead(cmd) && !TlbCmd.isAmo(cmd))
+    val stPf = (Mux(onlyS2xlate && s2xlate, false.B, stPermFail && perm.v) || pf) && (TlbCmd.isWrite(cmd) || TlbCmd.isAmo(cmd))
+    val instrPf = (Mux(onlyS2xlate && s2xlate, false.B, instrPermFail && perm.v) || pf) && TlbCmd.isExec(cmd)
+    val ldGPf = gpf && (TlbCmd.isRead(cmd) && !TlbCmd.isAmo(cmd)) && !ldPf
+    val stGPf = gpf && (TlbCmd.isWrite(cmd) || TlbCmd.isAmo(cmd)) && !stPf
+    val instrGPf = gpf && TlbCmd.isExec(cmd) && !instrPf // for cmd write the VSR_GUR page
     val fault_valid = portTranslateEnable(idx)
-    resp(idx).bits.excp(nDups).pf.ld := (ldPf || ldUpdate) && fault_valid && !af
-    resp(idx).bits.excp(nDups).pf.st := (stPf || stUpdate) && fault_valid && !af
-    resp(idx).bits.excp(nDups).pf.instr := (instrPf || instrUpdate) && fault_valid && !af
+    resp(idx).bits.excp(nDups).pf.ld := (ldPf || Mux(onlyS2xlate && s2xlate, false.B, ldUpdate)) && fault_valid && !af
+    resp(idx).bits.excp(nDups).pf.st := (stPf || Mux(onlyS2xlate && s2xlate, false.B, stUpdate)) && fault_valid && !af
+    resp(idx).bits.excp(nDups).pf.instr := (instrPf || Mux(onlyS2xlate && s2xlate, false.B, instrUpdate)) && fault_valid && !af
+    resp(idx).bits.excp(nDups).pf.ldG := ldGPf && fault_valid && !af
+    resp(idx).bits.excp(nDups).pf.stG := stGPf && fault_valid && !af
+    resp(idx).bits.excp(nDups).pf.instrG := instrGPf && fault_valid && !af
     // NOTE: pf need && with !af, page fault has higher priority than access fault
     // but ptw may also have access fault, then af happens, the translation is wrong.
     // In this case, pf has lower priority than af
@@ -194,6 +213,9 @@ class TLB(Width: Int, nRespDups: Int = 1, Block: Seq[Boolean], q: TLBParameters)
     resp(idx).bits.excp(nDups).af.ld    := (af || (spm_v && !spm.r)) && TlbCmd.isRead(cmd) && fault_valid
     resp(idx).bits.excp(nDups).af.st    := (af || (spm_v && !spm.w)) && TlbCmd.isWrite(cmd) && fault_valid
     resp(idx).bits.excp(nDups).af.instr := (af || (spm_v && !spm.x)) && TlbCmd.isExec(cmd) && fault_valid
+    resp(idx).bits.excp(nDups).af.ldG   := DontCare
+    resp(idx).bits.excp(nDups).af.stG   := DontCare
+    resp(idx).bits.excp(nDups).af.instrG:= DontCare
     resp(idx).bits.static_pm.valid := spm_v && fault_valid // ls/st unit should use this mmio, not the result from pmp
     resp(idx).bits.static_pm.bits := !spm.c
   }
@@ -203,13 +225,18 @@ class TLB(Width: Int, nRespDups: Int = 1, Block: Seq[Boolean], q: TLBParameters)
     io.requestor(idx).req.ready := io.requestor(idx).resp.ready // should always be true
     XSError(!io.requestor(idx).resp.ready, s"${q.name} port ${idx} is non-block, resp.ready must be true.B")
 
-    val ptw_just_back = ptw.resp.fire && ptw.resp.bits.entry.hit(get_pn(req_out(idx).vaddr), asid = io.csr.satp.asid, allType = true)
+    val ptw_just_back = ptw.resp.fire && ptw.resp.bits.entry.hit(get_pn(req_out(idx).vaddr), asid = io.csr.satp.asid, allType = true, s2xlate = virt || req_out(idx).hyperinst)
     io.ptw.req(idx).valid :=  RegNext(req_out_v(idx) && missVec(idx) && !ptw_just_back, false.B) // TODO: remove the regnext, timing
     when (RegEnable(io.requestor(idx).req_kill, RegNext(io.requestor(idx).req.fire))) {
       io.ptw.req(idx).valid := false.B
     }
     io.ptw.req(idx).bits.vpn := RegNext(get_pn(req_out(idx).vaddr))
+    io.ptw.req(idx).bits.gvpn := RegNext(get_pn(req_out(idx).vaddr))
+    io.ptw.req(idx).bits.cmd := RegNext(req_out(idx).cmd)
     io.ptw.req(idx).bits.memidx := RegNext(req_out(idx).memidx)
+    io.ptw.req(idx).bits.hyperinst := RegNext(req_out(idx).hyperinst)
+    io.ptw.req(idx).bits.hlvx := RegNext(req_out(idx).hlvx)
+    io.ptw.req(idx).bits.virt := RegNext(virt)
   }
 
   def handle_block(idx: Int): Unit = {
@@ -219,8 +246,9 @@ class TLB(Width: Int, nRespDups: Int = 1, Block: Seq[Boolean], q: TLBParameters)
 
     // miss request entries
     val miss_req_vpn = get_pn(req_out(idx).vaddr)
+    val miss_req_gvpn = get_pn(req_out(idx).vaddr)
     val miss_req_memidx = req_out(idx).memidx
-    val hit = io.ptw.resp.bits.entry.hit(miss_req_vpn, io.csr.satp.asid, allType = true) && io.ptw.resp.valid
+    val hit = io.ptw.resp.bits.entry.hit(miss_req_vpn, io.csr.satp.asid, allType = true, s2xlate = virt || req_out(idx).hyperinst) && io.ptw.resp.valid
 
     val new_coming = RegNext(req_in(idx).fire && !req_in(idx).bits.kill && !flush_pipe(idx), false.B)
     val miss_wire = new_coming && missVec(idx)
@@ -236,7 +264,8 @@ class TLB(Width: Int, nRespDups: Int = 1, Block: Seq[Boolean], q: TLBParameters)
       resp(idx).bits.miss := false.B // for blocked tlb, this is useless
       for (d <- 0 until nRespDups) {
         resp(idx).bits.paddr(d) := Cat(pte.entry.genPPN(get_pn(req_out(idx).vaddr)), get_off(req_out(idx).vaddr))
-        perm_check(pte, req_out(idx).cmd, 0.U.asTypeOf(new TlbPMBundle), false.B, idx, d)
+        resp(idx).bits.gpaddr(d) := Cat(pte.entry.genGVPN(get_pn(req_out(idx).vaddr)), get_off(req_out(idx).vaddr))
+        perm_check(pte, req_out(idx).cmd, 0.U.asTypeOf(new TlbPMBundle), false.B, idx, d, req_out(idx).hlvx, mode(idx), mxr(idx), sum(idx), virt || req_out(idx).hyperinst)
       }
       pmp_check(resp(idx).bits.paddr(0), req_out(idx).size, req_out(idx).cmd, idx)
 
@@ -248,8 +277,12 @@ class TLB(Width: Int, nRespDups: Int = 1, Block: Seq[Boolean], q: TLBParameters)
     val ptw_req = io.ptw.req(idx)
     ptw_req.valid := miss_req_v
     ptw_req.bits.vpn := miss_req_vpn
+    ptw_req.bits.gvpn := miss_req_gvpn
     ptw_req.bits.memidx := miss_req_memidx
-
+    ptw_req.bits.hyperinst := req_out(idx).hyperinst
+    ptw_req.bits.cmd := req_out(idx).cmd
+    ptw_req.bits.hlvx := req_out(idx).hlvx
+    ptw_req.bits.virt := virt
     // NOTE: when flush pipe, tlb should abandon last req
     // however, some outside modules like icache, dont care flushPipe, and still waiting for tlb resp
     // just resp valid and raise page fault to go through. The pipe(ifu) will abandon it.
@@ -260,6 +293,9 @@ class TLB(Width: Int, nRespDups: Int = 1, Block: Seq[Boolean], q: TLBParameters)
           resp(idx).bits.excp(d).pf.ld := true.B // sfence happened, pf for not to use this addr
           resp(idx).bits.excp(d).pf.st := true.B
           resp(idx).bits.excp(d).pf.instr := true.B
+          resp(idx).bits.excp(d).pf.ldG := true.B // sfence happened, pf for not to use this addr
+          resp(idx).bits.excp(d).pf.stG := true.B
+          resp(idx).bits.excp(d).pf.instrG := true.B
         }
       }
     }
@@ -267,11 +303,12 @@ class TLB(Width: Int, nRespDups: Int = 1, Block: Seq[Boolean], q: TLBParameters)
 
   // when ptw resp, tlb at refill_idx maybe set to miss by force.
   // Bypass ptw resp to check.
-  def ptw_resp_bypass(vpn: UInt) = {
-    val p_hit = RegNext(ptw.resp.bits.entry.hit(vpn, io.csr.satp.asid, allType = true) && io.ptw.resp.fire)
+  def ptw_resp_bypass(vpn: UInt, s2xlate: Bool) = {
+    val p_hit = RegNext(ptw.resp.bits.entry.hit(vpn, io.csr.satp.asid, allType = true, s2xlate = s2xlate) && io.ptw.resp.fire)
     val p_ppn = RegEnable(ptw.resp.bits.entry.genPPN(vpn), io.ptw.resp.fire)
+    val p_gvpn = RegEnable(ptw.resp.bits.entry.gvpn, io.ptw.resp.fire)
     val p_perm = RegEnable(ptwresp_to_tlbperm(ptw.resp.bits), io.ptw.resp.fire)
-    (p_hit, p_ppn, p_perm)
+    (p_hit, p_ppn, p_perm, p_gvpn)
   }
 
   // assert
@@ -339,7 +376,10 @@ class TLB(Width: Int, nRespDups: Int = 1, Block: Seq[Boolean], q: TLBParameters)
       difftest.io.valid := l1tlbid =/= 3.U && RegNext(io.requestor(i).req.fire) && !RegNext(io.requestor(i).req_kill) && io.requestor(i).resp.fire && !io.requestor(i).resp.bits.miss && !pf && !af && portTranslateEnable(i)
       difftest.io.index := i.U
       difftest.io.l1tlbid := l1tlbid
-      difftest.io.satp := io.csr.satp.ppn
+      difftest.io.satp := Cat(io.csr.satp.mode, io.csr.satp.asid, io.csr.satp.ppn)
+      difftest.io.vsatp := Cat(io.csr.vsatp.mode, io.csr.vsatp.asid, io.csr.vsatp.ppn)
+      difftest.io.hgatp := Cat(io.csr.hgatp.mode, io.csr.hgatp.asid, io.csr.hgatp.ppn)
+      difftest.io.s2xlate := RegNext(req_in(i).bits.hlvx || req_in(i).bits.hyperinst)
       difftest.io.vpn := RegNext(get_pn(req_in(i).bits.vaddr))
       difftest.io.ppn := get_pn(io.requestor(i).resp.bits.paddr(0))
     }
diff --git a/src/main/scala/xiangshan/cache/mmu/TLBStorage.scala b/src/main/scala/xiangshan/cache/mmu/TLBStorage.scala
index 9f8745cbc..92730f326 100644
--- a/src/main/scala/xiangshan/cache/mmu/TLBStorage.scala
+++ b/src/main/scala/xiangshan/cache/mmu/TLBStorage.scala
@@ -105,13 +105,13 @@ class TLBFA(
     val req = io.r.req(i)
     val resp = io.r.resp(i)
     val access = io.access(i)
-
+    val s2xlate = req.bits.s2xlate
     val vpn = req.bits.vpn
     val vpn_reg = RegEnable(vpn, req.fire())
     val vpn_gen_ppn = if(saveLevel) vpn else vpn_reg
 
     val refill_mask = Mux(io.w.valid, UIntToOH(io.w.bits.wayIdx), 0.U(nWays.W))
-    val hitVec = VecInit((entries.zipWithIndex).zip(v zip refill_mask.asBools).map{case (e, m) => e._1.hit(vpn, io.csr.satp.asid) && m._1 && !m._2 })
+    val hitVec = VecInit((entries.zipWithIndex).zip(v zip refill_mask.asBools).map{case (e, m) => e._1.hit(vpn, io.csr.satp.asid, s2xlate, vmid = io.csr.hgatp.asid) && m._1 && !m._2 })
 
     hitVec.suggestName("hitVec")
 
@@ -122,9 +122,11 @@ class TLBFA(
     resp.bits.hit := Cat(hitVecReg).orR
     if (nWays == 1) {
       resp.bits.ppn(0) := entries(0).genPPN(saveLevel, req.valid)(vpn_gen_ppn)
+      resp.bits.gvpn(0) := entries(0).gvpn
       resp.bits.perm(0) := entries(0).perm
     } else {
       resp.bits.ppn(0) := ParallelMux(hitVecReg zip entries.map(_.genPPN(saveLevel, req.valid)(vpn_gen_ppn)))
+      resp.bits.gvpn(0) := ParallelMux(hitVecReg zip entries.map(_.gvpn))
       resp.bits.perm(0) := ParallelMux(hitVecReg zip entries.map(_.perm))
     }
 
@@ -134,15 +136,16 @@ class TLBFA(
 
     resp.bits.hit.suggestName("hit")
     resp.bits.ppn.suggestName("ppn")
+    resp.bits.gvpn.suggestName("gvpn")
     resp.bits.perm.suggestName("perm")
   }
 
   when (io.w.valid) {
     v(io.w.bits.wayIdx) := true.B
-    entries(io.w.bits.wayIdx).apply(io.w.bits.data, io.csr.satp.asid, io.w.bits.data_replenish)
+    entries(io.w.bits.wayIdx).apply(io.w.bits.data, io.w.bits.data_replenish)
   }
   // write assert, shoulg not duplicate with the existing entries
-  val w_hit_vec = VecInit(entries.zip(v).map{case (e, vi) => e.hit(io.w.bits.data.entry.tag, io.csr.satp.asid) && vi })
+  val w_hit_vec = VecInit(entries.zip(v).map{case (e, vi) => e.hit(io.w.bits.data.entry.tag, io.csr.satp.asid, io.w.bits.data.entry.s2xlate, vmid = io.csr.hgatp.asid) && vi })
   XSError(io.w.valid && Cat(w_hit_vec).orR, s"${parentName} refill, duplicate with existing entries")
 
   val refill_vpn_reg = RegNext(io.w.bits.data.entry.tag)
@@ -156,28 +159,65 @@ class TLBFA(
   }
 
   val sfence = io.sfence
+  val sfence_valid = sfence.valid && !sfence.bits.hg && !sfence.bits.hv
   val sfence_vpn = sfence.bits.addr.asTypeOf(new VaBundle().cloneType).vpn
-  val sfenceHit = entries.map(_.hit(sfence_vpn, sfence.bits.asid))
-  val sfenceHit_noasid = entries.map(_.hit(sfence_vpn, sfence.bits.asid, ignoreAsid = true))
-  when (io.sfence.valid) {
+  val sfenceHit = entries.map(_.hit(sfence_vpn, sfence.bits.asid, io.csr.priv.virt, vmid = io.csr.hgatp.asid))
+  val sfenceHit_noasid = entries.map(_.hit(sfence_vpn, sfence.bits.asid, io.csr.priv.virt, ignoreAsid = true, vmid = io.csr.hgatp.asid))
+  when (sfence_valid) {
     when (sfence.bits.rs1) { // virtual address *.rs1 <- (rs1===0.U)
       when (sfence.bits.rs2) { // asid, but i do not want to support asid, *.rs2 <- (rs2===0.U)
-        // all addr and all asid
-        v.map(_ := false.B)
+        // all addr and all asid when virt == false
+        // all entry which vmid === hgatp.vmid when virt == true
+        v.zipWithIndex.map{ case (a, i) => a := a && !((io.csr.priv.virt === false.B && !entries(i).s2xlate) || (io.csr.priv.virt && entries(i).s2xlate && entries(i).vmid === io.csr.hgatp.asid))}
       }.otherwise {
-        // all addr but specific asid
-        v.zipWithIndex.map{ case (a,i) => a := a & (g(i) | !(entries(i).asid === sfence.bits.asid)) }
+        // when virt == false, host entries(asid == sfence.asid)
+        // when virt == true, guest entries(vimd == hgatp.vmid && asid == sfence.asid)
+        v.zipWithIndex.map{ case (a, i) => a := a && !(!g(i) && ((!io.csr.priv.virt && !entries(i).s2xlate && entries(i).asid === sfence.bits.asid) || (io.csr.priv.virt && entries(i).s2xlate && entries(i).vmid === io.csr.hgatp.asid && entries(i).asid === sfence.bits.asid)))}
       }
     }.otherwise {
       when (sfence.bits.rs2) {
         // specific addr but all asid
-        v.zipWithIndex.map{ case (a,i) => a := a & !sfenceHit_noasid(i) }
+        v.zipWithIndex.map{ case (a, i) => a := a && !sfenceHit_noasid(i) }
       }.otherwise {
         // specific addr and specific asid
-        v.zipWithIndex.map{ case (a,i) => a := a & !(sfenceHit(i) && !g(i)) }
+        v.zipWithIndex.map{ case (a, i) => a := a && !(sfenceHit(i) && !g(i)) }
+      }
+    }
+  }
+  val hfencev_valid = sfence.valid && sfence.bits.hv
+  val hfenceg_valid = sfence.valid && sfence.bits.hg
+  // it is similar to sfence in VS mode
+  val hfencev = io.sfence
+  val hfencev_vpn = sfence_vpn
+  val hfencevHit = entries.map(_.hit(hfencev_vpn, hfencev.bits.asid, true.B, vmid = io.csr.hgatp.asid))
+  val hfencevHit_noasid = entries.map(_.hit(hfencev_vpn, hfencev.bits.asid, true.B, ignoreAsid = true))
+  when (hfencev_valid) {
+    when(hfencev.bits.rs1){
+      when(hfencev.bits.rs2){
+        v.zipWithIndex.map{ case (a, i) => a := a && !(entries(i).s2xlate && entries(i).vmid === io.csr.hgatp.asid)}
+      }.otherwise {
+        v.zipWithIndex.map{ case (a, i) => a := a && !(!g(i) && (entries(i).s2xlate && entries(i).vmid === io.csr.hgatp.asid && entries(i).asid === sfence.bits.asid))}
+      }
+    }.otherwise {
+      when (hfencev.bits.rs2) {
+        v.zipWithIndex.map{ case (a, i) => a := a && !hfencevHit_noasid(i) }
+      }.otherwise {
+        v.zipWithIndex.map{ case(a, i) => a:= a && !(hfencevHit(i) && !g(i))}
       }
     }
   }
+  // for H extention, L1TLB store the map of guest vaddr -> host vaddr, don't store guest gpaddr, so we ignore the gaddr of hfence.gvma
+  // So we flush ,in the future, we will gradually perfect it.
+  val hfenceg = io.sfence
+  val hfenceg_vpn = sfence_vpn // gpaddr
+  // ignore rs1
+  when (hfenceg_valid) {
+    when(hfenceg.bits.rs2){
+        v.zipWithIndex.map { case (a, i) => a := a && !entries(i).s2xlate}
+    }.otherwise{
+        v.zipWithIndex.map { case (a, i) => a := a && !(entries(i).s2xlate && entries(i).vmid === io.csr.hgatp.asid)}
+    }
+  }
 
   val victim_idx = io.w.bits.wayIdx
   io.victim.out.valid := v(victim_idx) && io.w.valid && entries(victim_idx).is_normalentry()
@@ -189,6 +229,9 @@ class TLBFA(
     n.ppn := ns.ppn
     n.tag := ns.tag
     n.asid := ns.asid
+    n.gvpn := ns.gvpn
+    n.vmid := ns.vmid
+    n.s2xlate := ns.s2xlate
     n
   }
 
@@ -240,7 +283,7 @@ class TLBSA(
     val req = io.r.req(i)
     val resp = io.r.resp(i)
     val access = io.access(i)
-
+    val s2xlate = req.bits.s2xlate
     val vpn = req.bits.vpn
     val vpn_reg = RegEnable(vpn, req.fire())
 
@@ -252,16 +295,18 @@ class TLBSA(
     entries.io.raddr(i) := ridx
 
     val data = entries.io.rdata(i)
-    val hit = data(0).hit(vpn_reg, io.csr.satp.asid, nSets) && (vidx(0) || vidx_bypass)
+    val hit = data(0).hit(vpn_reg, io.csr.satp.asid, s2xlate, nSets, vmid = io.csr.hgatp.asid) && (vidx(0) || vidx_bypass)
     resp.bits.hit := hit
     for (d <- 0 until nDups) {
       resp.bits.ppn(d) := data(d).genPPN()(vpn_reg)
+      resp.bits.gvpn(d) := data(d).gvpn
       resp.bits.perm(d) := data(d).perm
     }
 
     resp.valid := { RegNext(req.valid) }
     resp.bits.hit.suggestName("hit")
     resp.bits.ppn.suggestName("ppn")
+    resp.bits.gvpn.suggestName("gvpn")
     resp.bits.perm.suggestName("perm")
 
     access.sets := get_set_idx(vpn_reg, nSets) // no use
@@ -275,7 +320,7 @@ class TLBSA(
     get_set_idx(io.w.bits.data.entry.tag, nSets),
     get_set_idx(io.victim.in.bits.entry.tag, nSets))
   entries.io.wdata := Mux(io.w.valid,
-    (Wire(new TlbEntry(normalPage, superPage)).apply(io.w.bits.data, io.csr.satp.asid, io.w.bits.data_replenish)),
+    (Wire(new TlbEntry(normalPage, superPage)).apply(io.w.bits.data, io.w.bits.data_replenish)),
     io.victim.in.bits.entry)
 
   when (io.victim.in.valid) {
@@ -295,10 +340,10 @@ class TLBSA(
       access.touch_ways.bits := refill_wayIdx_reg
     }
   }
-
+ // can't get entry's asid vmid, so only care addr
   val sfence = io.sfence
   val sfence_vpn = sfence.bits.addr.asTypeOf(new VaBundle().cloneType).vpn
-  when (io.sfence.valid) {
+  when (io.sfence.valid && !io.sfence.bits.hg) {
     when (sfence.bits.rs1) { // virtual address *.rs1 <- (rs1===0.U)
         v.map(a => a.map(b => b := false.B))
     }.otherwise {
@@ -481,12 +526,14 @@ class TlbStorageWrapper(ports: Int, q: TLBParameters, nDups: Int = 1)(implicit p
     normalPage.r_req_apply(
       valid = io.r.req(i).valid,
       vpn = io.r.req(i).bits.vpn,
-      i = i
+      i = i,
+      s2xlate = io.r.req(i).bits.s2xlate
     )
     superPage.r_req_apply(
       valid = io.r.req(i).valid,
       vpn = io.r.req(i).bits.vpn,
-      i = i
+      i = i,
+      s2xlate = io.r.req(i).bits.s2xlate
     )
   }
 
@@ -502,10 +549,12 @@ class TlbStorageWrapper(ports: Int, q: TLBParameters, nDups: Int = 1)(implicit p
     rp.bits.hit := np.bits.hit || sp.bits.hit
     for (d <- 0 until nDups) {
       rp.bits.ppn(d) := Mux(sp.bits.hit, sp.bits.ppn(0), np.bits.ppn(d))
+      rp.bits.gvpn(d) := Mux(sp.bits.hit, sp.bits.gvpn(0), np.bits.gvpn(d))
       rp.bits.perm(d) := Mux(sp.bits.hit, sp.bits.perm(0), np.bits.perm(d))
     }
     rp.bits.super_hit := sp.bits.hit
     rp.bits.super_ppn := sp.bits.ppn(0)
+    rp.bits.super_gvpn := sp.bits.gvpn(0)
     rp.bits.spm := np.bits.perm(0).pm
     assert(!np.bits.hit || !sp.bits.hit || !rp.valid, s"${q.name} storage ports${i} normal and super multi-hit")
   }
diff --git a/src/main/scala/xiangshan/frontend/Frontend.scala b/src/main/scala/xiangshan/frontend/Frontend.scala
index affbc52b4..40cbd48e6 100644
--- a/src/main/scala/xiangshan/frontend/Frontend.scala
+++ b/src/main/scala/xiangshan/frontend/Frontend.scala
@@ -73,7 +73,6 @@ class FrontendImp (outer: Frontend) extends LazyModuleImp(outer)
   val tlbCsr = DelayN(io.tlbCsr, 2)
   val csrCtrl = DelayN(io.csrCtrl, 2)
   val sfence = RegNext(RegNext(io.sfence))
-
   // trigger
   ifu.io.frontendTrigger := csrCtrl.frontend_trigger
   val triggerEn = csrCtrl.trigger_enable
diff --git a/src/main/scala/xiangshan/frontend/FrontendBundle.scala b/src/main/scala/xiangshan/frontend/FrontendBundle.scala
index f3c6deb86..1387a9694 100644
--- a/src/main/scala/xiangshan/frontend/FrontendBundle.scala
+++ b/src/main/scala/xiangshan/frontend/FrontendBundle.scala
@@ -114,10 +114,12 @@ class FetchToIBuffer(implicit p: Parameters) extends XSBundle {
   val enqEnable = UInt(PredictWidth.W)
   val pd        = Vec(PredictWidth, new PreDecodeInfo)
   val pc        = Vec(PredictWidth, UInt(VAddrBits.W))
+  val gpaddr    = Vec(PredictWidth, UInt(GPAddrBits.W))
   val foldpc    = Vec(PredictWidth, UInt(MemPredPCWidth.W))
   val ftqPtr       = new FtqPtr
   val ftqOffset    = Vec(PredictWidth, ValidUndirectioned(UInt(log2Ceil(PredictWidth).W)))
   val ipf          = Vec(PredictWidth, Bool())
+  val igpf         = Vec(PredictWidth, Bool())
   val acf          = Vec(PredictWidth, Bool())
   val crossPageIPFFix = Vec(PredictWidth, Bool())
   val triggered    = Vec(PredictWidth, new TriggerCf)
diff --git a/src/main/scala/xiangshan/frontend/IFU.scala b/src/main/scala/xiangshan/frontend/IFU.scala
index ddae33575..c3981f6e3 100644
--- a/src/main/scala/xiangshan/frontend/IFU.scala
+++ b/src/main/scala/xiangshan/frontend/IFU.scala
@@ -264,9 +264,12 @@ class NewIFU(implicit p: Parameters) extends XSModule
 
 
   val f2_except_pf    = VecInit((0 until PortNumber).map(i => fromICache(i).bits.tlbExcp.pageFault))
+  val f2_except_gpf   = VecInit((0 until PortNumber).map(i => fromICache(i).bits.tlbExcp.guestpageFault))
   val f2_except_af    = VecInit((0 until PortNumber).map(i => fromICache(i).bits.tlbExcp.accessFault))
+  val f2_gpaddrs      = VecInit((0 until PortNumber).map(i => fromICache(i).bits.gpaddr))
   val f2_mmio         = fromICache(0).bits.tlbExcp.mmio && !fromICache(0).bits.tlbExcp.accessFault &&
-                                                           !fromICache(0).bits.tlbExcp.pageFault
+                                                           !fromICache(0).bits.tlbExcp.pageFault   &&
+                                                           !fromICache(0).bits.tlbExcp.guestpageFault
 
   val f2_pc               = RegEnable(f1_pc,  f1_fire)
   val f2_half_snpc        = RegEnable(f1_half_snpc,  f1_fire)
@@ -287,7 +290,10 @@ class NewIFU(implicit p: Parameters) extends XSModule
   val f2_ftr_range  = Fill(PredictWidth,  f2_ftq_req.ftqOffset.valid) | Fill(PredictWidth, 1.U(1.W)) >> ~getBasicBlockIdx(f2_ftq_req.nextStartAddr, f2_ftq_req.startAddr)
   val f2_instr_range = f2_jump_range & f2_ftr_range
   val f2_pf_vec = VecInit((0 until PredictWidth).map(i => (!isNextLine(f2_pc(i), f2_ftq_req.startAddr) && f2_except_pf(0)   ||  isNextLine(f2_pc(i), f2_ftq_req.startAddr) && f2_doubleLine &&  f2_except_pf(1))))
+  val f2_gpf_vec = VecInit((0 until PredictWidth).map(i => (!isNextLine(f2_pc(i), f2_ftq_req.startAddr) && f2_except_gpf(0)   ||  isNextLine(f2_pc(i), f2_ftq_req.startAddr) && f2_doubleLine &&  f2_except_gpf(1))))
   val f2_af_vec = VecInit((0 until PredictWidth).map(i => (!isNextLine(f2_pc(i), f2_ftq_req.startAddr) && f2_except_af(0)   ||  isNextLine(f2_pc(i), f2_ftq_req.startAddr) && f2_doubleLine && f2_except_af(1))))
+  val f2_gpaddrs_vec = VecInit((0 until PredictWidth).map(i => Mux(!isNextLine(f2_pc(i), f2_ftq_req.startAddr), f2_gpaddrs(0),  Mux(isNextLine(f2_pc(i), f2_ftq_req.startAddr) && f2_doubleLine, f2_gpaddrs(1), 0.U(GPAddrBits.W)))))
+
 
   val f2_paddrs       = VecInit((0 until PortNumber).map(i => fromICache(i).bits.paddr))
   val f2_perf_info    = io.icachePerfInfo
@@ -344,6 +350,7 @@ class NewIFU(implicit p: Parameters) extends XSModule
   val f2_jump_offset    = preDecoderOut.jumpOffset
   val f2_hasHalfValid   =  preDecoderOut.hasHalfValid
   val f2_crossPageFault = VecInit((0 until PredictWidth).map(i => isLastInLine(f2_pc(i)) && !f2_except_pf(0) && f2_doubleLine &&  f2_except_pf(1) && !f2_pd(i).isRVC ))
+  val f2_crossGuestPageFault = VecInit((0 until PredictWidth).map(i => isLastInLine(f2_pc(i)) && !f2_except_gpf(0) && f2_doubleLine &&  f2_except_gpf(1) && !f2_pd(i).isRVC ))
 
   XSPerfAccumulate("fetch_bubble_icache_not_resp",   f2_valid && !icacheRespAllValid )
 
@@ -388,15 +395,18 @@ class NewIFU(implicit p: Parameters) extends XSModule
   val f3_jump_offset    = RegEnable(next = f2_jump_offset, enable = f2_fire)
   val f3_af_vec         = RegEnable(next = f2_af_vec,      enable = f2_fire)
   val f3_pf_vec         = RegEnable(next = f2_pf_vec ,     enable = f2_fire)
+  val f3_gpf_vec        = RegEnable(next = f2_gpf_vec ,    enable = f2_fire)
+  val f3_gpaddrs        = RegEnable(next = f2_gpaddrs_vec, enable = f2_fire)
   val f3_pc             = RegEnable(next = f2_pc,          enable = f2_fire)
-  val f3_half_snpc        = RegEnable(next = f2_half_snpc, enable = f2_fire)
+  val f3_half_snpc      = RegEnable(next = f2_half_snpc,   enable = f2_fire)
   val f3_instr_range    = RegEnable(next = f2_instr_range, enable = f2_fire)
   val f3_foldpc         = RegEnable(next = f2_foldpc,      enable = f2_fire)
   val f3_crossPageFault = RegEnable(next = f2_crossPageFault,      enable = f2_fire)
+  val f3_crossGuestPageFault = RegEnable(next = f2_crossGuestPageFault,      enable = f2_fire)
   val f3_hasHalfValid   = RegEnable(next = f2_hasHalfValid,      enable = f2_fire)
   val f3_except         = VecInit((0 until 2).map{i => f3_except_pf(i) || f3_except_af(i)})
   val f3_has_except     = f3_valid && (f3_except_af.reduce(_||_) || f3_except_pf.reduce(_||_))
-  val f3_pAddrs   = RegEnable(f2_paddrs,  f2_fire)
+  val f3_pAddrs         = RegEnable(f2_paddrs,  f2_fire)
   val f3_resend_vaddr   = RegEnable(f2_resend_vaddr,       f2_fire)
 
   when(f3_valid && !f3_ftq_req.ftqOffset.valid){
@@ -409,6 +419,7 @@ class NewIFU(implicit p: Parameters) extends XSModule
   val mmio_resend_addr =RegInit(0.U(PAddrBits.W))
   val mmio_resend_af  = RegInit(false.B)
   val mmio_resend_pf  = RegInit(false.B)
+  val mmio_resend_gpf  = RegInit(false.B)
 
   //last instuction finish
   val is_first_instr = RegInit(true.B)
@@ -492,11 +503,13 @@ class NewIFU(implicit p: Parameters) extends XSModule
 
     is(m_tlbResp){
       val tlbExept = io.iTLBInter.resp.bits.excp(0).pf.instr ||
+                     io.iTLBInter.resp.bits.excp(0).pf.instrG||
                      io.iTLBInter.resp.bits.excp(0).af.instr
       mmio_state :=  Mux(tlbExept,m_waitCommit,m_sendPMP)
       mmio_resend_addr := io.iTLBInter.resp.bits.paddr(0)
       mmio_resend_af := mmio_resend_af || io.iTLBInter.resp.bits.excp(0).af.instr
       mmio_resend_pf := mmio_resend_pf || io.iTLBInter.resp.bits.excp(0).pf.instr
+      mmio_resend_gpf := mmio_resend_gpf || io.iTLBInter.resp.bits.excp(0).pf.instrG
     }
 
     is(m_sendPMP){
@@ -554,6 +567,8 @@ class NewIFU(implicit p: Parameters) extends XSModule
   io.iTLBInter.req.bits.debug.robIdx        := DontCare
   io.iTLBInter.req.bits.no_translate        := false.B
   io.iTLBInter.req.bits.debug.isFirstIssue  := DontCare
+  io.iTLBInter.req.bits.hyperinst           := DontCare
+  io.iTLBInter.req.bits.hlvx                := DontCare
 
   io.pmp.req.valid := (mmio_state === m_sendPMP) && f3_req_is_mmio
   io.pmp.req.bits.addr  := mmio_resend_addr
@@ -623,11 +638,13 @@ class NewIFU(implicit p: Parameters) extends XSModule
   io.toIbuffer.bits.pd          := f3_pd
   io.toIbuffer.bits.ftqPtr      := f3_ftq_req.ftqIdx
   io.toIbuffer.bits.pc          := f3_pc
+  io.toIbuffer.bits.gpaddr      := f3_gpaddrs
   io.toIbuffer.bits.ftqOffset.zipWithIndex.map{case(a, i) => a.bits := i.U; a.valid := checkerOutStage1.fixedTaken(i) && !f3_req_is_mmio}
   io.toIbuffer.bits.foldpc      := f3_foldpc
   io.toIbuffer.bits.ipf         := VecInit(f3_pf_vec.zip(f3_crossPageFault).map{case (pf, crossPF) => pf || crossPF})
+  io.toIbuffer.bits.igpf         := VecInit(f3_gpf_vec.zip(f3_crossGuestPageFault).map{case (gpf, crossGPF) => gpf || crossGPF})
   io.toIbuffer.bits.acf         := f3_af_vec
-  io.toIbuffer.bits.crossPageIPFFix := f3_crossPageFault
+  io.toIbuffer.bits.crossPageIPFFix := (0 until PredictWidth).map(i => f3_crossPageFault(i) || f3_crossGuestPageFault(i))
   io.toIbuffer.bits.triggered   := f3_triggered
 
   when(f3_lastHalf.valid){
@@ -678,6 +695,7 @@ class NewIFU(implicit p: Parameters) extends XSModule
 
     io.toIbuffer.bits.acf(0) := mmio_resend_af
     io.toIbuffer.bits.ipf(0) := mmio_resend_pf
+    io.toIbuffer.bits.igpf(0) := mmio_resend_gpf
     io.toIbuffer.bits.crossPageIPFFix(0) := mmio_resend_pf
 
     io.toIbuffer.bits.enqEnable   := f3_mmio_range.asUInt
diff --git a/src/main/scala/xiangshan/frontend/Ibuffer.scala b/src/main/scala/xiangshan/frontend/Ibuffer.scala
index fa23ddee1..087045969 100644
--- a/src/main/scala/xiangshan/frontend/Ibuffer.scala
+++ b/src/main/scala/xiangshan/frontend/Ibuffer.scala
@@ -45,19 +45,23 @@ class IBufEntry(implicit p: Parameters) extends XSBundle {
   val ftqPtr = new FtqPtr
   val ftqOffset = UInt(log2Ceil(PredictWidth).W)
   val ipf = Bool()
+  val igpf = Bool()
   val acf = Bool()
   val crossPageIPFFix = Bool()
   val triggered = new TriggerCf
+  val gpaddr = UInt(GPAddrBits.W) // for H extention
 
   def fromFetch(fetch: FetchToIBuffer, i: Int): IBufEntry = {
     inst   := fetch.instrs(i)
     pc     := fetch.pc(i)
     foldpc := fetch.foldpc(i)
+    gpaddr := fetch.gpaddr(i)
     pd     := fetch.pd(i)
     pred_taken := fetch.ftqOffset(i).valid
     ftqPtr := fetch.ftqPtr
     ftqOffset := fetch.ftqOffset(i).bits
     ipf := fetch.ipf(i)
+    igpf := fetch.igpf(i)
     acf := fetch.acf(i)
     crossPageIPFFix := fetch.crossPageIPFFix(i)
     triggered := fetch.triggered(i)
@@ -68,9 +72,11 @@ class IBufEntry(implicit p: Parameters) extends XSBundle {
     val cf = Wire(new CtrlFlow)
     cf.instr := inst
     cf.pc := pc
+    cf.gpaddr := gpaddr
     cf.foldpc := foldpc
     cf.exceptionVec := 0.U.asTypeOf(ExceptionVec())
     cf.exceptionVec(instrPageFault) := ipf
+    cf.exceptionVec(instrGuestPageFault) := igpf
     cf.exceptionVec(instrAccessFault) := acf
     cf.trigger := triggered
     cf.pd := pd
diff --git a/src/main/scala/xiangshan/frontend/icache/ICacheMainPipe.scala b/src/main/scala/xiangshan/frontend/icache/ICacheMainPipe.scala
index 89449382b..a733876b1 100644
--- a/src/main/scala/xiangshan/frontend/icache/ICacheMainPipe.scala
+++ b/src/main/scala/xiangshan/frontend/icache/ICacheMainPipe.scala
@@ -40,8 +40,10 @@ class ICacheMainPipeResp(implicit p: Parameters) extends ICacheBundle
   val sramData = UInt(blockBits.W)
   val select   = Bool()
   val paddr    = UInt(PAddrBits.W)
+  val gpaddr    = UInt(GPAddrBits.W)
   val tlbExcp  = new Bundle{
     val pageFault = Bool()
+    val guestpageFault = Bool()
     val accessFault = Bool()
     val mmio = Bool()
   }
@@ -199,6 +201,8 @@ class ICacheMainPipe(implicit p: Parameters) extends ICacheModule
     port.bits.debug.robIdx        := DontCare
     port.bits.no_translate        := false.B
     port.bits.debug.isFirstIssue  := DontCare
+    port.bits.hyperinst           := DontCare
+    port.bits.hlvx                := DontCare
   }
 
   /** ITLB & ICACHE sync case
@@ -261,9 +265,11 @@ class ICacheMainPipe(implicit p: Parameters) extends ICacheModule
   assert(RegNext(s1_valid || !Cat(tlb_resp_valid).orR, true.B), "when !s1_valid, should not tlb_resp_valid")
 
   val tlbRespPAddr = VecInit((0 until PortNumber).map(i => ResultHoldBypass(valid = tlb_back(i), data = fromITLB(i).bits.paddr(0))))
+  val tlbRespGPAddr = VecInit((0 until PortNumber).map(i => ResultHoldBypass(valid = tlb_back(i), data = fromITLB(i).bits.gpaddr(0))))
   val tlbExcpPF = VecInit((0 until PortNumber).map(i => ResultHoldBypass(valid = tlb_back(i), data = fromITLB(i).bits.excp(0).pf.instr) && tlb_need_back(i)))
+  val tlbExcpGPF = VecInit((0 until PortNumber).map(i => ResultHoldBypass(valid = tlb_back(i), data = fromITLB(i).bits.excp(0).pf.instrG) && tlb_need_back(i)))
   val tlbExcpAF = VecInit((0 until PortNumber).map(i => ResultHoldBypass(valid = tlb_back(i), data = fromITLB(i).bits.excp(0).af.instr) && tlb_need_back(i)))
-  val tlbExcp = VecInit((0 until PortNumber).map(i => tlbExcpPF(i) || tlbExcpPF(i)))
+  val tlbExcp = VecInit((0 until PortNumber).map(i => tlbExcpPF(i) || tlbExcpAF(i) || tlbExcpGPF(i)))
 
   val tlbRespAllValid = Cat((0 until PortNumber).map(i => !tlb_need_back(i) || tlb_resp_valid(i))).andR
   s1_ready := s2_ready && tlbRespAllValid  || !s1_valid
@@ -271,6 +277,7 @@ class ICacheMainPipe(implicit p: Parameters) extends ICacheModule
 
   /** s1 hit check/tag compare */
   val s1_req_paddr              = tlbRespPAddr
+  val s1_req_gpaddr             = tlbRespGPAddr
   val s1_req_ptags              = VecInit(s1_req_paddr.map(get_phy_tag(_)))
 
   val s1_meta_ptags              = ResultHoldBypass(data = metaResp.tags, valid = RegNext(s0_fire))
@@ -341,6 +348,7 @@ class ICacheMainPipe(implicit p: Parameters) extends ICacheModule
   val mmio = fromPMP.map(port => port.mmio) // TODO: handle it
 
   val (s2_req_paddr , s2_req_vaddr)   = (RegEnable(s1_req_paddr, s1_fire), RegEnable(s1_req_vaddr, s1_fire))
+  val s2_req_gpaddr   = RegEnable(s1_req_gpaddr, s1_fire)
   val s2_req_vsetIdx  = RegEnable(s1_req_vsetIdx, s1_fire)
   val s2_req_ptags    = RegEnable(s1_req_ptags, s1_fire)
   val s2_only_first   = RegEnable(s1_only_first, s1_fire)
@@ -412,15 +420,16 @@ class ICacheMainPipe(implicit p: Parameters) extends ICacheModule
   //exception information
   //short delay exception signal
   val s2_except_pf        = RegEnable(tlbExcpPF, s1_fire)
+  val s2_except_gpf       = RegEnable(tlbExcpGPF, s1_fire)
   val s2_except_tlb_af    = RegEnable(tlbExcpAF, s1_fire)
   //long delay exception signal
   val s2_except_pmp_af    =  DataHoldBypass(pmpExcpAF, RegNext(s1_fire))    
   // val s2_except_parity_af =  VecInit(s2_parity_error(i) && RegNext(RegNext(s1_fire))                      )
 
-  val s2_except    = VecInit((0 until 2).map{i => s2_except_pf(i) || s2_except_tlb_af(i)})
-  val s2_has_except = s2_valid && (s2_except_tlb_af.reduce(_||_) || s2_except_pf.reduce(_||_))
+  val s2_except    = VecInit((0 until 2).map{i => s2_except_pf(i) || s2_except_gpf(i) || s2_except_tlb_af(i)})
+  val s2_has_except = s2_valid && (s2_except_tlb_af.reduce(_||_) || s2_except_pf.reduce(_||_) || s2_except_gpf.reduce(_||_))
   //MMIO
-  val s2_mmio      = DataHoldBypass(io.pmp(0).resp.mmio && !s2_except_tlb_af(0) && !s2_except_pmp_af(0) && !s2_except_pf(0), RegNext(s1_fire)).asBool() && s2_valid
+  val s2_mmio      = DataHoldBypass(io.pmp(0).resp.mmio && !s2_except_tlb_af(0) && !s2_except_pmp_af(0) && !s2_except_pf(0) && !s2_except_gpf(0), RegNext(s1_fire)).asBool() && s2_valid
 
   //send physical address to PMP
   io.pmp.zipWithIndex.map { case (p, i) =>
@@ -702,8 +711,10 @@ class ICacheMainPipe(implicit p: Parameters) extends ICacheModule
     toIFU(i).bits.sramData  := s2_hit_datas(i)
     toIFU(i).bits.select    := s2_port_hit(i)
     toIFU(i).bits.paddr     := s2_req_paddr(i)
+    toIFU(i).bits.gpaddr    := s2_req_gpaddr(i)
     toIFU(i).bits.vaddr     := s2_req_vaddr(i)
     toIFU(i).bits.tlbExcp.pageFault     := s2_except_pf(i)
+    toIFU(i).bits.tlbExcp.guestpageFault     := s2_except_gpf(i)
     toIFU(i).bits.tlbExcp.accessFault   := s2_except_tlb_af(i) || missSlot(i).m_corrupt || s2_except_pmp_af(i)
     toIFU(i).bits.tlbExcp.mmio          := s2_mmio
 
diff --git a/src/main/scala/xiangshan/frontend/icache/IPrefetch.scala b/src/main/scala/xiangshan/frontend/icache/IPrefetch.scala
index e99be2be4..35fb7bdd9 100644
--- a/src/main/scala/xiangshan/frontend/icache/IPrefetch.scala
+++ b/src/main/scala/xiangshan/frontend/icache/IPrefetch.scala
@@ -110,7 +110,8 @@ class IPrefetchPipe(implicit p: Parameters) extends  IPrefetchModule
   toITLB.bits.debug.robIdx        := DontCare
   toITLB.bits.no_translate        := false.B
   toITLB.bits.debug.isFirstIssue  := DontCare
-
+  toITLB.bits.hyperinst           := DontCare
+  toITLB.bits.hlvx                := DontCare
 
   fromITLB.ready := true.B
 
@@ -128,9 +129,10 @@ class IPrefetchPipe(implicit p: Parameters) extends  IPrefetchModule
 
   val tlb_resp_paddr = ResultHoldBypass(valid = RegNext(p0_fire), data = fromITLB.bits.paddr(0))
   val tlb_resp_pf    = ResultHoldBypass(valid = RegNext(p0_fire), data = fromITLB.bits.excp(0).pf.instr && tlb_resp_valid)
+  val tlb_resp_gpf    = ResultHoldBypass(valid = RegNext(p0_fire), data = fromITLB.bits.excp(0).pf.instrG && tlb_resp_valid)
   val tlb_resp_af    = ResultHoldBypass(valid = RegNext(p0_fire), data = fromITLB.bits.excp(0).af.instr && tlb_resp_valid)
 
-  val p1_exception  = VecInit(Seq(tlb_resp_pf, tlb_resp_af))
+  val p1_exception  = VecInit(Seq(tlb_resp_pf, tlb_resp_gpf, tlb_resp_af))
   val p1_has_except =  p1_exception.reduce(_ || _)
 
   val p1_ptag = get_phy_tag(tlb_resp_paddr)
@@ -156,13 +158,14 @@ class IPrefetchPipe(implicit p: Parameters) extends  IPrefetchModule
 
   val p2_paddr     = RegEnable(next = tlb_resp_paddr,  enable = p1_fire)
   val p2_except_pf = RegEnable(next =tlb_resp_pf, enable = p1_fire)
+  val p2_except_gpf = RegEnable(next =tlb_resp_gpf, enable = p1_fire)
   val p2_except_tlb_af = RegEnable(next = tlb_resp_af, enable = p1_fire)
 
   /*when a prefetch req meet with a miss req in MSHR cancle the prefetch req */
   val p2_check_in_mshr = VecInit(io.fromMSHR.map(mshr => mshr.valid && mshr.bits === addrAlign(p2_paddr, blockBytes, PAddrBits))).reduce(_||_)
 
   //TODO wait PMP logic
-  val p2_exception  = VecInit(Seq(p2_except_tlb_af, p2_except_pf)).reduce(_||_)
+  val p2_exception  = VecInit(Seq(p2_except_tlb_af, p2_except_pf, p2_except_gpf)).reduce(_||_)
 
   p2_ready :=   p2_fire || p2_discard || !p2_valid
   p2_fire  :=   p2_valid && !p2_exception && p3_ready
diff --git a/src/main/scala/xiangshan/mem/MemCommon.scala b/src/main/scala/xiangshan/mem/MemCommon.scala
index 71f98f682..d135e8d0d 100644
--- a/src/main/scala/xiangshan/mem/MemCommon.scala
+++ b/src/main/scala/xiangshan/mem/MemCommon.scala
@@ -52,6 +52,7 @@ object genWdata {
 
 class LsPipelineBundle(implicit p: Parameters) extends XSBundleWithMicroOp with HasDCacheParameters{
   val vaddr = UInt(VAddrBits.W)
+  val gpaddr = UInt(GPAddrBits.W)
   val paddr = UInt(PAddrBits.W)
   // val func = UInt(6.W)
   val mask = UInt(8.W)
@@ -93,6 +94,7 @@ class LdPrefetchTrainBundle(implicit p: Parameters) extends LsPipelineBundle {
   def fromLsPipelineBundle(input: LsPipelineBundle) = {
     vaddr := input.vaddr
     paddr := input.paddr
+    gpaddr := input.gpaddr
     mask := input.mask
     data := input.data
     uop := input.uop
@@ -125,6 +127,7 @@ class LqWriteBundle(implicit p: Parameters) extends LsPipelineBundle {
   def fromLsPipelineBundle(input: LsPipelineBundle) = {
     vaddr := input.vaddr
     paddr := input.paddr
+    gpaddr := input.gpaddr
     mask := input.mask
     data := input.data
     uop := input.uop
@@ -151,6 +154,7 @@ class LqWriteBundle(implicit p: Parameters) extends LsPipelineBundle {
 
 class LoadForwardQueryIO(implicit p: Parameters) extends XSBundleWithMicroOp {
   val vaddr = Output(UInt(VAddrBits.W))
+  val gpaddr = Output(UInt(GPAddrBits.W))
   val paddr = Output(UInt(PAddrBits.W))
   val mask = Output(UInt(8.W))
   override val uop = Output(new MicroOp) // for replay
diff --git a/src/main/scala/xiangshan/mem/lsqueue/LSQWrapper.scala b/src/main/scala/xiangshan/mem/lsqueue/LSQWrapper.scala
index bfa520534..5d28335cd 100644
--- a/src/main/scala/xiangshan/mem/lsqueue/LSQWrapper.scala
+++ b/src/main/scala/xiangshan/mem/lsqueue/LSQWrapper.scala
@@ -31,6 +31,7 @@ import xiangshan.backend.rob.RobLsqIO
 class ExceptionAddrIO(implicit p: Parameters) extends XSBundle {
   val isStore = Input(Bool())
   val vaddr = Output(UInt(VAddrBits.W))
+  val gpaddr = Output(UInt(GPAddrBits.W))
 }
 
 class FwdEntry extends Bundle {
@@ -181,7 +182,7 @@ class LsqWrappper(implicit p: Parameters) extends XSModule with HasDCacheParamet
   // s3: ptr updated & new address
   // address will be used at the next cycle after exception is triggered
   io.exceptionAddr.vaddr := Mux(RegNext(io.exceptionAddr.isStore), storeQueue.io.exceptionAddr.vaddr, loadQueue.io.exceptionAddr.vaddr)
-
+  io.exceptionAddr.gpaddr := Mux(RegNext(io.exceptionAddr.isStore), storeQueue.io.exceptionAddr.gpaddr, loadQueue.io.exceptionAddr.gpaddr)
   // naive uncache arbiter
   val s_idle :: s_load :: s_store :: Nil = Enum(3)
   val pendingstate = RegInit(s_idle)
diff --git a/src/main/scala/xiangshan/mem/lsqueue/LoadQueue.scala b/src/main/scala/xiangshan/mem/lsqueue/LoadQueue.scala
index 5fa994cad..e130f3293 100644
--- a/src/main/scala/xiangshan/mem/lsqueue/LoadQueue.scala
+++ b/src/main/scala/xiangshan/mem/lsqueue/LoadQueue.scala
@@ -60,6 +60,17 @@ trait HasLoadHelper { this: XSModule =>
       LSUOpType.lbu  -> ZeroExt(rdata(7, 0) , XLEN),
       LSUOpType.lhu  -> ZeroExt(rdata(15, 0), XLEN),
       LSUOpType.lwu  -> ZeroExt(rdata(31, 0), XLEN),
+
+      // hypervisor
+      LSUOpType.hlvb  -> SignExt(rdata( 7, 0), XLEN),
+      LSUOpType.hlvh  -> SignExt(rdata(15, 0), XLEN),
+      LSUOpType.hlvw  -> SignExt(rdata(31, 0), XLEN),
+      LSUOpType.hlvd  -> SignExt(rdata(63, 0), XLEN),
+      LSUOpType.hlvbu -> ZeroExt(rdata( 7, 0), XLEN),
+      LSUOpType.hlvhu -> ZeroExt(rdata(15, 0), XLEN),
+      LSUOpType.hlvwu -> ZeroExt(rdata(31, 0), XLEN),
+      LSUOpType.hlvxhu-> ZeroExt(rdata(15, 0), XLEN),
+      LSUOpType.hlvxwu-> ZeroExt(rdata(31, 0), XLEN),
     ))
   }
 }
@@ -79,6 +90,7 @@ class LqPaddrWriteBundle(implicit p: Parameters) extends XSBundle {
 
 class LqVaddrWriteBundle(implicit p: Parameters) extends XSBundle {
   val vaddr = Output(UInt(VAddrBits.W))
+  val gpaddr = Output(UInt(GPAddrBits.W)) // for guest page fault
   val lqIdx = Output(new LqPtr)
 }
 
@@ -143,6 +155,8 @@ class LoadQueue(implicit p: Parameters) extends XSModule
   // vaddrModule's read port 0 for exception addr, port 1 for uncache vaddr read, port {2, 3} for load replay
   val vaddrModule = Module(new SyncDataModuleTemplate(UInt(VAddrBits.W), LoadQueueSize, numRead = 1 + 1 + LoadPipelineWidth, numWrite = LoadPipelineWidth))
   vaddrModule.io := DontCare
+  val gpaddrModule = Module(new SyncDataModuleTemplate(UInt(GPAddrBits.W), LoadQueueSize, numRead = 1 + 1 + LoadPipelineWidth, numWrite = LoadPipelineWidth))
+  gpaddrModule.io := DontCare
   val vaddrTriggerResultModule = Module(new SyncDataModuleTemplate(Vec(3, Bool()), LoadQueueSize, numRead = LoadPipelineWidth, numWrite = LoadPipelineWidth))
   vaddrTriggerResultModule.io := DontCare
   val allocated = RegInit(VecInit(List.fill(LoadQueueSize)(false.B))) // lq entry has been allocated
@@ -227,6 +241,7 @@ class LoadQueue(implicit p: Parameters) extends XSModule
   val debug_mmio = Reg(Vec(LoadQueueSize, Bool())) // mmio: inst is an mmio inst
   val debug_paddr = Reg(Vec(LoadQueueSize, UInt(PAddrBits.W))) // mmio: inst is an mmio inst
 
+  val gpaddr = Reg(Vec(LoadQueueSize, UInt(GPAddrBits.W))) // for guest page fault
   val enqPtrExt = RegInit(VecInit((0 until io.enq.req.length).map(_.U.asTypeOf(new LqPtr))))
   val deqPtrExt = RegInit(0.U.asTypeOf(new LqPtr))
   val deqPtrExtNext = Wire(new LqPtr)
@@ -349,6 +364,7 @@ class LoadQueue(implicit p: Parameters) extends XSModule
   (0 until LoadPipelineWidth).map(i => {
     // vaddrModule rport 0 and 1 is used by exception and mmio 
     vaddrModule.io.raddr(2 + i) := loadReplaySelGen(i)
+    gpaddrModule.io.raddr(2 + i) := loadReplaySelGen(i)
   })
 
   (0 until LoadPipelineWidth).map(i => {
@@ -376,7 +392,8 @@ class LoadQueue(implicit p: Parameters) extends XSModule
 
     io.loadOut(i).bits := DontCare
     io.loadOut(i).bits.uop := uop(replayIdx)
-    io.loadOut(i).bits.vaddr := vaddrModule.io.rdata(LoadPipelineWidth + i)
+    io.loadOut(i).bits.vaddr  := vaddrModule.io.rdata(LoadPipelineWidth + i)
+    io.loadOut(i).bits.gpaddr := gpaddrModule.io.rdata(LoadPipelineWidth + i)
     io.loadOut(i).bits.mask := genWmask(vaddrModule.io.rdata(LoadPipelineWidth + i), uop(replayIdx).ctrl.fuOpType(1,0))
     io.loadOut(i).bits.isFirstIssue := false.B
     io.loadOut(i).bits.isLoadReplay := true.B
@@ -405,6 +422,7 @@ class LoadQueue(implicit p: Parameters) extends XSModule
     dataModule.io.wb.wen(i) := false.B
     dataModule.io.paddr.wen(i) := false.B
     vaddrModule.io.wen(i) := false.B
+    gpaddrModule.io.wen(i) := false.B
     vaddrTriggerResultModule.io.wen(i) := false.B
     val loadWbIndex = io.loadIn(i).bits.uop.lqIdx.value
 
@@ -445,7 +463,7 @@ class LoadQueue(implicit p: Parameters) extends XSModule
 
       debug_mmio(loadWbIndex) := io.loadIn(i).bits.mmio
       debug_paddr(loadWbIndex) := io.loadIn(i).bits.paddr
-
+      gpaddr(loadWbIndex) := io.loadIn(i).bits.paddr
       val dcacheMissed = io.loadIn(i).bits.miss && !io.loadIn(i).bits.mmio
       if(EnableFastForward){
         miss(loadWbIndex) := dcacheMissed && !io.s2_load_data_forwarded(i) && !io.s2_dcache_require_replay(i)
@@ -516,6 +534,10 @@ class LoadQueue(implicit p: Parameters) extends XSModule
       vaddrModule.io.wen(i) := true.B
       vaddrModule.io.waddr(i) := io.loadVaddrIn(i).bits.lqIdx.value
       vaddrModule.io.wdata(i) := io.loadVaddrIn(i).bits.vaddr
+
+      gpaddrModule.io.wen(i) := true.B
+      gpaddrModule.io.waddr(i) := io.loadVaddrIn(i).bits.lqIdx.value
+      gpaddrModule.io.wdata(i) := io.loadVaddrIn(i).bits.gpaddr
     }
 
     /**
@@ -1010,10 +1032,12 @@ def detectRollback(i: Int) = {
   // Read vaddr for mem exception
   // no inst will be commited 1 cycle before tval update
   vaddrModule.io.raddr(0) := (deqPtrExt + commitCount).value
+  gpaddrModule.io.raddr(0) := (deqPtrExt + commitCount).value
   io.exceptionAddr.vaddr := vaddrModule.io.rdata(0)
-
+  io.exceptionAddr.gpaddr := gpaddrModule.io.rdata(0)
   // read vaddr for mmio, and only port {1} is used
   vaddrModule.io.raddr(1) := deqPtr
+  gpaddrModule.io.raddr(1) := deqPtr
 
   (0 until LoadPipelineWidth).map(i => {
     if(i == 0) {
diff --git a/src/main/scala/xiangshan/mem/lsqueue/StoreQueue.scala b/src/main/scala/xiangshan/mem/lsqueue/StoreQueue.scala
index 5f9679474..d1f60e77a 100644
--- a/src/main/scala/xiangshan/mem/lsqueue/StoreQueue.scala
+++ b/src/main/scala/xiangshan/mem/lsqueue/StoreQueue.scala
@@ -106,6 +106,14 @@ class StoreQueue(implicit p: Parameters) extends XSModule
     numForward = StorePipelineWidth
   ))
   paddrModule.io := DontCare
+  val gpaddrModule = Module(new SQAddrModule(
+    dataWidth = GPAddrBits,
+    numEntries = StoreQueueSize,
+    numRead = EnsbufferWidth + 1,
+    numWrite = StorePipelineWidth,
+    numForward = StorePipelineWidth
+  ))
+  gpaddrModule.io := DontCare
   val vaddrModule = Module(new SQAddrModule(
     dataWidth = VAddrBits,
     numEntries = StoreQueueSize,
@@ -188,11 +196,13 @@ class StoreQueue(implicit p: Parameters) extends XSModule
     dataModule.io.raddr(i) := rdataPtrExtNext(i).value
     paddrModule.io.raddr(i) := rdataPtrExtNext(i).value
     vaddrModule.io.raddr(i) := rdataPtrExtNext(i).value
+    gpaddrModule.io.raddr(i) := rdataPtrExtNext(i).value
   }
 
   // no inst will be committed 1 cycle before tval update
   vaddrModule.io.raddr(EnsbufferWidth) := (cmtPtrExt(0) + commitCount).value
 
+  gpaddrModule.io.raddr(EnsbufferWidth) := (cmtPtrExt(0) + commitCount).value
   /**
     * Enqueue at dispatch
     *
@@ -263,6 +273,7 @@ class StoreQueue(implicit p: Parameters) extends XSModule
   for (i <- 0 until StorePipelineWidth) {
     paddrModule.io.wen(i) := false.B
     vaddrModule.io.wen(i) := false.B
+    gpaddrModule.io.wen(i) := false.B
     dataModule.io.mask.wen(i) := false.B
     val stWbIndex = io.storeIn(i).bits.uop.sqIdx.value
     when (io.storeIn(i).fire()) {
@@ -280,6 +291,11 @@ class StoreQueue(implicit p: Parameters) extends XSModule
       vaddrModule.io.wlineflag(i) := io.storeIn(i).bits.wlineflag
       vaddrModule.io.wen(i) := true.B
 
+      gpaddrModule.io.waddr(i) := stWbIndex
+      gpaddrModule.io.wdata(i) := io.storeIn(i).bits.gpaddr
+      gpaddrModule.io.wlineflag(i) := io.storeIn(i).bits.wlineflag
+      gpaddrModule.io.wen(i) := true.B
+
       debug_paddr(paddrModule.io.waddr(i)) := paddrModule.io.wdata(i)
 
       // mmio(stWbIndex) := io.storeIn(i).bits.mmio
@@ -637,6 +653,7 @@ class StoreQueue(implicit p: Parameters) extends XSModule
 
   // Read vaddr for mem exception
   io.exceptionAddr.vaddr := vaddrModule.io.rdata(EnsbufferWidth)
+  io.exceptionAddr.gpaddr := gpaddrModule.io.rdata(EnsbufferWidth)
 
   // misprediction recovery / exception redirect
   // invalidate sq term using robIdx
diff --git a/src/main/scala/xiangshan/mem/pipeline/AtomicsUnit.scala b/src/main/scala/xiangshan/mem/pipeline/AtomicsUnit.scala
index 1f4620295..6fca405a2 100644
--- a/src/main/scala/xiangshan/mem/pipeline/AtomicsUnit.scala
+++ b/src/main/scala/xiangshan/mem/pipeline/AtomicsUnit.scala
@@ -41,7 +41,10 @@ class AtomicsUnit(implicit p: Parameters) extends XSModule with MemoryOpConstant
     val flush_sbuffer = new SbufferFlushBundle
     val feedbackSlow  = ValidIO(new RSFeedback)
     val redirect      = Flipped(ValidIO(new Redirect))
-    val exceptionAddr = ValidIO(UInt(VAddrBits.W))
+    val exceptionAddr = ValidIO(new Bundle{
+      val vaddr = UInt(VAddrBits.W)
+      val gpaddr = UInt(GPAddrBits.W)
+    })
     val csrCtrl       = Flipped(new CustomCSRCtrlIO)
   })
 
@@ -78,8 +81,8 @@ class AtomicsUnit(implicit p: Parameters) extends XSModule with MemoryOpConstant
   val fuop_reg = Reg(UInt(8.W))
 
   io.exceptionAddr.valid := atom_override_xtval
-  io.exceptionAddr.bits  := in.src(0)
-
+  io.exceptionAddr.bits.vaddr  := in.src(0)
+  io.exceptionAddr.bits.gpaddr := paddr
   // assign default value to output signals
   io.in.ready          := false.B
 
@@ -157,6 +160,8 @@ class AtomicsUnit(implicit p: Parameters) extends XSModule with MemoryOpConstant
       exceptionVec(storeAddrMisaligned) := !addrAligned && !isLr
       exceptionVec(storePageFault)      := io.dtlb.resp.bits.excp(0).pf.st
       exceptionVec(loadPageFault)       := io.dtlb.resp.bits.excp(0).pf.ld
+      exceptionVec(storeGuestPageFault) := io.dtlb.resp.bits.excp(0).pf.stG
+      exceptionVec(loadGuestPageFault)  := io.dtlb.resp.bits.excp(0).pf.ldG
       exceptionVec(storeAccessFault)    := io.dtlb.resp.bits.excp(0).af.st
       exceptionVec(loadAccessFault)     := io.dtlb.resp.bits.excp(0).af.ld
       static_pm := io.dtlb.resp.bits.static_pm
diff --git a/src/main/scala/xiangshan/mem/pipeline/LoadUnit.scala b/src/main/scala/xiangshan/mem/pipeline/LoadUnit.scala
index ed25dbb47..8876fcc68 100644
--- a/src/main/scala/xiangshan/mem/pipeline/LoadUnit.scala
+++ b/src/main/scala/xiangshan/mem/pipeline/LoadUnit.scala
@@ -208,6 +208,8 @@ class LoadUnit_S0(implicit p: Parameters) extends XSModule with HasDCacheParamet
   val isPrefetchRead = WireInit(s0_uop.ctrl.fuOpType === LSUOpType.prefetch_r)
   val isPrefetchWrite = WireInit(s0_uop.ctrl.fuOpType === LSUOpType.prefetch_w)
   val isHWPrefetch = lfsrc_hwprefetch_select
+  val isHlv = WireInit(LSUOpType.isHlv(s0_uop.ctrl.fuOpType))
+  val isHlvx = WireInit(LSUOpType.isHlvx(s0_uop.ctrl.fuOpType))
 
   // query DTLB
   io.dtlbReq.valid := s0_valid
@@ -217,6 +219,8 @@ class LoadUnit_S0(implicit p: Parameters) extends XSModule with HasDCacheParamet
     Mux(isPrefetchWrite, TlbCmd.write, TlbCmd.read),
     TlbCmd.read
   )
+  io.dtlbReq.bits.hyperinst := isHlv
+  io.dtlbReq.bits.hlvx := isHlvx
   io.dtlbReq.bits.size := LSUOpType.size(s0_uop.ctrl.fuOpType)
   io.dtlbReq.bits.kill := DontCare
   io.dtlbReq.bits.memidx.is_ld := true.B
@@ -396,6 +400,7 @@ class LoadUnit_S1(implicit p: Parameters) extends XSModule with HasCircularQueue
 
   val s1_uop = io.in.bits.uop
   val s1_paddr_dup_lsu = io.dtlbResp.bits.paddr(0)
+  val s1_gpaddr_dup_lsu = io.dtlbResp.bits.gpaddr(0)
   val s1_paddr_dup_dcache = io.dtlbResp.bits.paddr(1)
   // af & pf exception were modified below.
   val s1_exception = ExceptionNO.selectByFu(io.out.bits.uop.cf.exceptionVec, lduCfg).asUInt.orR
@@ -424,6 +429,7 @@ class LoadUnit_S1(implicit p: Parameters) extends XSModule with HasCircularQueue
   io.sbuffer.valid := io.in.valid && !(s1_exception || s1_tlb_miss || io.s1_kill || s1_is_prefetch)
   io.sbuffer.vaddr := io.in.bits.vaddr
   io.sbuffer.paddr := s1_paddr_dup_lsu
+  io.sbuffer.gpaddr := s1_gpaddr_dup_lsu
   io.sbuffer.uop := s1_uop
   io.sbuffer.sqIdx := s1_uop.sqIdx
   io.sbuffer.mask := s1_mask
@@ -431,6 +437,7 @@ class LoadUnit_S1(implicit p: Parameters) extends XSModule with HasCircularQueue
 
   io.lsq.valid := io.in.valid && !(s1_exception || s1_tlb_miss || io.s1_kill || s1_is_prefetch)
   io.lsq.vaddr := io.in.bits.vaddr
+  io.lsq.gpaddr := s1_gpaddr_dup_lsu
   io.lsq.paddr := s1_paddr_dup_lsu
   io.lsq.uop := s1_uop
   io.lsq.sqIdx := s1_uop.sqIdx
@@ -496,11 +503,13 @@ class LoadUnit_S1(implicit p: Parameters) extends XSModule with HasCircularQueue
   // load inst will be canceled immediately
   io.out.valid := io.in.valid && (!needLdVioCheckRedo && !s1_bank_conflict && !needReExecute || s1_is_sw_prefetch) && !io.s1_kill
   io.out.bits.paddr := s1_paddr_dup_lsu
+  io.out.bits.gpaddr := s1_gpaddr_dup_lsu
   io.out.bits.tlbMiss := s1_tlb_miss
 
   // current ori test will cause the case of ldest == 0, below will be modifeid in the future.
   // af & pf exception were modified
   io.out.bits.uop.cf.exceptionVec(loadPageFault) := io.dtlbResp.bits.excp(0).pf.ld
+  io.out.bits.uop.cf.exceptionVec(loadGuestPageFault) := io.dtlbResp.bits.excp(0).pf.ldG
   io.out.bits.uop.cf.exceptionVec(loadAccessFault) := io.dtlbResp.bits.excp(0).af.ld
 
   io.out.bits.ptwBack := io.dtlbResp.bits.ptwBack
@@ -928,6 +937,7 @@ class LoadUnit(implicit p: Parameters) extends XSModule
   io.lsq.loadVaddrIn.valid := load_s1.io.in.valid && !load_s1.io.s1_kill && !load_s1.io.out.bits.isHWPrefetch
   io.lsq.loadVaddrIn.bits.lqIdx := load_s1.io.out.bits.uop.lqIdx
   io.lsq.loadVaddrIn.bits.vaddr := load_s1.io.out.bits.vaddr
+  io.lsq.loadVaddrIn.bits.gpaddr := load_s1.io.out.bits.gpaddr
 
   // when S0 has opportunity to try pointerchasing, make sure it truely goes to S1
   // which is S0's out is ready and dcache is ready
diff --git a/src/main/scala/xiangshan/mem/pipeline/StoreUnit.scala b/src/main/scala/xiangshan/mem/pipeline/StoreUnit.scala
index 36399b3f9..56e9f8239 100644
--- a/src/main/scala/xiangshan/mem/pipeline/StoreUnit.scala
+++ b/src/main/scala/xiangshan/mem/pipeline/StoreUnit.scala
@@ -47,10 +47,13 @@ class StoreUnit_S0(implicit p: Parameters) extends XSModule {
     Mux(imm12(11), io.in.bits.src(0)(VAddrBits-1, 12)+SignExt(1.U, VAddrBits-12), io.in.bits.src(0)(VAddrBits-1, 12)),
   )
   val saddr = Cat(saddr_hi, saddr_lo(11,0))
+  val isHsv = WireInit(LSUOpType.isHsv(io.in.bits.uop.ctrl.fuOpType))
 
   io.dtlbReq.bits.vaddr := saddr
   io.dtlbReq.valid := io.in.valid
   io.dtlbReq.bits.cmd := TlbCmd.write
+  io.dtlbReq.bits.hyperinst := isHsv
+  io.dtlbReq.bits.hlvx := false.B
   io.dtlbReq.bits.size := LSUOpType.size(io.in.bits.uop.ctrl.fuOpType)
   io.dtlbReq.bits.kill := DontCare
   io.dtlbReq.bits.memidx.is_ld := false.B
@@ -116,6 +119,7 @@ class StoreUnit_S1(implicit p: Parameters) extends XSModule {
     io.in.bits.uop.ctrl.fuOpType === LSUOpType.cbo_inval
 
   val s1_paddr = io.dtlbResp.bits.paddr(0)
+  val s1_gpaddr = io.dtlbResp.bits.gpaddr(0)
   val s1_tlb_miss = io.dtlbResp.bits.miss
 
   val s1_mmio = is_mmio_cbo
@@ -150,10 +154,12 @@ class StoreUnit_S1(implicit p: Parameters) extends XSModule {
   io.out.valid := io.in.valid && !s1_tlb_miss
   io.out.bits := io.in.bits
   io.out.bits.paddr := s1_paddr
+  io.out.bits.gpaddr := s1_gpaddr
   io.out.bits.miss := false.B
   io.out.bits.mmio := s1_mmio
   io.out.bits.atomic := s1_mmio
   io.out.bits.uop.cf.exceptionVec(storePageFault) := io.dtlbResp.bits.excp(0).pf.st
+  io.out.bits.uop.cf.exceptionVec(storeGuestPageFault) := io.dtlbResp.bits.excp(0).pf.stG
   io.out.bits.uop.cf.exceptionVec(storeAccessFault) := io.dtlbResp.bits.excp(0).af.st
 
   io.lsq.valid := io.in.valid
diff --git a/src/main/scala/xiangshan/package.scala b/src/main/scala/xiangshan/package.scala
index cf58ba23c..fa219748c 100644
--- a/src/main/scala/xiangshan/package.scala
+++ b/src/main/scala/xiangshan/package.scala
@@ -149,7 +149,7 @@ package object xiangshan {
   }
 
   object ExceptionVec {
-    def apply() = Vec(16, Bool())
+    def apply() = Vec(24, Bool())
   }
 
   object PMAMode {
@@ -216,6 +216,8 @@ package object xiangshan {
     def fence  = "b10000".U
     def sfence = "b10001".U
     def fencei = "b10010".U
+    def hfence_v = "b10011".U
+    def hfence_g = "b10100".U
     def nofence= "b00000".U
   }
 
@@ -375,6 +377,19 @@ package object xiangshan {
     def lbu      = "b0100".U
     def lhu      = "b0101".U
     def lwu      = "b0110".U
+    // hypervior load
+    // bit encoding: | hlvx 1 | hlv 1 | load 0 | is unsigned(1bit) | size(2bit) |
+    def hlvb = "b10000".U
+    def hlvh = "b10001".U
+    def hlvw = "b10010".U
+    def hlvd = "b10011".U
+    def hlvbu = "b10100".U
+    def hlvhu = "b10101".U
+    def hlvwu = "b10110".U
+    def hlvxhu = "b110101".U
+    def hlvxwu = "b110110".U
+    def isHlv(op: UInt): Bool = op(4)
+    def isHlvx(op: UInt): Bool = op(5)
 
     // Zicbop software prefetch
     // bit encoding: | prefetch 1 | 0 | prefetch type (2bit) |
@@ -392,6 +407,14 @@ package object xiangshan {
     def sw       = "b0010".U
     def sd       = "b0011".U
 
+    //hypervisor store
+    // bit encoding: |hsv 1 | store 00 | size(2bit) |
+    def hsvb = "b10000".U
+    def hsvh = "b10001".U
+    def hsvw = "b10010".U
+    def hsvd = "b10011".U
+    def isHsv(op: UInt): Bool = op(4)
+
     // l1 cache op
     // bit encoding: | cbo_zero 01 | size(2bit) 11 |
     def cbo_zero  = "b0111".U
@@ -519,22 +542,31 @@ package object xiangshan {
     def storeAccessFault    = 7
     def ecallU              = 8
     def ecallS              = 9
+    def ecallVS             = 10
     def ecallM              = 11
     def instrPageFault      = 12
     def loadPageFault       = 13
     // def singleStep          = 14
     def storePageFault      = 15
+    def instrGuestPageFault = 20
+    def loadGuestPageFault  = 21
+    def virtualInstr        = 22
+    def storeGuestPageFault = 23
     def priorities = Seq(
       breakPoint, // TODO: different BP has different priority
       instrPageFault,
+      instrGuestPageFault,
       instrAccessFault,
       illegalInstr,
+      virtualInstr,
       instrAddrMisaligned,
-      ecallM, ecallS, ecallU,
+      ecallM, ecallS, ecallVS, ecallU,
       storeAddrMisaligned,
       loadAddrMisaligned,
       storePageFault,
       loadPageFault,
+      storeGuestPageFault,
+      loadGuestPageFault,
       storeAccessFault,
       loadAccessFault
     )
@@ -543,7 +575,9 @@ package object xiangshan {
       instrAddrMisaligned,
       instrAccessFault,
       illegalInstr,
-      instrPageFault
+      instrPageFault,
+      instrGuestPageFault,
+      virtualInstr
     )
     def partialSelect(vec: Vec[Bool], select: Seq[Int]): Vec[Bool] = {
       val new_vec = Wire(ExceptionVec())
@@ -623,7 +657,7 @@ package object xiangshan {
     fuGen = fenceGen,
     fuSel = (uop: MicroOp) => uop.ctrl.fuType === FuType.fence,
     FuType.fence, 2, 0, writeIntRf = false, writeFpRf = false,
-    latency = UncertainLatency(), exceptionOut = Seq(illegalInstr), // TODO: need rewrite latency structure, not just this value,
+    latency = UncertainLatency(), exceptionOut = Seq(illegalInstr, virtualInstr), // TODO: need rewrite latency structure, not just this value,
     flushPipe = true
   )
 
@@ -636,7 +670,7 @@ package object xiangshan {
     numFpSrc = 0,
     writeIntRf = true,
     writeFpRf = false,
-    exceptionOut = Seq(illegalInstr, breakPoint, ecallU, ecallS, ecallM),
+    exceptionOut = Seq(illegalInstr, virtualInstr, breakPoint, ecallU, ecallS, ecallVS, ecallM),
     flushPipe = true
   )
 
@@ -735,7 +769,7 @@ package object xiangshan {
     (uop: MicroOp) => FuType.loadCanAccept(uop.ctrl.fuType),
     FuType.ldu, 1, 0, writeIntRf = true, writeFpRf = true,
     latency = UncertainLatency(),
-    exceptionOut = Seq(loadAddrMisaligned, loadAccessFault, loadPageFault),
+    exceptionOut = Seq(loadAddrMisaligned, loadAccessFault, loadPageFault, loadGuestPageFault),
     flushPipe = true,
     replayInst = true,
     hasLoadError = true
@@ -747,7 +781,7 @@ package object xiangshan {
     (uop: MicroOp) => FuType.storeCanAccept(uop.ctrl.fuType),
     FuType.stu, 1, 0, writeIntRf = false, writeFpRf = false,
     latency = UncertainLatency(),
-    exceptionOut = Seq(storeAddrMisaligned, storeAccessFault, storePageFault)
+    exceptionOut = Seq(storeAddrMisaligned, storeAccessFault, storePageFault, storeGuestPageFault)
   )
 
   val stdCfg = FuConfig(
diff --git a/utility b/utility
index ede0e0027..940829124 160000
--- a/utility
+++ b/utility
@@ -1 +1 @@
-Subproject commit ede0e0027d547a94faa3d35bba9812d3a4ca59fc
+Subproject commit 940829124579fce0716d34ed8db40b773e461163
